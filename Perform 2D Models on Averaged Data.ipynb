{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "54aQmQxYZa4a"
   },
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "kk5jlB-DZbTH",
    "outputId": "454f38f9-e725-45a2-b531-b1a43e61ce5e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from google.colab import drive\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.losses import categorical_crossentropy\n",
    "import keras.layers as l\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras import Sequential\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "drive.mount('/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8qpxXfXUZhHo"
   },
   "source": [
    "## Load Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y2s67JUpxPqD"
   },
   "outputs": [],
   "source": [
    "data = [np.load('/gdrive/My Drive/CS 168/DSA Colab Notebook/means/fractals_' + str(i) + '.npy', allow_pickle=True).item() for i in range(1, 202)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RZpfJYZXZuV0"
   },
   "source": [
    "we could probably try throwing these images out\n",
    "1, 2, 101\n",
    "\n",
    "we could modify the following images (where they are averaged), because the mean doesn't look as good as the original \n",
    "10, 13, 37, 74, 75, 88, 93, 96, 108"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5Ornu8BD2ObF"
   },
   "outputs": [],
   "source": [
    "# get the labels by determining which is the best label to use (dr1, dr2, or report)\n",
    "def get_y(x):\n",
    "    yr = x['TICI_report'][0]\n",
    "    yd1 = x['TICI_Dr1'][0]\n",
    "    yd2 = x['TICI_Dr2'][0]\n",
    "    if (isinstance(yr, np.ndarray)):\n",
    "        yr = yr[0]\n",
    "    if (isinstance(yd1, np.ndarray)):\n",
    "        yd1 = yd1[0]\n",
    "    if(isinstance(yd2, np.ndarray)):\n",
    "        yd2 = yd2[0]\n",
    "        \n",
    "    if (isinstance(yd1, str)) or (isinstance(yd1, np.uint8)):\n",
    "      return yd1\n",
    "    elif (isinstance(yd2, str)) or (isinstance(yd2, np.uint8)):\n",
    "      return yd2\n",
    "    elif (isinstance(yr, str)) or (isinstance(yr, np.uint8)):\n",
    "      return yr\n",
    "    else:\n",
    "      return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i-26cTUVQjF_"
   },
   "outputs": [],
   "source": [
    "def get_clean_data (data):\n",
    "  df = pd.DataFrame(data)\n",
    "  df = df.drop(['__globals__', '__header__', '__version__'], axis=1)\n",
    "  df['y'] = 0 # New column in which I'll fill values\n",
    "  df['y'] = df.apply(lambda x: get_y(x), axis=1)\n",
    "  data = df[['X', 'y']]\n",
    "  clean_data = data.dropna(how='any')\n",
    "  clean_data.loc[clean_data['y'] == '0 (bilateral MCA)', ['y']]= 0\n",
    "\n",
    "  # For this datapoint, I thought '2a?' might be another type of category, but\n",
    "  # I believe there is only one data point that has '2a?' as a category so I'm\n",
    "  # going to treat it as an error and convert it to '2a'\n",
    "  clean_data.loc[clean_data['y'] == '2a?', ['y']] = '2a'\n",
    "  return clean_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 301
    },
    "colab_type": "code",
    "id": "C2tRejD6al0n",
    "outputId": "45261d90-5564-4b5f-cfad-52351e89a23c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:190: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "clean_data = get_clean_data(data)\n",
    "del data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eAuCxba_o2k8"
   },
   "source": [
    "## Train-Test-split and to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fk_SZLm8RCkM"
   },
   "outputs": [],
   "source": [
    "TEST_SIZE = 0.2\n",
    "RANDOM_STATE = 42\n",
    "X_train, X_test, y_train, y_test = train_test_split(clean_data.drop('y', axis=1), clean_data['y'], test_size=TEST_SIZE, random_state=RANDOM_STATE, shuffle=False, stratify=None)\n",
    "del clean_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bMO3F5wxRMBi"
   },
   "outputs": [],
   "source": [
    "# to_categorical (I used pd.get_dummies because they work with dataframes and return column name)\n",
    "y_train = pd.get_dummies(y_train)\n",
    "y_test = pd.get_dummies(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lKhB_tqdRiOK"
   },
   "outputs": [],
   "source": [
    "# Converting Pandas DataFrames into numpy arrays, which keras supports\n",
    "X_train = X_train.values\n",
    "X_test = X_test.values\n",
    "y_train = y_train.values\n",
    "y_test = y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lh_fLA8OQSND"
   },
   "outputs": [],
   "source": [
    "X_train = np.array([arr[0] for arr in X_train])\n",
    "X_test = np.array([arr[0] for arr in X_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M9kWqDzNnfFe"
   },
   "source": [
    "## Run CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BHGpRtxFTE2q"
   },
   "outputs": [],
   "source": [
    "def create_simple_model():\n",
    "  model = Sequential([\n",
    "      l.Conv2D(32, 5, padding='same', activation='relu', input_shape=(512,512,1)),\n",
    "      l.MaxPooling2D((2, 2), (2, 2), padding='same'),\n",
    "      l.BatchNormalization(),\n",
    "      l.Conv2D(64, 5, padding='same', activation='relu'),\n",
    "      l.MaxPooling2D((2, 2), (2, 2), padding='same'),\n",
    "      l.Conv2D(64, 5, padding='same', activation='relu'),\n",
    "      l.MaxPooling2D((2, 2), (2, 2), padding='same'),\n",
    "      l.Conv2D(64, 5, padding='same', activation='relu'),\n",
    "      l.MaxPooling2D((2, 2), (2, 2), padding='same'),\n",
    "      l.Conv2D(64, 5, padding='same', activation='relu'),\n",
    "      l.MaxPooling2D((2, 2), (2, 2), padding='same'),\n",
    "      l.Conv2D(64, 5, padding='same', activation='relu'),\n",
    "      l.MaxPooling2D((2, 2), (2, 2), padding='same'),\n",
    "      l.Flatten(),\n",
    "      l.Dense(1024, activation='relu'),\n",
    "      l.Dropout(0.4),\n",
    "      l.Dense(5,activation='softmax')\n",
    "  ])\n",
    "\n",
    "\n",
    "  model.summary()\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y7kCDt61-Glf"
   },
   "outputs": [],
   "source": [
    "img_rows,img_cols=512,512\n",
    "def train_simple_model (model, batch_size, epochs, x, y, xt, yt):\n",
    "  x = x.reshape(x.shape[0], img_rows, img_cols, 1)\n",
    "  xt = xt.reshape(xt.shape[0], img_rows, img_cols, 1)\n",
    "  model.compile(\n",
    "    loss=categorical_crossentropy,\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "  model.fit(x, y,\n",
    "            batch_size=batch_size,\n",
    "            epochs=epochs,\n",
    "            verbose=1,\n",
    "            validation_data=(xt, yt))\n",
    "  score = model.evaluate(xt, yt, verbose=0)\n",
    "  return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1181
    },
    "colab_type": "code",
    "id": "sr8LYaZHbYxH",
    "outputId": "09b8b3d5-63f9-4740-9908-162315033c4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_13 (Conv2D)           (None, 512, 512, 32)      832       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling (None, 256, 256, 32)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 256, 256, 32)      128       \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 256, 256, 64)      51264     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling (None, 128, 128, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 128, 128, 64)      102464    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling (None, 64, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 64, 64, 64)        102464    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_16 (MaxPooling (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 32, 32, 64)        102464    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_17 (MaxPooling (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 16, 16, 64)        102464    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_18 (MaxPooling (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1024)              4195328   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 5)                 5125      \n",
      "=================================================================\n",
      "Total params: 4,662,533\n",
      "Trainable params: 4,662,469\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 144 samples, validate on 37 samples\n",
      "Epoch 1/10\n",
      "144/144 [==============================] - 10s 68ms/step - loss: 1.6124 - acc: 0.2986 - val_loss: 1.4948 - val_acc: 0.3243\n",
      "Epoch 2/10\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.4268 - acc: 0.3056 - val_loss: 1.3601 - val_acc: 0.3784\n",
      "Epoch 3/10\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.3312 - acc: 0.4722 - val_loss: 1.3476 - val_acc: 0.3784\n",
      "Epoch 4/10\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.3397 - acc: 0.4722 - val_loss: 1.3443 - val_acc: 0.3784\n",
      "Epoch 5/10\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.3146 - acc: 0.4722 - val_loss: 1.3529 - val_acc: 0.3784\n",
      "Epoch 6/10\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.3043 - acc: 0.4722 - val_loss: 1.3411 - val_acc: 0.3784\n",
      "Epoch 7/10\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.2941 - acc: 0.4722 - val_loss: 1.3472 - val_acc: 0.3784\n",
      "Epoch 8/10\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.2943 - acc: 0.4722 - val_loss: 1.3746 - val_acc: 0.3784\n",
      "Epoch 9/10\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.3014 - acc: 0.4722 - val_loss: 1.3628 - val_acc: 0.3784\n",
      "Epoch 10/10\n",
      "144/144 [==============================] - 2s 15ms/step - loss: 1.2990 - acc: 0.4722 - val_loss: 1.3504 - val_acc: 0.3784\n"
     ]
    }
   ],
   "source": [
    "model = create_simple_model()\n",
    "loss, accuracy = train_simple_model(model, 20, 10, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "AeGvZcQ6THcM",
    "outputId": "845f4092-3530-4dbe-8af6-b70726cc04a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 1.3504295316902366\n",
      "Test accuracy: 0.37837838160025106\n"
     ]
    }
   ],
   "source": [
    "print('Test loss:', loss)\n",
    "print('Test accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R6XLqW4vaqsA"
   },
   "source": [
    "## Run Inception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "02xWE3gQbO7e"
   },
   "outputs": [],
   "source": [
    "def create_inception_model():\n",
    "  base_model = InceptionV3(weights='imagenet', include_top=False, input_shape = (512,512,3)) \n",
    "\n",
    "  # DNN portion\n",
    "  # average pooling layer\n",
    "  x = l.GlobalAveragePooling2D()(base_model.output)\n",
    "  # fully-connected layer\n",
    "  x = l.Dense(1024, activation='relu')(x)\n",
    "  # logistic layer\n",
    "  output = l.Dense(5, activation='softmax')(x) # 5 is the number of classes in the data, since TICI is 0,1,2a,2b,3\n",
    "\n",
    "  model_inception = Model(inputs=base_model.input, outputs=output)\n",
    "\n",
    "  #model_inception.summary()\n",
    "  \n",
    "  return model_inception\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L79lSha_de4K"
   },
   "outputs": [],
   "source": [
    "def train_inception_model(model, batch_size, epochs, x, y, xt, yt):\n",
    "  # force images to have three channels\n",
    "  xnew = np.zeros((x.shape[0], 512, 512, 3))\n",
    "  for i in range(0, x.shape[0]):\n",
    "      xnew[i, :, :, 0] = x[i]\n",
    "      xnew[i, :, :, 1] = x[i]\n",
    "      xnew[i, :, :, 2] = x[i]\n",
    "\n",
    "  xtnew = np.zeros((xt.shape[0], 512, 512, 3))\n",
    "  for i in range(0, X_test.shape[0]):\n",
    "      xtnew[i, :, :, 0] = xt[i]\n",
    "      xtnew[i, :, :, 1] = xt[i]\n",
    "      xtnew[i, :, :, 2] = xt[i]\n",
    "  \n",
    "  # train model\n",
    "  rmsprop = keras.optimizers.RMSprop(lr=0.005)\n",
    "  \n",
    "  model.compile(optimizer=rmsprop,\n",
    "                loss=categorical_crossentropy,\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "  model.fit(xnew, y,\n",
    "            batch_size=batch_size,\n",
    "            epochs=epochs,\n",
    "            verbose=1,\n",
    "            validation_data=(xtnew, yt))\n",
    "  # evaluate model score is accuracy and loss\n",
    "  score = model.evaluate(xtnew, yt, verbose=0)\n",
    "  return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 12234
    },
    "colab_type": "code",
    "id": "sp_biWsZa6qB",
    "outputId": "89a33362-dc0f-4ff1-cb80-416a01964ef3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.5/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "87916544/87910968 [==============================] - 3s 0us/step\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 512, 512, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 255, 255, 32) 864         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 255, 255, 32) 96          conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 255, 255, 32) 0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 253, 253, 32) 9216        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 253, 253, 32) 96          conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 253, 253, 32) 0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 253, 253, 64) 18432       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 253, 253, 64) 192         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 253, 253, 64) 0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_19 (MaxPooling2D) (None, 126, 126, 64) 0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 126, 126, 80) 5120        max_pooling2d_19[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 126, 126, 80) 240         conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 126, 126, 80) 0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 124, 124, 192 138240      activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 124, 124, 192 576         conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 124, 124, 192 0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_20 (MaxPooling2D) (None, 61, 61, 192)  0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 61, 61, 64)   12288       max_pooling2d_20[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 61, 61, 64)   192         conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 61, 61, 64)   0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 61, 61, 48)   9216        max_pooling2d_20[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 61, 61, 96)   55296       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 61, 61, 48)   144         conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 61, 61, 96)   288         conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 61, 61, 48)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 61, 61, 96)   0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 61, 61, 192)  0           max_pooling2d_20[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 61, 61, 64)   12288       max_pooling2d_20[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 61, 61, 64)   76800       activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 61, 61, 96)   82944       activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 61, 61, 32)   6144        average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 61, 61, 64)   192         conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 61, 61, 64)   192         conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 61, 61, 96)   288         conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 61, 61, 32)   96          conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 61, 61, 64)   0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 61, 61, 64)   0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 61, 61, 96)   0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 61, 61, 32)   0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed0 (Concatenate)            (None, 61, 61, 256)  0           activation_6[0][0]               \n",
      "                                                                 activation_8[0][0]               \n",
      "                                                                 activation_11[0][0]              \n",
      "                                                                 activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 61, 61, 64)   16384       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 61, 61, 64)   192         conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 61, 61, 64)   0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 61, 61, 48)   12288       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 61, 61, 96)   55296       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 61, 61, 48)   144         conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 61, 61, 96)   288         conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 61, 61, 48)   0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 61, 61, 96)   0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_2 (AveragePoo (None, 61, 61, 256)  0           mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 61, 61, 64)   16384       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 61, 61, 64)   76800       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 61, 61, 96)   82944       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 61, 61, 64)   16384       average_pooling2d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 61, 61, 64)   192         conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 61, 61, 64)   192         conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 61, 61, 96)   288         conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 61, 61, 64)   192         conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 61, 61, 64)   0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 61, 61, 64)   0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 61, 61, 96)   0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 61, 61, 64)   0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed1 (Concatenate)            (None, 61, 61, 288)  0           activation_13[0][0]              \n",
      "                                                                 activation_15[0][0]              \n",
      "                                                                 activation_18[0][0]              \n",
      "                                                                 activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, 61, 61, 64)   18432       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 61, 61, 64)   192         conv2d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 61, 61, 64)   0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 61, 61, 48)   13824       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, 61, 61, 96)   55296       activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 61, 61, 48)   144         conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 61, 61, 96)   288         conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 61, 61, 48)   0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 61, 61, 96)   0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_3 (AveragePoo (None, 61, 61, 288)  0           mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 61, 61, 64)   18432       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, 61, 61, 64)   76800       activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, 61, 61, 96)   82944       activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, 61, 61, 64)   18432       average_pooling2d_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 61, 61, 64)   192         conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 61, 61, 64)   192         conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 61, 61, 96)   288         conv2d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 61, 61, 64)   192         conv2d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 61, 61, 64)   0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 61, 61, 64)   0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 61, 61, 96)   0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 61, 61, 64)   0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed2 (Concatenate)            (None, 61, 61, 288)  0           activation_20[0][0]              \n",
      "                                                                 activation_22[0][0]              \n",
      "                                                                 activation_25[0][0]              \n",
      "                                                                 activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)              (None, 61, 61, 64)   18432       mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 61, 61, 64)   192         conv2d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 61, 61, 64)   0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_47 (Conv2D)              (None, 61, 61, 96)   55296       activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 61, 61, 96)   288         conv2d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 61, 61, 96)   0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (None, 30, 30, 384)  995328      mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)              (None, 30, 30, 96)   82944       activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 30, 30, 384)  1152        conv2d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 30, 30, 96)   288         conv2d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 30, 30, 384)  0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 30, 30, 96)   0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_21 (MaxPooling2D) (None, 30, 30, 288)  0           mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "mixed3 (Concatenate)            (None, 30, 30, 768)  0           activation_27[0][0]              \n",
      "                                                                 activation_30[0][0]              \n",
      "                                                                 max_pooling2d_21[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_53 (Conv2D)              (None, 30, 30, 128)  98304       mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 30, 30, 128)  384         conv2d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 30, 30, 128)  0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_54 (Conv2D)              (None, 30, 30, 128)  114688      activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 30, 30, 128)  384         conv2d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 30, 30, 128)  0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_50 (Conv2D)              (None, 30, 30, 128)  98304       mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_55 (Conv2D)              (None, 30, 30, 128)  114688      activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 30, 30, 128)  384         conv2d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 30, 30, 128)  384         conv2d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 30, 30, 128)  0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 30, 30, 128)  0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_51 (Conv2D)              (None, 30, 30, 128)  114688      activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_56 (Conv2D)              (None, 30, 30, 128)  114688      activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 30, 30, 128)  384         conv2d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 30, 30, 128)  384         conv2d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 30, 30, 128)  0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 30, 30, 128)  0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_4 (AveragePoo (None, 30, 30, 768)  0           mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_49 (Conv2D)              (None, 30, 30, 192)  147456      mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_52 (Conv2D)              (None, 30, 30, 192)  172032      activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_57 (Conv2D)              (None, 30, 30, 192)  172032      activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_58 (Conv2D)              (None, 30, 30, 192)  147456      average_pooling2d_4[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 30, 30, 192)  576         conv2d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 30, 30, 192)  576         conv2d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 30, 30, 192)  576         conv2d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNo (None, 30, 30, 192)  576         conv2d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 30, 30, 192)  0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 30, 30, 192)  0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 30, 30, 192)  0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 30, 30, 192)  0           batch_normalization_43[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed4 (Concatenate)            (None, 30, 30, 768)  0           activation_31[0][0]              \n",
      "                                                                 activation_34[0][0]              \n",
      "                                                                 activation_39[0][0]              \n",
      "                                                                 activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_63 (Conv2D)              (None, 30, 30, 160)  122880      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_48 (BatchNo (None, 30, 30, 160)  480         conv2d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 30, 30, 160)  0           batch_normalization_48[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_64 (Conv2D)              (None, 30, 30, 160)  179200      activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_49 (BatchNo (None, 30, 30, 160)  480         conv2d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 30, 30, 160)  0           batch_normalization_49[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_60 (Conv2D)              (None, 30, 30, 160)  122880      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_65 (Conv2D)              (None, 30, 30, 160)  179200      activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNo (None, 30, 30, 160)  480         conv2d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_50 (BatchNo (None, 30, 30, 160)  480         conv2d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 30, 30, 160)  0           batch_normalization_45[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 30, 30, 160)  0           batch_normalization_50[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_61 (Conv2D)              (None, 30, 30, 160)  179200      activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_66 (Conv2D)              (None, 30, 30, 160)  179200      activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNo (None, 30, 30, 160)  480         conv2d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_51 (BatchNo (None, 30, 30, 160)  480         conv2d_66[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 30, 30, 160)  0           batch_normalization_46[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 30, 30, 160)  0           batch_normalization_51[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_5 (AveragePoo (None, 30, 30, 768)  0           mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_59 (Conv2D)              (None, 30, 30, 192)  147456      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_62 (Conv2D)              (None, 30, 30, 192)  215040      activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_67 (Conv2D)              (None, 30, 30, 192)  215040      activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_68 (Conv2D)              (None, 30, 30, 192)  147456      average_pooling2d_5[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, 30, 30, 192)  576         conv2d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_47 (BatchNo (None, 30, 30, 192)  576         conv2d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_52 (BatchNo (None, 30, 30, 192)  576         conv2d_67[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_53 (BatchNo (None, 30, 30, 192)  576         conv2d_68[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 30, 30, 192)  0           batch_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 30, 30, 192)  0           batch_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 30, 30, 192)  0           batch_normalization_52[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 30, 30, 192)  0           batch_normalization_53[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed5 (Concatenate)            (None, 30, 30, 768)  0           activation_41[0][0]              \n",
      "                                                                 activation_44[0][0]              \n",
      "                                                                 activation_49[0][0]              \n",
      "                                                                 activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_73 (Conv2D)              (None, 30, 30, 160)  122880      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_58 (BatchNo (None, 30, 30, 160)  480         conv2d_73[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 30, 30, 160)  0           batch_normalization_58[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_74 (Conv2D)              (None, 30, 30, 160)  179200      activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_59 (BatchNo (None, 30, 30, 160)  480         conv2d_74[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 30, 30, 160)  0           batch_normalization_59[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_70 (Conv2D)              (None, 30, 30, 160)  122880      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_75 (Conv2D)              (None, 30, 30, 160)  179200      activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_55 (BatchNo (None, 30, 30, 160)  480         conv2d_70[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_60 (BatchNo (None, 30, 30, 160)  480         conv2d_75[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 30, 30, 160)  0           batch_normalization_55[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 30, 30, 160)  0           batch_normalization_60[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_71 (Conv2D)              (None, 30, 30, 160)  179200      activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_76 (Conv2D)              (None, 30, 30, 160)  179200      activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_56 (BatchNo (None, 30, 30, 160)  480         conv2d_71[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_61 (BatchNo (None, 30, 30, 160)  480         conv2d_76[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 30, 30, 160)  0           batch_normalization_56[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 30, 30, 160)  0           batch_normalization_61[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_6 (AveragePoo (None, 30, 30, 768)  0           mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_69 (Conv2D)              (None, 30, 30, 192)  147456      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_72 (Conv2D)              (None, 30, 30, 192)  215040      activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_77 (Conv2D)              (None, 30, 30, 192)  215040      activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_78 (Conv2D)              (None, 30, 30, 192)  147456      average_pooling2d_6[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_54 (BatchNo (None, 30, 30, 192)  576         conv2d_69[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_57 (BatchNo (None, 30, 30, 192)  576         conv2d_72[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_62 (BatchNo (None, 30, 30, 192)  576         conv2d_77[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_63 (BatchNo (None, 30, 30, 192)  576         conv2d_78[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 30, 30, 192)  0           batch_normalization_54[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 30, 30, 192)  0           batch_normalization_57[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 30, 30, 192)  0           batch_normalization_62[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 30, 30, 192)  0           batch_normalization_63[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed6 (Concatenate)            (None, 30, 30, 768)  0           activation_51[0][0]              \n",
      "                                                                 activation_54[0][0]              \n",
      "                                                                 activation_59[0][0]              \n",
      "                                                                 activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_83 (Conv2D)              (None, 30, 30, 192)  147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_68 (BatchNo (None, 30, 30, 192)  576         conv2d_83[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 30, 30, 192)  0           batch_normalization_68[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_84 (Conv2D)              (None, 30, 30, 192)  258048      activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_69 (BatchNo (None, 30, 30, 192)  576         conv2d_84[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_66 (Activation)      (None, 30, 30, 192)  0           batch_normalization_69[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_80 (Conv2D)              (None, 30, 30, 192)  147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_85 (Conv2D)              (None, 30, 30, 192)  258048      activation_66[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_65 (BatchNo (None, 30, 30, 192)  576         conv2d_80[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_70 (BatchNo (None, 30, 30, 192)  576         conv2d_85[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 30, 30, 192)  0           batch_normalization_65[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_67 (Activation)      (None, 30, 30, 192)  0           batch_normalization_70[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_81 (Conv2D)              (None, 30, 30, 192)  258048      activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_86 (Conv2D)              (None, 30, 30, 192)  258048      activation_67[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_66 (BatchNo (None, 30, 30, 192)  576         conv2d_81[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_71 (BatchNo (None, 30, 30, 192)  576         conv2d_86[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 30, 30, 192)  0           batch_normalization_66[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_68 (Activation)      (None, 30, 30, 192)  0           batch_normalization_71[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_7 (AveragePoo (None, 30, 30, 768)  0           mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_79 (Conv2D)              (None, 30, 30, 192)  147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_82 (Conv2D)              (None, 30, 30, 192)  258048      activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_87 (Conv2D)              (None, 30, 30, 192)  258048      activation_68[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_88 (Conv2D)              (None, 30, 30, 192)  147456      average_pooling2d_7[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_64 (BatchNo (None, 30, 30, 192)  576         conv2d_79[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_67 (BatchNo (None, 30, 30, 192)  576         conv2d_82[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_72 (BatchNo (None, 30, 30, 192)  576         conv2d_87[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_73 (BatchNo (None, 30, 30, 192)  576         conv2d_88[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 30, 30, 192)  0           batch_normalization_64[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 30, 30, 192)  0           batch_normalization_67[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_69 (Activation)      (None, 30, 30, 192)  0           batch_normalization_72[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_70 (Activation)      (None, 30, 30, 192)  0           batch_normalization_73[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed7 (Concatenate)            (None, 30, 30, 768)  0           activation_61[0][0]              \n",
      "                                                                 activation_64[0][0]              \n",
      "                                                                 activation_69[0][0]              \n",
      "                                                                 activation_70[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_91 (Conv2D)              (None, 30, 30, 192)  147456      mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_76 (BatchNo (None, 30, 30, 192)  576         conv2d_91[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_73 (Activation)      (None, 30, 30, 192)  0           batch_normalization_76[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_92 (Conv2D)              (None, 30, 30, 192)  258048      activation_73[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_77 (BatchNo (None, 30, 30, 192)  576         conv2d_92[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_74 (Activation)      (None, 30, 30, 192)  0           batch_normalization_77[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_89 (Conv2D)              (None, 30, 30, 192)  147456      mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_93 (Conv2D)              (None, 30, 30, 192)  258048      activation_74[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_74 (BatchNo (None, 30, 30, 192)  576         conv2d_89[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_78 (BatchNo (None, 30, 30, 192)  576         conv2d_93[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_71 (Activation)      (None, 30, 30, 192)  0           batch_normalization_74[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_75 (Activation)      (None, 30, 30, 192)  0           batch_normalization_78[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_90 (Conv2D)              (None, 14, 14, 320)  552960      activation_71[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_94 (Conv2D)              (None, 14, 14, 192)  331776      activation_75[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_75 (BatchNo (None, 14, 14, 320)  960         conv2d_90[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_79 (BatchNo (None, 14, 14, 192)  576         conv2d_94[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_72 (Activation)      (None, 14, 14, 320)  0           batch_normalization_75[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_76 (Activation)      (None, 14, 14, 192)  0           batch_normalization_79[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_22 (MaxPooling2D) (None, 14, 14, 768)  0           mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "mixed8 (Concatenate)            (None, 14, 14, 1280) 0           activation_72[0][0]              \n",
      "                                                                 activation_76[0][0]              \n",
      "                                                                 max_pooling2d_22[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_99 (Conv2D)              (None, 14, 14, 448)  573440      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_84 (BatchNo (None, 14, 14, 448)  1344        conv2d_99[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_81 (Activation)      (None, 14, 14, 448)  0           batch_normalization_84[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_96 (Conv2D)              (None, 14, 14, 384)  491520      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_100 (Conv2D)             (None, 14, 14, 384)  1548288     activation_81[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_81 (BatchNo (None, 14, 14, 384)  1152        conv2d_96[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_85 (BatchNo (None, 14, 14, 384)  1152        conv2d_100[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_78 (Activation)      (None, 14, 14, 384)  0           batch_normalization_81[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_82 (Activation)      (None, 14, 14, 384)  0           batch_normalization_85[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_97 (Conv2D)              (None, 14, 14, 384)  442368      activation_78[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_98 (Conv2D)              (None, 14, 14, 384)  442368      activation_78[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_101 (Conv2D)             (None, 14, 14, 384)  442368      activation_82[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_102 (Conv2D)             (None, 14, 14, 384)  442368      activation_82[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_8 (AveragePoo (None, 14, 14, 1280) 0           mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_95 (Conv2D)              (None, 14, 14, 320)  409600      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_82 (BatchNo (None, 14, 14, 384)  1152        conv2d_97[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_83 (BatchNo (None, 14, 14, 384)  1152        conv2d_98[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_86 (BatchNo (None, 14, 14, 384)  1152        conv2d_101[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_87 (BatchNo (None, 14, 14, 384)  1152        conv2d_102[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_103 (Conv2D)             (None, 14, 14, 192)  245760      average_pooling2d_8[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_80 (BatchNo (None, 14, 14, 320)  960         conv2d_95[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_79 (Activation)      (None, 14, 14, 384)  0           batch_normalization_82[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_80 (Activation)      (None, 14, 14, 384)  0           batch_normalization_83[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_83 (Activation)      (None, 14, 14, 384)  0           batch_normalization_86[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_84 (Activation)      (None, 14, 14, 384)  0           batch_normalization_87[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_88 (BatchNo (None, 14, 14, 192)  576         conv2d_103[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_77 (Activation)      (None, 14, 14, 320)  0           batch_normalization_80[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed9_0 (Concatenate)          (None, 14, 14, 768)  0           activation_79[0][0]              \n",
      "                                                                 activation_80[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 14, 14, 768)  0           activation_83[0][0]              \n",
      "                                                                 activation_84[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_85 (Activation)      (None, 14, 14, 192)  0           batch_normalization_88[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed9 (Concatenate)            (None, 14, 14, 2048) 0           activation_77[0][0]              \n",
      "                                                                 mixed9_0[0][0]                   \n",
      "                                                                 concatenate_1[0][0]              \n",
      "                                                                 activation_85[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_108 (Conv2D)             (None, 14, 14, 448)  917504      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_93 (BatchNo (None, 14, 14, 448)  1344        conv2d_108[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_90 (Activation)      (None, 14, 14, 448)  0           batch_normalization_93[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_105 (Conv2D)             (None, 14, 14, 384)  786432      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_109 (Conv2D)             (None, 14, 14, 384)  1548288     activation_90[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_90 (BatchNo (None, 14, 14, 384)  1152        conv2d_105[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_94 (BatchNo (None, 14, 14, 384)  1152        conv2d_109[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_87 (Activation)      (None, 14, 14, 384)  0           batch_normalization_90[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_91 (Activation)      (None, 14, 14, 384)  0           batch_normalization_94[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_106 (Conv2D)             (None, 14, 14, 384)  442368      activation_87[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_107 (Conv2D)             (None, 14, 14, 384)  442368      activation_87[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_110 (Conv2D)             (None, 14, 14, 384)  442368      activation_91[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_111 (Conv2D)             (None, 14, 14, 384)  442368      activation_91[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_9 (AveragePoo (None, 14, 14, 2048) 0           mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_104 (Conv2D)             (None, 14, 14, 320)  655360      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_91 (BatchNo (None, 14, 14, 384)  1152        conv2d_106[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_92 (BatchNo (None, 14, 14, 384)  1152        conv2d_107[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_95 (BatchNo (None, 14, 14, 384)  1152        conv2d_110[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_96 (BatchNo (None, 14, 14, 384)  1152        conv2d_111[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_112 (Conv2D)             (None, 14, 14, 192)  393216      average_pooling2d_9[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_89 (BatchNo (None, 14, 14, 320)  960         conv2d_104[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_88 (Activation)      (None, 14, 14, 384)  0           batch_normalization_91[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_89 (Activation)      (None, 14, 14, 384)  0           batch_normalization_92[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_92 (Activation)      (None, 14, 14, 384)  0           batch_normalization_95[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_93 (Activation)      (None, 14, 14, 384)  0           batch_normalization_96[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_97 (BatchNo (None, 14, 14, 192)  576         conv2d_112[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_86 (Activation)      (None, 14, 14, 320)  0           batch_normalization_89[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed9_1 (Concatenate)          (None, 14, 14, 768)  0           activation_88[0][0]              \n",
      "                                                                 activation_89[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 14, 14, 768)  0           activation_92[0][0]              \n",
      "                                                                 activation_93[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_94 (Activation)      (None, 14, 14, 192)  0           batch_normalization_97[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed10 (Concatenate)           (None, 14, 14, 2048) 0           activation_86[0][0]              \n",
      "                                                                 mixed9_1[0][0]                   \n",
      "                                                                 concatenate_2[0][0]              \n",
      "                                                                 activation_94[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_1 (Glo (None, 2048)         0           mixed10[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 1024)         2098176     global_average_pooling2d_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 5)            5125        dense_7[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 23,906,085\n",
      "Trainable params: 23,871,653\n",
      "Non-trainable params: 34,432\n",
      "__________________________________________________________________________________________________\n",
      "Train on 144 samples, validate on 37 samples\n",
      "Epoch 1/10\n",
      "144/144 [==============================] - 30s 211ms/step - loss: 3.4249 - acc: 0.3681 - val_loss: 4.8830 - val_acc: 0.2973\n",
      "Epoch 2/10\n",
      "144/144 [==============================] - 8s 52ms/step - loss: 1.8585 - acc: 0.4653 - val_loss: 10.8906 - val_acc: 0.3243\n",
      "Epoch 3/10\n",
      "144/144 [==============================] - 8s 53ms/step - loss: 1.9289 - acc: 0.4306 - val_loss: 6.7491 - val_acc: 0.3243\n",
      "Epoch 4/10\n",
      "144/144 [==============================] - 8s 54ms/step - loss: 1.6416 - acc: 0.4792 - val_loss: 5.4672 - val_acc: 0.3784\n",
      "Epoch 5/10\n",
      "144/144 [==============================] - 8s 54ms/step - loss: 1.4958 - acc: 0.4722 - val_loss: 6.2297 - val_acc: 0.3784\n",
      "Epoch 6/10\n",
      "144/144 [==============================] - 8s 55ms/step - loss: 2.0287 - acc: 0.4722 - val_loss: 10.0194 - val_acc: 0.3784\n",
      "Epoch 7/10\n",
      "144/144 [==============================] - 8s 54ms/step - loss: 1.7330 - acc: 0.4583 - val_loss: 10.0194 - val_acc: 0.3784\n",
      "Epoch 8/10\n",
      "144/144 [==============================] - 8s 53ms/step - loss: 1.7915 - acc: 0.4792 - val_loss: 9.6645 - val_acc: 0.3784\n",
      "Epoch 9/10\n",
      "144/144 [==============================] - 8s 53ms/step - loss: 1.7806 - acc: 0.4444 - val_loss: 10.0194 - val_acc: 0.3784\n",
      "Epoch 10/10\n",
      "144/144 [==============================] - 8s 53ms/step - loss: 1.8014 - acc: 0.4722 - val_loss: 10.0194 - val_acc: 0.3784\n"
     ]
    }
   ],
   "source": [
    "model = create_inception_model()\n",
    "loss, accuracy = train_inception_model (model, 20, 10, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "I9drqJh9bZ8Y",
    "outputId": "9b185a4d-897a-42a9-eb75-f9d81ea8c97a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 10.019356173437995\n",
      "Test accuracy: 0.37837837998931473\n"
     ]
    }
   ],
   "source": [
    "print('Test loss:', loss)\n",
    "print('Test accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "20x32ghIbgx4"
   },
   "source": [
    "Whether we use inception or not, we get 37.8% accuracy.... But the test loss is much higher with inception"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jTFgY-KjbpkR"
   },
   "source": [
    "## Try Bigger Batch Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2hSrHgq4b75l"
   },
   "source": [
    "### Regular 2D CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8jvFccGfYmY"
   },
   "source": [
    "50 Batch_Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1128
    },
    "colab_type": "code",
    "id": "aqqWgIhebnCC",
    "outputId": "a0dcebfb-7656-409e-b1dc-16b6b2f253e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_119 (Conv2D)          (None, 512, 512, 32)      832       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_29 (MaxPooling (None, 256, 256, 32)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_99 (Batc (None, 256, 256, 32)      128       \n",
      "_________________________________________________________________\n",
      "conv2d_120 (Conv2D)          (None, 256, 256, 64)      51264     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_30 (MaxPooling (None, 128, 128, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_121 (Conv2D)          (None, 128, 128, 64)      102464    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_31 (MaxPooling (None, 64, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_122 (Conv2D)          (None, 64, 64, 64)        102464    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_32 (MaxPooling (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_123 (Conv2D)          (None, 32, 32, 64)        102464    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_33 (MaxPooling (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_124 (Conv2D)          (None, 16, 16, 64)        102464    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_34 (MaxPooling (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1024)              4195328   \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 5)                 5125      \n",
      "=================================================================\n",
      "Total params: 4,662,533\n",
      "Trainable params: 4,662,469\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n",
      "Train on 144 samples, validate on 37 samples\n",
      "Epoch 1/10\n",
      "144/144 [==============================] - 18s 122ms/step - loss: 1.5341 - acc: 0.3542 - val_loss: 1.3702 - val_acc: 0.3784\n",
      "Epoch 2/10\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 1.3997 - acc: 0.4722 - val_loss: 1.4232 - val_acc: 0.3784\n",
      "Epoch 3/10\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 1.3095 - acc: 0.4653 - val_loss: 1.9604 - val_acc: 0.3784\n",
      "Epoch 4/10\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 1.3077 - acc: 0.4722 - val_loss: 1.4801 - val_acc: 0.3784\n",
      "Epoch 5/10\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 1.3058 - acc: 0.4722 - val_loss: 1.6681 - val_acc: 0.3784\n",
      "Epoch 6/10\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 1.3470 - acc: 0.4722 - val_loss: 1.6424 - val_acc: 0.3784\n",
      "Epoch 7/10\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 1.3006 - acc: 0.4792 - val_loss: 1.3597 - val_acc: 0.3784\n",
      "Epoch 8/10\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 1.3255 - acc: 0.4722 - val_loss: 1.3736 - val_acc: 0.3784\n",
      "Epoch 9/10\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 1.2891 - acc: 0.4722 - val_loss: 1.4699 - val_acc: 0.3784\n",
      "Epoch 10/10\n",
      "144/144 [==============================] - 2s 13ms/step - loss: 1.2981 - acc: 0.4722 - val_loss: 1.4601 - val_acc: 0.3784\n"
     ]
    }
   ],
   "source": [
    "model = create_simple_model()\n",
    "loss, accuracy = train_simple_model(model, 50, 10, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "dgpbd3qXfNsm",
    "outputId": "3b406efd-4f1a-4fa9-db91-44e461006891"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 1.4601407147742607\n",
      "Test accuracy: 0.37837838160025106\n"
     ]
    }
   ],
   "source": [
    "print('Test loss:', loss)\n",
    "print('Test accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "avWr6DWrfeOu"
   },
   "source": [
    "100 Batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1128
    },
    "colab_type": "code",
    "id": "23czhtlibfGP",
    "outputId": "f99ec050-72b8-47c1-d505-5fcdb8b4f29f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_125 (Conv2D)          (None, 512, 512, 32)      832       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_35 (MaxPooling (None, 256, 256, 32)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_100 (Bat (None, 256, 256, 32)      128       \n",
      "_________________________________________________________________\n",
      "conv2d_126 (Conv2D)          (None, 256, 256, 64)      51264     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_36 (MaxPooling (None, 128, 128, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_127 (Conv2D)          (None, 128, 128, 64)      102464    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_37 (MaxPooling (None, 64, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_128 (Conv2D)          (None, 64, 64, 64)        102464    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_38 (MaxPooling (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_129 (Conv2D)          (None, 32, 32, 64)        102464    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_39 (MaxPooling (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_130 (Conv2D)          (None, 16, 16, 64)        102464    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_40 (MaxPooling (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 1024)              4195328   \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 5)                 5125      \n",
      "=================================================================\n",
      "Total params: 4,662,533\n",
      "Trainable params: 4,662,469\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n",
      "Train on 144 samples, validate on 37 samples\n",
      "Epoch 1/10\n",
      "144/144 [==============================] - 15s 102ms/step - loss: 1.5978 - acc: 0.2222 - val_loss: 1.4874 - val_acc: 0.3784\n",
      "Epoch 2/10\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 2.1632 - acc: 0.4722 - val_loss: 1.9838 - val_acc: 0.3784\n",
      "Epoch 3/10\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.3421 - acc: 0.4514 - val_loss: 1.4449 - val_acc: 0.3243\n",
      "Epoch 4/10\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.5194 - acc: 0.2917 - val_loss: 1.5253 - val_acc: 0.3243\n",
      "Epoch 5/10\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.5555 - acc: 0.2917 - val_loss: 1.5307 - val_acc: 0.3243\n",
      "Epoch 6/10\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.5494 - acc: 0.2917 - val_loss: 1.5143 - val_acc: 0.3243\n",
      "Epoch 7/10\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.5334 - acc: 0.2917 - val_loss: 1.4849 - val_acc: 0.3243\n",
      "Epoch 8/10\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.5004 - acc: 0.2917 - val_loss: 1.4450 - val_acc: 0.3243\n",
      "Epoch 9/10\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.4597 - acc: 0.2917 - val_loss: 1.3935 - val_acc: 0.3243\n",
      "Epoch 10/10\n",
      "144/144 [==============================] - 2s 14ms/step - loss: 1.3925 - acc: 0.2917 - val_loss: 1.3512 - val_acc: 0.3243\n"
     ]
    }
   ],
   "source": [
    "model = create_simple_model()\n",
    "loss, accuracy = train_simple_model(model, 100, 10, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "_hbUZLZ3fWie",
    "outputId": "22cf6973-2c2a-449e-a925-28a083f699c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 1.3511938339955099\n",
      "Test accuracy: 0.3243243251297925\n"
     ]
    }
   ],
   "source": [
    "print('Test loss:', loss)\n",
    "print('Test accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X4Q1e9QjczsO"
   },
   "source": [
    "This is really not getting higher than 37.8%. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O26nl7biWt8e"
   },
   "source": [
    "## Try Adding Some Data Augmentation to the Mix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O_9aaVjXsCFG"
   },
   "outputs": [],
   "source": [
    "# augment the images with rotation, width shifts, and height shifts\n",
    "# only augment the train data\n",
    "def create_generators(x_train, y_train, x_test, y_test, batch_size, rot, width, height):\n",
    "    train_d_gen = ImageDataGenerator(\n",
    "        rotation_range=rot,\n",
    "        width_shift_range=width,\n",
    "        height_shift_range=height,\n",
    "        horizontal_flip=True)\n",
    "    \n",
    "    train_d_gen.fit(x_train)\n",
    "    \n",
    "    train_gen = train_d_gen.flow(x_train, y_train, batch_size)\n",
    "    \n",
    "    validation_gen = ImageDataGenerator().flow(x_test, y_test, batch_size)\n",
    "    \n",
    "    return train_gen, validation_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "trtunNxfWtJP"
   },
   "outputs": [],
   "source": [
    "img_rows,img_cols=512,512\n",
    "def train_simple_gen_model (model, batch_size, epochs, x, y, xt, yt, rot=20, width=0.2, height=0.2):\n",
    "  # add a channel\n",
    "  x = x.reshape(x.shape[0], img_rows, img_cols, 1)\n",
    "  xt = xt.reshape(xt.shape[0], img_rows, img_cols, 1)\n",
    "  # augment\n",
    "  train_generator, validation_generator = create_generators(x, y, xt, yt, batch_size, rot, width, height)\n",
    " \n",
    "  # train\n",
    "  model.compile(\n",
    "    loss=categorical_crossentropy,\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "  model.fit_generator(\n",
    "        train_generator,\n",
    "        epochs=10,\n",
    "        verbose=1,\n",
    "        steps_per_epoch = np.ceil(x.shape[0]/batch_size),\n",
    "        validation_steps = np.ceil(xt.shape[0]/batch_size),\n",
    "        validation_data=validation_generator)\n",
    "  # evaluate\n",
    "  score = model.evaluate(xt, yt, verbose=0)\n",
    "  return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "21pzu0AqiaPR"
   },
   "outputs": [],
   "source": [
    "def train_inception_gen_model(model, batch_size, epochs, x, y, xt, yt, rot=20, width=0.2, height=0.2):\n",
    "  # force images to have three channels\n",
    "  xnew = np.zeros((x.shape[0], 512, 512, 3))\n",
    "  for i in range(0, x.shape[0]):\n",
    "      xnew[i, :, :, 0] = x[i]\n",
    "      xnew[i, :, :, 1] = x[i]\n",
    "      xnew[i, :, :, 2] = x[i]\n",
    "\n",
    "  xtnew = np.zeros((xt.shape[0], 512, 512, 3))\n",
    "  for i in range(0, xt.shape[0]):\n",
    "      xtnew[i, :, :, 0] = xt[i]\n",
    "      xtnew[i, :, :, 1] = xt[i]\n",
    "      xtnew[i, :, :, 2] = xt[i]\n",
    " \n",
    "  # augment\n",
    "  train_generator, validation_generator = create_generators(xnew, y, xtnew, yt, batch_size, rot, width, height)\n",
    "\n",
    "  rmsprop = keras.optimizers.RMSprop(lr=0.005)\n",
    "  \n",
    "  #train\n",
    "  model.compile(\n",
    "    loss=categorical_crossentropy,\n",
    "    optimizer=rmsprop,\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "  model.fit_generator(\n",
    "        train_generator,\n",
    "        epochs=epochs,\n",
    "        verbose=1,\n",
    "        steps_per_epoch = np.ceil(x.shape[0]/batch_size),\n",
    "        validation_steps = np.ceil(xt.shape[0]/batch_size),\n",
    "        validation_data=validation_generator)\n",
    "  # evaluate\n",
    "  score = model.evaluate(xtnew, yt, verbose=0)\n",
    "  return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "flrwRL7pv6RC"
   },
   "source": [
    "### Simple 2D Conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1111
    },
    "colab_type": "code",
    "id": "YCPWHVOA2aUY",
    "outputId": "340548f4-3afb-4571-f524-645fea73ae0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_219 (Conv2D)          (None, 512, 512, 32)      832       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_39 (MaxPooling (None, 256, 256, 32)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_194 (Bat (None, 256, 256, 32)      128       \n",
      "_________________________________________________________________\n",
      "conv2d_220 (Conv2D)          (None, 256, 256, 64)      51264     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_40 (MaxPooling (None, 128, 128, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_221 (Conv2D)          (None, 128, 128, 64)      102464    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_41 (MaxPooling (None, 64, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_222 (Conv2D)          (None, 64, 64, 64)        102464    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_42 (MaxPooling (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_223 (Conv2D)          (None, 32, 32, 64)        102464    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_43 (MaxPooling (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_224 (Conv2D)          (None, 16, 16, 64)        102464    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_44 (MaxPooling (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 1024)              4195328   \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 5)                 5125      \n",
      "=================================================================\n",
      "Total params: 4,662,533\n",
      "Trainable params: 4,662,469\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "8/8 [==============================] - 11s 1s/step - loss: 1.6701 - acc: 0.3006 - val_loss: 1.5606 - val_acc: 0.3243\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 2s 249ms/step - loss: 1.4091 - acc: 0.2880 - val_loss: 1.6677 - val_acc: 0.3243\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - 2s 253ms/step - loss: 1.4028 - acc: 0.4367 - val_loss: 1.3610 - val_acc: 0.3784\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - 2s 249ms/step - loss: 1.3123 - acc: 0.4747 - val_loss: 1.3929 - val_acc: 0.3784\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - 2s 280ms/step - loss: 1.3064 - acc: 0.4525 - val_loss: 1.3646 - val_acc: 0.3784\n",
      "Epoch 6/10\n",
      "8/8 [==============================] - 3s 342ms/step - loss: 1.2649 - acc: 0.4747 - val_loss: 1.3538 - val_acc: 0.3784\n",
      "Epoch 7/10\n",
      "8/8 [==============================] - 3s 338ms/step - loss: 1.3004 - acc: 0.4525 - val_loss: 1.3644 - val_acc: 0.3784\n",
      "Epoch 8/10\n",
      "8/8 [==============================] - 3s 339ms/step - loss: 1.2645 - acc: 0.5191 - val_loss: 1.3479 - val_acc: 0.3784\n",
      "Epoch 9/10\n",
      "8/8 [==============================] - 3s 338ms/step - loss: 1.3220 - acc: 0.4525 - val_loss: 1.3692 - val_acc: 0.3784\n",
      "Epoch 10/10\n",
      "8/8 [==============================] - 3s 339ms/step - loss: 1.3613 - acc: 0.4747 - val_loss: 1.3502 - val_acc: 0.3784\n"
     ]
    }
   ],
   "source": [
    "model = create_simple_model()\n",
    "loss, accuracy = train_simple_gen_model(model, 20, 10, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "lxVLnXE_2qLw",
    "outputId": "87412e4d-51d1-4ccb-8bfb-03191f0c144b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.3501889866751593\n",
      "Test Accuracy: 0.37837838160025106\n"
     ]
    }
   ],
   "source": [
    "print('Test Loss:', loss)\n",
    "print('Test Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M7hZsNCMv9Xz"
   },
   "source": [
    "### Inception Conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 12181
    },
    "colab_type": "code",
    "id": "7ZgfkyixAEGr",
    "outputId": "7d27d728-86ce-495b-b06b-e46e665b52f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 512, 512, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_343 (Conv2D)             (None, 255, 255, 32) 864         input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_293 (BatchN (None, 255, 255, 32) 96          conv2d_343[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_283 (Activation)     (None, 255, 255, 32) 0           batch_normalization_293[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_344 (Conv2D)             (None, 253, 253, 32) 9216        activation_283[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_294 (BatchN (None, 253, 253, 32) 96          conv2d_344[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_284 (Activation)     (None, 253, 253, 32) 0           batch_normalization_294[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_345 (Conv2D)             (None, 253, 253, 64) 18432       activation_284[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_295 (BatchN (None, 253, 253, 64) 192         conv2d_345[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_285 (Activation)     (None, 253, 253, 64) 0           batch_normalization_295[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_73 (MaxPooling2D) (None, 126, 126, 64) 0           activation_285[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_346 (Conv2D)             (None, 126, 126, 80) 5120        max_pooling2d_73[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_296 (BatchN (None, 126, 126, 80) 240         conv2d_346[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_286 (Activation)     (None, 126, 126, 80) 0           batch_normalization_296[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_347 (Conv2D)             (None, 124, 124, 192 138240      activation_286[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_297 (BatchN (None, 124, 124, 192 576         conv2d_347[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_287 (Activation)     (None, 124, 124, 192 0           batch_normalization_297[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_74 (MaxPooling2D) (None, 61, 61, 192)  0           activation_287[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_351 (Conv2D)             (None, 61, 61, 64)   12288       max_pooling2d_74[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_301 (BatchN (None, 61, 61, 64)   192         conv2d_351[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_291 (Activation)     (None, 61, 61, 64)   0           batch_normalization_301[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_349 (Conv2D)             (None, 61, 61, 48)   9216        max_pooling2d_74[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_352 (Conv2D)             (None, 61, 61, 96)   55296       activation_291[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_299 (BatchN (None, 61, 61, 48)   144         conv2d_349[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_302 (BatchN (None, 61, 61, 96)   288         conv2d_352[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_289 (Activation)     (None, 61, 61, 48)   0           batch_normalization_299[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_292 (Activation)     (None, 61, 61, 96)   0           batch_normalization_302[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_28 (AveragePo (None, 61, 61, 192)  0           max_pooling2d_74[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_348 (Conv2D)             (None, 61, 61, 64)   12288       max_pooling2d_74[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_350 (Conv2D)             (None, 61, 61, 64)   76800       activation_289[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_353 (Conv2D)             (None, 61, 61, 96)   82944       activation_292[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_354 (Conv2D)             (None, 61, 61, 32)   6144        average_pooling2d_28[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_298 (BatchN (None, 61, 61, 64)   192         conv2d_348[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_300 (BatchN (None, 61, 61, 64)   192         conv2d_350[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_303 (BatchN (None, 61, 61, 96)   288         conv2d_353[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_304 (BatchN (None, 61, 61, 32)   96          conv2d_354[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_288 (Activation)     (None, 61, 61, 64)   0           batch_normalization_298[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_290 (Activation)     (None, 61, 61, 64)   0           batch_normalization_300[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_293 (Activation)     (None, 61, 61, 96)   0           batch_normalization_303[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_294 (Activation)     (None, 61, 61, 32)   0           batch_normalization_304[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed0 (Concatenate)            (None, 61, 61, 256)  0           activation_288[0][0]             \n",
      "                                                                 activation_290[0][0]             \n",
      "                                                                 activation_293[0][0]             \n",
      "                                                                 activation_294[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_358 (Conv2D)             (None, 61, 61, 64)   16384       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_308 (BatchN (None, 61, 61, 64)   192         conv2d_358[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_298 (Activation)     (None, 61, 61, 64)   0           batch_normalization_308[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_356 (Conv2D)             (None, 61, 61, 48)   12288       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_359 (Conv2D)             (None, 61, 61, 96)   55296       activation_298[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_306 (BatchN (None, 61, 61, 48)   144         conv2d_356[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_309 (BatchN (None, 61, 61, 96)   288         conv2d_359[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_296 (Activation)     (None, 61, 61, 48)   0           batch_normalization_306[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_299 (Activation)     (None, 61, 61, 96)   0           batch_normalization_309[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_29 (AveragePo (None, 61, 61, 256)  0           mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_355 (Conv2D)             (None, 61, 61, 64)   16384       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_357 (Conv2D)             (None, 61, 61, 64)   76800       activation_296[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_360 (Conv2D)             (None, 61, 61, 96)   82944       activation_299[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_361 (Conv2D)             (None, 61, 61, 64)   16384       average_pooling2d_29[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_305 (BatchN (None, 61, 61, 64)   192         conv2d_355[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_307 (BatchN (None, 61, 61, 64)   192         conv2d_357[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_310 (BatchN (None, 61, 61, 96)   288         conv2d_360[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_311 (BatchN (None, 61, 61, 64)   192         conv2d_361[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_295 (Activation)     (None, 61, 61, 64)   0           batch_normalization_305[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_297 (Activation)     (None, 61, 61, 64)   0           batch_normalization_307[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_300 (Activation)     (None, 61, 61, 96)   0           batch_normalization_310[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_301 (Activation)     (None, 61, 61, 64)   0           batch_normalization_311[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed1 (Concatenate)            (None, 61, 61, 288)  0           activation_295[0][0]             \n",
      "                                                                 activation_297[0][0]             \n",
      "                                                                 activation_300[0][0]             \n",
      "                                                                 activation_301[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_365 (Conv2D)             (None, 61, 61, 64)   18432       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_315 (BatchN (None, 61, 61, 64)   192         conv2d_365[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_305 (Activation)     (None, 61, 61, 64)   0           batch_normalization_315[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_363 (Conv2D)             (None, 61, 61, 48)   13824       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_366 (Conv2D)             (None, 61, 61, 96)   55296       activation_305[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_313 (BatchN (None, 61, 61, 48)   144         conv2d_363[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_316 (BatchN (None, 61, 61, 96)   288         conv2d_366[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_303 (Activation)     (None, 61, 61, 48)   0           batch_normalization_313[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_306 (Activation)     (None, 61, 61, 96)   0           batch_normalization_316[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_30 (AveragePo (None, 61, 61, 288)  0           mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_362 (Conv2D)             (None, 61, 61, 64)   18432       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_364 (Conv2D)             (None, 61, 61, 64)   76800       activation_303[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_367 (Conv2D)             (None, 61, 61, 96)   82944       activation_306[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_368 (Conv2D)             (None, 61, 61, 64)   18432       average_pooling2d_30[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_312 (BatchN (None, 61, 61, 64)   192         conv2d_362[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_314 (BatchN (None, 61, 61, 64)   192         conv2d_364[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_317 (BatchN (None, 61, 61, 96)   288         conv2d_367[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_318 (BatchN (None, 61, 61, 64)   192         conv2d_368[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_302 (Activation)     (None, 61, 61, 64)   0           batch_normalization_312[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_304 (Activation)     (None, 61, 61, 64)   0           batch_normalization_314[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_307 (Activation)     (None, 61, 61, 96)   0           batch_normalization_317[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_308 (Activation)     (None, 61, 61, 64)   0           batch_normalization_318[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed2 (Concatenate)            (None, 61, 61, 288)  0           activation_302[0][0]             \n",
      "                                                                 activation_304[0][0]             \n",
      "                                                                 activation_307[0][0]             \n",
      "                                                                 activation_308[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_370 (Conv2D)             (None, 61, 61, 64)   18432       mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_320 (BatchN (None, 61, 61, 64)   192         conv2d_370[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_310 (Activation)     (None, 61, 61, 64)   0           batch_normalization_320[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_371 (Conv2D)             (None, 61, 61, 96)   55296       activation_310[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_321 (BatchN (None, 61, 61, 96)   288         conv2d_371[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_311 (Activation)     (None, 61, 61, 96)   0           batch_normalization_321[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_369 (Conv2D)             (None, 30, 30, 384)  995328      mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_372 (Conv2D)             (None, 30, 30, 96)   82944       activation_311[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_319 (BatchN (None, 30, 30, 384)  1152        conv2d_369[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_322 (BatchN (None, 30, 30, 96)   288         conv2d_372[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_309 (Activation)     (None, 30, 30, 384)  0           batch_normalization_319[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_312 (Activation)     (None, 30, 30, 96)   0           batch_normalization_322[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_75 (MaxPooling2D) (None, 30, 30, 288)  0           mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "mixed3 (Concatenate)            (None, 30, 30, 768)  0           activation_309[0][0]             \n",
      "                                                                 activation_312[0][0]             \n",
      "                                                                 max_pooling2d_75[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_377 (Conv2D)             (None, 30, 30, 128)  98304       mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_327 (BatchN (None, 30, 30, 128)  384         conv2d_377[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_317 (Activation)     (None, 30, 30, 128)  0           batch_normalization_327[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_378 (Conv2D)             (None, 30, 30, 128)  114688      activation_317[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_328 (BatchN (None, 30, 30, 128)  384         conv2d_378[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_318 (Activation)     (None, 30, 30, 128)  0           batch_normalization_328[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_374 (Conv2D)             (None, 30, 30, 128)  98304       mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_379 (Conv2D)             (None, 30, 30, 128)  114688      activation_318[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_324 (BatchN (None, 30, 30, 128)  384         conv2d_374[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_329 (BatchN (None, 30, 30, 128)  384         conv2d_379[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_314 (Activation)     (None, 30, 30, 128)  0           batch_normalization_324[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_319 (Activation)     (None, 30, 30, 128)  0           batch_normalization_329[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_375 (Conv2D)             (None, 30, 30, 128)  114688      activation_314[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_380 (Conv2D)             (None, 30, 30, 128)  114688      activation_319[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_325 (BatchN (None, 30, 30, 128)  384         conv2d_375[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_330 (BatchN (None, 30, 30, 128)  384         conv2d_380[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_315 (Activation)     (None, 30, 30, 128)  0           batch_normalization_325[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_320 (Activation)     (None, 30, 30, 128)  0           batch_normalization_330[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_31 (AveragePo (None, 30, 30, 768)  0           mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_373 (Conv2D)             (None, 30, 30, 192)  147456      mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_376 (Conv2D)             (None, 30, 30, 192)  172032      activation_315[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_381 (Conv2D)             (None, 30, 30, 192)  172032      activation_320[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_382 (Conv2D)             (None, 30, 30, 192)  147456      average_pooling2d_31[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_323 (BatchN (None, 30, 30, 192)  576         conv2d_373[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_326 (BatchN (None, 30, 30, 192)  576         conv2d_376[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_331 (BatchN (None, 30, 30, 192)  576         conv2d_381[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_332 (BatchN (None, 30, 30, 192)  576         conv2d_382[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_313 (Activation)     (None, 30, 30, 192)  0           batch_normalization_323[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_316 (Activation)     (None, 30, 30, 192)  0           batch_normalization_326[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_321 (Activation)     (None, 30, 30, 192)  0           batch_normalization_331[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_322 (Activation)     (None, 30, 30, 192)  0           batch_normalization_332[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed4 (Concatenate)            (None, 30, 30, 768)  0           activation_313[0][0]             \n",
      "                                                                 activation_316[0][0]             \n",
      "                                                                 activation_321[0][0]             \n",
      "                                                                 activation_322[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_387 (Conv2D)             (None, 30, 30, 160)  122880      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_337 (BatchN (None, 30, 30, 160)  480         conv2d_387[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_327 (Activation)     (None, 30, 30, 160)  0           batch_normalization_337[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_388 (Conv2D)             (None, 30, 30, 160)  179200      activation_327[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_338 (BatchN (None, 30, 30, 160)  480         conv2d_388[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_328 (Activation)     (None, 30, 30, 160)  0           batch_normalization_338[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_384 (Conv2D)             (None, 30, 30, 160)  122880      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_389 (Conv2D)             (None, 30, 30, 160)  179200      activation_328[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_334 (BatchN (None, 30, 30, 160)  480         conv2d_384[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_339 (BatchN (None, 30, 30, 160)  480         conv2d_389[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_324 (Activation)     (None, 30, 30, 160)  0           batch_normalization_334[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_329 (Activation)     (None, 30, 30, 160)  0           batch_normalization_339[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_385 (Conv2D)             (None, 30, 30, 160)  179200      activation_324[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_390 (Conv2D)             (None, 30, 30, 160)  179200      activation_329[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_335 (BatchN (None, 30, 30, 160)  480         conv2d_385[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_340 (BatchN (None, 30, 30, 160)  480         conv2d_390[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_325 (Activation)     (None, 30, 30, 160)  0           batch_normalization_335[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_330 (Activation)     (None, 30, 30, 160)  0           batch_normalization_340[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_32 (AveragePo (None, 30, 30, 768)  0           mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_383 (Conv2D)             (None, 30, 30, 192)  147456      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_386 (Conv2D)             (None, 30, 30, 192)  215040      activation_325[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_391 (Conv2D)             (None, 30, 30, 192)  215040      activation_330[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_392 (Conv2D)             (None, 30, 30, 192)  147456      average_pooling2d_32[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_333 (BatchN (None, 30, 30, 192)  576         conv2d_383[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_336 (BatchN (None, 30, 30, 192)  576         conv2d_386[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_341 (BatchN (None, 30, 30, 192)  576         conv2d_391[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_342 (BatchN (None, 30, 30, 192)  576         conv2d_392[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_323 (Activation)     (None, 30, 30, 192)  0           batch_normalization_333[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_326 (Activation)     (None, 30, 30, 192)  0           batch_normalization_336[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_331 (Activation)     (None, 30, 30, 192)  0           batch_normalization_341[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_332 (Activation)     (None, 30, 30, 192)  0           batch_normalization_342[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed5 (Concatenate)            (None, 30, 30, 768)  0           activation_323[0][0]             \n",
      "                                                                 activation_326[0][0]             \n",
      "                                                                 activation_331[0][0]             \n",
      "                                                                 activation_332[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_397 (Conv2D)             (None, 30, 30, 160)  122880      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_347 (BatchN (None, 30, 30, 160)  480         conv2d_397[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_337 (Activation)     (None, 30, 30, 160)  0           batch_normalization_347[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_398 (Conv2D)             (None, 30, 30, 160)  179200      activation_337[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_348 (BatchN (None, 30, 30, 160)  480         conv2d_398[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_338 (Activation)     (None, 30, 30, 160)  0           batch_normalization_348[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_394 (Conv2D)             (None, 30, 30, 160)  122880      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_399 (Conv2D)             (None, 30, 30, 160)  179200      activation_338[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_344 (BatchN (None, 30, 30, 160)  480         conv2d_394[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_349 (BatchN (None, 30, 30, 160)  480         conv2d_399[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_334 (Activation)     (None, 30, 30, 160)  0           batch_normalization_344[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_339 (Activation)     (None, 30, 30, 160)  0           batch_normalization_349[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_395 (Conv2D)             (None, 30, 30, 160)  179200      activation_334[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_400 (Conv2D)             (None, 30, 30, 160)  179200      activation_339[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_345 (BatchN (None, 30, 30, 160)  480         conv2d_395[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_350 (BatchN (None, 30, 30, 160)  480         conv2d_400[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_335 (Activation)     (None, 30, 30, 160)  0           batch_normalization_345[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_340 (Activation)     (None, 30, 30, 160)  0           batch_normalization_350[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_33 (AveragePo (None, 30, 30, 768)  0           mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_393 (Conv2D)             (None, 30, 30, 192)  147456      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_396 (Conv2D)             (None, 30, 30, 192)  215040      activation_335[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_401 (Conv2D)             (None, 30, 30, 192)  215040      activation_340[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_402 (Conv2D)             (None, 30, 30, 192)  147456      average_pooling2d_33[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_343 (BatchN (None, 30, 30, 192)  576         conv2d_393[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_346 (BatchN (None, 30, 30, 192)  576         conv2d_396[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_351 (BatchN (None, 30, 30, 192)  576         conv2d_401[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_352 (BatchN (None, 30, 30, 192)  576         conv2d_402[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_333 (Activation)     (None, 30, 30, 192)  0           batch_normalization_343[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_336 (Activation)     (None, 30, 30, 192)  0           batch_normalization_346[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_341 (Activation)     (None, 30, 30, 192)  0           batch_normalization_351[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_342 (Activation)     (None, 30, 30, 192)  0           batch_normalization_352[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed6 (Concatenate)            (None, 30, 30, 768)  0           activation_333[0][0]             \n",
      "                                                                 activation_336[0][0]             \n",
      "                                                                 activation_341[0][0]             \n",
      "                                                                 activation_342[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_407 (Conv2D)             (None, 30, 30, 192)  147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_357 (BatchN (None, 30, 30, 192)  576         conv2d_407[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_347 (Activation)     (None, 30, 30, 192)  0           batch_normalization_357[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_408 (Conv2D)             (None, 30, 30, 192)  258048      activation_347[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_358 (BatchN (None, 30, 30, 192)  576         conv2d_408[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_348 (Activation)     (None, 30, 30, 192)  0           batch_normalization_358[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_404 (Conv2D)             (None, 30, 30, 192)  147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_409 (Conv2D)             (None, 30, 30, 192)  258048      activation_348[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_354 (BatchN (None, 30, 30, 192)  576         conv2d_404[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_359 (BatchN (None, 30, 30, 192)  576         conv2d_409[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_344 (Activation)     (None, 30, 30, 192)  0           batch_normalization_354[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_349 (Activation)     (None, 30, 30, 192)  0           batch_normalization_359[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_405 (Conv2D)             (None, 30, 30, 192)  258048      activation_344[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_410 (Conv2D)             (None, 30, 30, 192)  258048      activation_349[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_355 (BatchN (None, 30, 30, 192)  576         conv2d_405[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_360 (BatchN (None, 30, 30, 192)  576         conv2d_410[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_345 (Activation)     (None, 30, 30, 192)  0           batch_normalization_355[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_350 (Activation)     (None, 30, 30, 192)  0           batch_normalization_360[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_34 (AveragePo (None, 30, 30, 768)  0           mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_403 (Conv2D)             (None, 30, 30, 192)  147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_406 (Conv2D)             (None, 30, 30, 192)  258048      activation_345[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_411 (Conv2D)             (None, 30, 30, 192)  258048      activation_350[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_412 (Conv2D)             (None, 30, 30, 192)  147456      average_pooling2d_34[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_353 (BatchN (None, 30, 30, 192)  576         conv2d_403[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_356 (BatchN (None, 30, 30, 192)  576         conv2d_406[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_361 (BatchN (None, 30, 30, 192)  576         conv2d_411[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_362 (BatchN (None, 30, 30, 192)  576         conv2d_412[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_343 (Activation)     (None, 30, 30, 192)  0           batch_normalization_353[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_346 (Activation)     (None, 30, 30, 192)  0           batch_normalization_356[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_351 (Activation)     (None, 30, 30, 192)  0           batch_normalization_361[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_352 (Activation)     (None, 30, 30, 192)  0           batch_normalization_362[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed7 (Concatenate)            (None, 30, 30, 768)  0           activation_343[0][0]             \n",
      "                                                                 activation_346[0][0]             \n",
      "                                                                 activation_351[0][0]             \n",
      "                                                                 activation_352[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_415 (Conv2D)             (None, 30, 30, 192)  147456      mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_365 (BatchN (None, 30, 30, 192)  576         conv2d_415[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_355 (Activation)     (None, 30, 30, 192)  0           batch_normalization_365[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_416 (Conv2D)             (None, 30, 30, 192)  258048      activation_355[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_366 (BatchN (None, 30, 30, 192)  576         conv2d_416[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_356 (Activation)     (None, 30, 30, 192)  0           batch_normalization_366[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_413 (Conv2D)             (None, 30, 30, 192)  147456      mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_417 (Conv2D)             (None, 30, 30, 192)  258048      activation_356[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_363 (BatchN (None, 30, 30, 192)  576         conv2d_413[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_367 (BatchN (None, 30, 30, 192)  576         conv2d_417[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_353 (Activation)     (None, 30, 30, 192)  0           batch_normalization_363[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_357 (Activation)     (None, 30, 30, 192)  0           batch_normalization_367[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_414 (Conv2D)             (None, 14, 14, 320)  552960      activation_353[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_418 (Conv2D)             (None, 14, 14, 192)  331776      activation_357[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_364 (BatchN (None, 14, 14, 320)  960         conv2d_414[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_368 (BatchN (None, 14, 14, 192)  576         conv2d_418[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_354 (Activation)     (None, 14, 14, 320)  0           batch_normalization_364[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_358 (Activation)     (None, 14, 14, 192)  0           batch_normalization_368[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_76 (MaxPooling2D) (None, 14, 14, 768)  0           mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "mixed8 (Concatenate)            (None, 14, 14, 1280) 0           activation_354[0][0]             \n",
      "                                                                 activation_358[0][0]             \n",
      "                                                                 max_pooling2d_76[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_423 (Conv2D)             (None, 14, 14, 448)  573440      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_373 (BatchN (None, 14, 14, 448)  1344        conv2d_423[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_363 (Activation)     (None, 14, 14, 448)  0           batch_normalization_373[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_420 (Conv2D)             (None, 14, 14, 384)  491520      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_424 (Conv2D)             (None, 14, 14, 384)  1548288     activation_363[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_370 (BatchN (None, 14, 14, 384)  1152        conv2d_420[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_374 (BatchN (None, 14, 14, 384)  1152        conv2d_424[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_360 (Activation)     (None, 14, 14, 384)  0           batch_normalization_370[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_364 (Activation)     (None, 14, 14, 384)  0           batch_normalization_374[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_421 (Conv2D)             (None, 14, 14, 384)  442368      activation_360[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_422 (Conv2D)             (None, 14, 14, 384)  442368      activation_360[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_425 (Conv2D)             (None, 14, 14, 384)  442368      activation_364[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_426 (Conv2D)             (None, 14, 14, 384)  442368      activation_364[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_35 (AveragePo (None, 14, 14, 1280) 0           mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_419 (Conv2D)             (None, 14, 14, 320)  409600      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_371 (BatchN (None, 14, 14, 384)  1152        conv2d_421[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_372 (BatchN (None, 14, 14, 384)  1152        conv2d_422[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_375 (BatchN (None, 14, 14, 384)  1152        conv2d_425[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_376 (BatchN (None, 14, 14, 384)  1152        conv2d_426[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_427 (Conv2D)             (None, 14, 14, 192)  245760      average_pooling2d_35[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_369 (BatchN (None, 14, 14, 320)  960         conv2d_419[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_361 (Activation)     (None, 14, 14, 384)  0           batch_normalization_371[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_362 (Activation)     (None, 14, 14, 384)  0           batch_normalization_372[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_365 (Activation)     (None, 14, 14, 384)  0           batch_normalization_375[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_366 (Activation)     (None, 14, 14, 384)  0           batch_normalization_376[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_377 (BatchN (None, 14, 14, 192)  576         conv2d_427[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_359 (Activation)     (None, 14, 14, 320)  0           batch_normalization_369[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed9_0 (Concatenate)          (None, 14, 14, 768)  0           activation_361[0][0]             \n",
      "                                                                 activation_362[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 14, 14, 768)  0           activation_365[0][0]             \n",
      "                                                                 activation_366[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_367 (Activation)     (None, 14, 14, 192)  0           batch_normalization_377[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed9 (Concatenate)            (None, 14, 14, 2048) 0           activation_359[0][0]             \n",
      "                                                                 mixed9_0[0][0]                   \n",
      "                                                                 concatenate_7[0][0]              \n",
      "                                                                 activation_367[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_432 (Conv2D)             (None, 14, 14, 448)  917504      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_382 (BatchN (None, 14, 14, 448)  1344        conv2d_432[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_372 (Activation)     (None, 14, 14, 448)  0           batch_normalization_382[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_429 (Conv2D)             (None, 14, 14, 384)  786432      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_433 (Conv2D)             (None, 14, 14, 384)  1548288     activation_372[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_379 (BatchN (None, 14, 14, 384)  1152        conv2d_429[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_383 (BatchN (None, 14, 14, 384)  1152        conv2d_433[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_369 (Activation)     (None, 14, 14, 384)  0           batch_normalization_379[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_373 (Activation)     (None, 14, 14, 384)  0           batch_normalization_383[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_430 (Conv2D)             (None, 14, 14, 384)  442368      activation_369[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_431 (Conv2D)             (None, 14, 14, 384)  442368      activation_369[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_434 (Conv2D)             (None, 14, 14, 384)  442368      activation_373[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_435 (Conv2D)             (None, 14, 14, 384)  442368      activation_373[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_36 (AveragePo (None, 14, 14, 2048) 0           mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_428 (Conv2D)             (None, 14, 14, 320)  655360      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_380 (BatchN (None, 14, 14, 384)  1152        conv2d_430[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_381 (BatchN (None, 14, 14, 384)  1152        conv2d_431[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_384 (BatchN (None, 14, 14, 384)  1152        conv2d_434[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_385 (BatchN (None, 14, 14, 384)  1152        conv2d_435[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_436 (Conv2D)             (None, 14, 14, 192)  393216      average_pooling2d_36[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_378 (BatchN (None, 14, 14, 320)  960         conv2d_428[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_370 (Activation)     (None, 14, 14, 384)  0           batch_normalization_380[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_371 (Activation)     (None, 14, 14, 384)  0           batch_normalization_381[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_374 (Activation)     (None, 14, 14, 384)  0           batch_normalization_384[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_375 (Activation)     (None, 14, 14, 384)  0           batch_normalization_385[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_386 (BatchN (None, 14, 14, 192)  576         conv2d_436[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_368 (Activation)     (None, 14, 14, 320)  0           batch_normalization_378[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed9_1 (Concatenate)          (None, 14, 14, 768)  0           activation_370[0][0]             \n",
      "                                                                 activation_371[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 14, 14, 768)  0           activation_374[0][0]             \n",
      "                                                                 activation_375[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_376 (Activation)     (None, 14, 14, 192)  0           batch_normalization_386[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed10 (Concatenate)           (None, 14, 14, 2048) 0           activation_368[0][0]             \n",
      "                                                                 mixed9_1[0][0]                   \n",
      "                                                                 concatenate_8[0][0]              \n",
      "                                                                 activation_376[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_4 (Glo (None, 2048)         0           mixed10[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_27 (Dense)                (None, 1024)         2098176     global_average_pooling2d_4[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_28 (Dense)                (None, 5)            5125        dense_27[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 23,906,085\n",
      "Trainable params: 23,871,653\n",
      "Non-trainable params: 34,432\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/10\n",
      "8/8 [==============================] - 30s 4s/step - loss: 3.3751 - acc: 0.3449 - val_loss: 8.4226 - val_acc: 0.3243\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 7s 881ms/step - loss: 2.3495 - acc: 0.4526 - val_loss: 6.8525 - val_acc: 0.2973\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - 7s 891ms/step - loss: 2.0806 - acc: 0.4620 - val_loss: 5.1737 - val_acc: 0.3243\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - 7s 896ms/step - loss: 2.4792 - acc: 0.3480 - val_loss: 1.7262 - val_acc: 0.3784\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - 8s 946ms/step - loss: 2.3903 - acc: 0.4208 - val_loss: 1.6835 - val_acc: 0.3784\n",
      "Epoch 6/10\n",
      "8/8 [==============================] - 9s 1s/step - loss: 2.1826 - acc: 0.4747 - val_loss: 1.7536 - val_acc: 0.3784\n",
      "Epoch 7/10\n",
      "8/8 [==============================] - 9s 1s/step - loss: 2.5097 - acc: 0.4272 - val_loss: 9.8341 - val_acc: 0.3243\n",
      "Epoch 8/10\n",
      "8/8 [==============================] - 9s 1s/step - loss: 2.5210 - acc: 0.3986 - val_loss: 2.1628 - val_acc: 0.3243\n",
      "Epoch 9/10\n",
      "8/8 [==============================] - 9s 1s/step - loss: 2.7933 - acc: 0.2943 - val_loss: 7.5542 - val_acc: 0.2973\n",
      "Epoch 10/10\n",
      "8/8 [==============================] - 9s 1s/step - loss: 2.1395 - acc: 0.4906 - val_loss: 10.0194 - val_acc: 0.3784\n"
     ]
    }
   ],
   "source": [
    "model = create_inception_model()\n",
    "loss, accuracy = train_inception_gen_model(model, 20, 10, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "ygyOr9qUH0Zt",
    "outputId": "638cec82-638c-4fda-e67a-48becab9e343"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 10.01935677915006\n",
      "Test accuracy: 0.37837838160025106\n"
     ]
    }
   ],
   "source": [
    "print('Test loss:', loss)\n",
    "print('Test accuracy:', accuracy) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sY4KT_9Z6-rR"
   },
   "source": [
    "Same accuracy again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aR2uvp0Krz4S"
   },
   "source": [
    "## Try Modulating Batch Size and fit_generator parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FsV2Kwgw5dvy"
   },
   "source": [
    "#### Simple Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "37 batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1111
    },
    "colab_type": "code",
    "id": "UoNynG0IsD85",
    "outputId": "c63e1b7b-871c-453b-ad6a-c14cc7ee980c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_149 (Conv2D)          (None, 512, 512, 32)      832       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_59 (MaxPooling (None, 256, 256, 32)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_104 (Bat (None, 256, 256, 32)      128       \n",
      "_________________________________________________________________\n",
      "conv2d_150 (Conv2D)          (None, 256, 256, 64)      51264     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_60 (MaxPooling (None, 128, 128, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_151 (Conv2D)          (None, 128, 128, 64)      102464    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_61 (MaxPooling (None, 64, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_152 (Conv2D)          (None, 64, 64, 64)        102464    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_62 (MaxPooling (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_153 (Conv2D)          (None, 32, 32, 64)        102464    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_63 (MaxPooling (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_154 (Conv2D)          (None, 16, 16, 64)        102464    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_64 (MaxPooling (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 1024)              4195328   \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 5)                 5125      \n",
      "=================================================================\n",
      "Total params: 4,662,533\n",
      "Trainable params: 4,662,469\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "4/4 [==============================] - 7s 2s/step - loss: 1.4717 - acc: 0.3958 - val_loss: 1.3654 - val_acc: 0.2703\n",
      "Epoch 2/10\n",
      "4/4 [==============================] - 2s 450ms/step - loss: 1.3267 - acc: 0.3774 - val_loss: 1.7793 - val_acc: 0.3784\n",
      "Epoch 3/10\n",
      "4/4 [==============================] - 2s 452ms/step - loss: 1.3316 - acc: 0.4725 - val_loss: 1.3813 - val_acc: 0.3784\n",
      "Epoch 4/10\n",
      "4/4 [==============================] - 2s 449ms/step - loss: 1.3070 - acc: 0.4738 - val_loss: 1.4384 - val_acc: 0.3784\n",
      "Epoch 5/10\n",
      "4/4 [==============================] - 2s 625ms/step - loss: 1.3197 - acc: 0.4712 - val_loss: 1.3574 - val_acc: 0.3784\n",
      "Epoch 6/10\n",
      "4/4 [==============================] - 3s 742ms/step - loss: 1.3027 - acc: 0.4725 - val_loss: 1.3745 - val_acc: 0.3784\n",
      "Epoch 7/10\n",
      "4/4 [==============================] - 3s 744ms/step - loss: 1.2995 - acc: 0.4725 - val_loss: 1.3556 - val_acc: 0.3784\n",
      "Epoch 8/10\n",
      "4/4 [==============================] - 3s 743ms/step - loss: 1.2933 - acc: 0.4705 - val_loss: 1.3625 - val_acc: 0.3784\n",
      "Epoch 9/10\n",
      "4/4 [==============================] - 3s 681ms/step - loss: 1.2922 - acc: 0.4725 - val_loss: 1.3659 - val_acc: 0.3784\n",
      "Epoch 10/10\n",
      "4/4 [==============================] - 3s 654ms/step - loss: 1.2941 - acc: 0.4732 - val_loss: 1.3732 - val_acc: 0.3784\n"
     ]
    }
   ],
   "source": [
    "model = create_simple_model()\n",
    "loss, accuracy = train_simple_gen_model(model, 37, 10, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "jErO5ViOyVID",
    "outputId": "833f5ff9-3b00-4513-8e02-8bd0b9a266db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.373185157775879\n",
      "Test Accuracy: 0.37837838160025106\n"
     ]
    }
   ],
   "source": [
    "print('Test Loss:', loss)\n",
    "print('Test Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "50 batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1269
    },
    "colab_type": "code",
    "id": "3pCRbieC1Bf8",
    "outputId": "c4cf940c-0823-4313-8bfb-6117f1240fe9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 512, 512, 32)      832       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 256, 256, 32)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 256, 256, 32)      128       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 256, 256, 64)      51264     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 128, 128, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 128, 128, 64)      102464    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 64, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 64, 64, 64)        102464    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 32, 32, 64)        102464    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 16, 16, 64)        102464    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              4195328   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 5125      \n",
      "=================================================================\n",
      "Total params: 4,662,533\n",
      "Trainable params: 4,662,469\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/10\n",
      "3/3 [==============================] - 18s 6s/step - loss: 1.5469 - acc: 0.3996 - val_loss: 1.3772 - val_acc: 0.3243\n",
      "Epoch 2/10\n",
      "3/3 [==============================] - 2s 561ms/step - loss: 1.4634 - acc: 0.3215 - val_loss: 1.4423 - val_acc: 0.3243\n",
      "Epoch 3/10\n",
      "3/3 [==============================] - 2s 555ms/step - loss: 1.4415 - acc: 0.2891 - val_loss: 1.8806 - val_acc: 0.3243\n",
      "Epoch 4/10\n",
      "3/3 [==============================] - 2s 556ms/step - loss: 1.3509 - acc: 0.3646 - val_loss: 1.4885 - val_acc: 0.3784\n",
      "Epoch 5/10\n",
      "3/3 [==============================] - 2s 554ms/step - loss: 1.3035 - acc: 0.4724 - val_loss: 1.9137 - val_acc: 0.3784\n",
      "Epoch 6/10\n",
      "3/3 [==============================] - 2s 561ms/step - loss: 1.3223 - acc: 0.4724 - val_loss: 1.4860 - val_acc: 0.3784\n",
      "Epoch 7/10\n",
      "3/3 [==============================] - 2s 558ms/step - loss: 1.3028 - acc: 0.4724 - val_loss: 1.4050 - val_acc: 0.3784\n",
      "Epoch 8/10\n",
      "3/3 [==============================] - 2s 555ms/step - loss: 1.3037 - acc: 0.4703 - val_loss: 1.4518 - val_acc: 0.3784\n",
      "Epoch 9/10\n",
      "3/3 [==============================] - 2s 684ms/step - loss: 1.3006 - acc: 0.4731 - val_loss: 1.3884 - val_acc: 0.3784\n",
      "Epoch 10/10\n",
      "3/3 [==============================] - 3s 976ms/step - loss: 1.2848 - acc: 0.4710 - val_loss: 1.3606 - val_acc: 0.3784\n"
     ]
    }
   ],
   "source": [
    "model = create_simple_model()\n",
    "loss, accuracy = train_simple_gen_model(model, 50, 10, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "EbLFNheQkjUV",
    "outputId": "bd59dec4-c0ef-4f89-f281-0f0f5f260ae4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 1.3605993261208404\n",
      "Test accuracy: 0.37837838160025106\n"
     ]
    }
   ],
   "source": [
    "print('Test loss:', loss)\n",
    "print('Test accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1111
    },
    "colab_type": "code",
    "id": "UAGjO3vpkfHN",
    "outputId": "3c1d79dc-de65-4e61-b2a1-56163b344d24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_7 (Conv2D)            (None, 512, 512, 32)      832       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 256, 256, 32)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 256, 256, 32)      128       \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 256, 256, 64)      51264     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 128, 128, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 128, 128, 64)      102464    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 64, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 64, 64, 64)        102464    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 32, 32, 64)        102464    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 16, 16, 64)        102464    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1024)              4195328   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 5)                 5125      \n",
      "=================================================================\n",
      "Total params: 4,662,533\n",
      "Trainable params: 4,662,469\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "3/3 [==============================] - 4s 1s/step - loss: 1.5445 - acc: 0.3417 - val_loss: 4.0358 - val_acc: 0.3784\n",
      "Epoch 2/10\n",
      "3/3 [==============================] - 2s 817ms/step - loss: 1.5914 - acc: 0.3025 - val_loss: 1.4968 - val_acc: 0.3243\n",
      "Epoch 3/10\n",
      "3/3 [==============================] - 3s 856ms/step - loss: 1.5378 - acc: 0.2897 - val_loss: 1.5291 - val_acc: 0.3243\n",
      "Epoch 4/10\n",
      "3/3 [==============================] - 3s 864ms/step - loss: 1.5231 - acc: 0.2911 - val_loss: 1.4740 - val_acc: 0.3243\n",
      "Epoch 5/10\n",
      "3/3 [==============================] - 3s 855ms/step - loss: 1.4511 - acc: 0.2945 - val_loss: 1.3930 - val_acc: 0.3243\n",
      "Epoch 6/10\n",
      "3/3 [==============================] - 3s 918ms/step - loss: 1.3934 - acc: 0.2891 - val_loss: 1.3501 - val_acc: 0.3784\n",
      "Epoch 7/10\n",
      "3/3 [==============================] - 3s 984ms/step - loss: 1.3007 - acc: 0.4266 - val_loss: 1.3800 - val_acc: 0.3784\n",
      "Epoch 8/10\n",
      "3/3 [==============================] - 3s 968ms/step - loss: 1.2895 - acc: 0.4703 - val_loss: 1.4100 - val_acc: 0.3784\n",
      "Epoch 9/10\n",
      "3/3 [==============================] - 3s 976ms/step - loss: 1.3207 - acc: 0.4690 - val_loss: 1.3951 - val_acc: 0.3784\n",
      "Epoch 10/10\n",
      "3/3 [==============================] - 3s 880ms/step - loss: 1.3019 - acc: 0.4710 - val_loss: 1.3720 - val_acc: 0.3784\n"
     ]
    }
   ],
   "source": [
    "model = create_simple_model()\n",
    "loss, accuracy = train_simple_gen_model(model, 50, 15, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "xYehBZba5QdM",
    "outputId": "820b48fa-db46-48e2-ab41-7cc7d8af3f96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 1.3719598538166768\n",
      "Test accuracy: 0.37837838160025106\n"
     ]
    }
   ],
   "source": [
    "print('Test loss:', loss)\n",
    "print('Test accuracy:', accuracy) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LYKAlAV35gPq"
   },
   "source": [
    "#### Inception Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10 batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 12181
    },
    "colab_type": "code",
    "id": "emuheGfR5mBr",
    "outputId": "02ae3687-b151-4279-cdb2-224f7ffb5672"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 512, 512, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_455 (Conv2D)             (None, 255, 255, 32) 864         input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_390 (BatchN (None, 255, 255, 32) 96          conv2d_455[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_377 (Activation)     (None, 255, 255, 32) 0           batch_normalization_390[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_456 (Conv2D)             (None, 253, 253, 32) 9216        activation_377[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_391 (BatchN (None, 253, 253, 32) 96          conv2d_456[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_378 (Activation)     (None, 253, 253, 32) 0           batch_normalization_391[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_457 (Conv2D)             (None, 253, 253, 64) 18432       activation_378[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_392 (BatchN (None, 253, 253, 64) 192         conv2d_457[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_379 (Activation)     (None, 253, 253, 64) 0           batch_normalization_392[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_95 (MaxPooling2D) (None, 126, 126, 64) 0           activation_379[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_458 (Conv2D)             (None, 126, 126, 80) 5120        max_pooling2d_95[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_393 (BatchN (None, 126, 126, 80) 240         conv2d_458[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_380 (Activation)     (None, 126, 126, 80) 0           batch_normalization_393[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_459 (Conv2D)             (None, 124, 124, 192 138240      activation_380[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_394 (BatchN (None, 124, 124, 192 576         conv2d_459[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_381 (Activation)     (None, 124, 124, 192 0           batch_normalization_394[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_96 (MaxPooling2D) (None, 61, 61, 192)  0           activation_381[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_463 (Conv2D)             (None, 61, 61, 64)   12288       max_pooling2d_96[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_398 (BatchN (None, 61, 61, 64)   192         conv2d_463[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_385 (Activation)     (None, 61, 61, 64)   0           batch_normalization_398[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_461 (Conv2D)             (None, 61, 61, 48)   9216        max_pooling2d_96[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_464 (Conv2D)             (None, 61, 61, 96)   55296       activation_385[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_396 (BatchN (None, 61, 61, 48)   144         conv2d_461[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_399 (BatchN (None, 61, 61, 96)   288         conv2d_464[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_383 (Activation)     (None, 61, 61, 48)   0           batch_normalization_396[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_386 (Activation)     (None, 61, 61, 96)   0           batch_normalization_399[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_37 (AveragePo (None, 61, 61, 192)  0           max_pooling2d_96[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_460 (Conv2D)             (None, 61, 61, 64)   12288       max_pooling2d_96[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_462 (Conv2D)             (None, 61, 61, 64)   76800       activation_383[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_465 (Conv2D)             (None, 61, 61, 96)   82944       activation_386[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_466 (Conv2D)             (None, 61, 61, 32)   6144        average_pooling2d_37[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_395 (BatchN (None, 61, 61, 64)   192         conv2d_460[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_397 (BatchN (None, 61, 61, 64)   192         conv2d_462[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_400 (BatchN (None, 61, 61, 96)   288         conv2d_465[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_401 (BatchN (None, 61, 61, 32)   96          conv2d_466[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_382 (Activation)     (None, 61, 61, 64)   0           batch_normalization_395[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_384 (Activation)     (None, 61, 61, 64)   0           batch_normalization_397[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_387 (Activation)     (None, 61, 61, 96)   0           batch_normalization_400[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_388 (Activation)     (None, 61, 61, 32)   0           batch_normalization_401[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed0 (Concatenate)            (None, 61, 61, 256)  0           activation_382[0][0]             \n",
      "                                                                 activation_384[0][0]             \n",
      "                                                                 activation_387[0][0]             \n",
      "                                                                 activation_388[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_470 (Conv2D)             (None, 61, 61, 64)   16384       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_405 (BatchN (None, 61, 61, 64)   192         conv2d_470[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_392 (Activation)     (None, 61, 61, 64)   0           batch_normalization_405[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_468 (Conv2D)             (None, 61, 61, 48)   12288       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_471 (Conv2D)             (None, 61, 61, 96)   55296       activation_392[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_403 (BatchN (None, 61, 61, 48)   144         conv2d_468[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_406 (BatchN (None, 61, 61, 96)   288         conv2d_471[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_390 (Activation)     (None, 61, 61, 48)   0           batch_normalization_403[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_393 (Activation)     (None, 61, 61, 96)   0           batch_normalization_406[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_38 (AveragePo (None, 61, 61, 256)  0           mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_467 (Conv2D)             (None, 61, 61, 64)   16384       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_469 (Conv2D)             (None, 61, 61, 64)   76800       activation_390[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_472 (Conv2D)             (None, 61, 61, 96)   82944       activation_393[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_473 (Conv2D)             (None, 61, 61, 64)   16384       average_pooling2d_38[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_402 (BatchN (None, 61, 61, 64)   192         conv2d_467[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_404 (BatchN (None, 61, 61, 64)   192         conv2d_469[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_407 (BatchN (None, 61, 61, 96)   288         conv2d_472[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_408 (BatchN (None, 61, 61, 64)   192         conv2d_473[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_389 (Activation)     (None, 61, 61, 64)   0           batch_normalization_402[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_391 (Activation)     (None, 61, 61, 64)   0           batch_normalization_404[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_394 (Activation)     (None, 61, 61, 96)   0           batch_normalization_407[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_395 (Activation)     (None, 61, 61, 64)   0           batch_normalization_408[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed1 (Concatenate)            (None, 61, 61, 288)  0           activation_389[0][0]             \n",
      "                                                                 activation_391[0][0]             \n",
      "                                                                 activation_394[0][0]             \n",
      "                                                                 activation_395[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_477 (Conv2D)             (None, 61, 61, 64)   18432       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_412 (BatchN (None, 61, 61, 64)   192         conv2d_477[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_399 (Activation)     (None, 61, 61, 64)   0           batch_normalization_412[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_475 (Conv2D)             (None, 61, 61, 48)   13824       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_478 (Conv2D)             (None, 61, 61, 96)   55296       activation_399[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_410 (BatchN (None, 61, 61, 48)   144         conv2d_475[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_413 (BatchN (None, 61, 61, 96)   288         conv2d_478[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_397 (Activation)     (None, 61, 61, 48)   0           batch_normalization_410[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_400 (Activation)     (None, 61, 61, 96)   0           batch_normalization_413[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_39 (AveragePo (None, 61, 61, 288)  0           mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_474 (Conv2D)             (None, 61, 61, 64)   18432       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_476 (Conv2D)             (None, 61, 61, 64)   76800       activation_397[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_479 (Conv2D)             (None, 61, 61, 96)   82944       activation_400[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_480 (Conv2D)             (None, 61, 61, 64)   18432       average_pooling2d_39[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_409 (BatchN (None, 61, 61, 64)   192         conv2d_474[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_411 (BatchN (None, 61, 61, 64)   192         conv2d_476[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_414 (BatchN (None, 61, 61, 96)   288         conv2d_479[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_415 (BatchN (None, 61, 61, 64)   192         conv2d_480[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_396 (Activation)     (None, 61, 61, 64)   0           batch_normalization_409[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_398 (Activation)     (None, 61, 61, 64)   0           batch_normalization_411[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_401 (Activation)     (None, 61, 61, 96)   0           batch_normalization_414[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_402 (Activation)     (None, 61, 61, 64)   0           batch_normalization_415[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed2 (Concatenate)            (None, 61, 61, 288)  0           activation_396[0][0]             \n",
      "                                                                 activation_398[0][0]             \n",
      "                                                                 activation_401[0][0]             \n",
      "                                                                 activation_402[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_482 (Conv2D)             (None, 61, 61, 64)   18432       mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_417 (BatchN (None, 61, 61, 64)   192         conv2d_482[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_404 (Activation)     (None, 61, 61, 64)   0           batch_normalization_417[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_483 (Conv2D)             (None, 61, 61, 96)   55296       activation_404[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_418 (BatchN (None, 61, 61, 96)   288         conv2d_483[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_405 (Activation)     (None, 61, 61, 96)   0           batch_normalization_418[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_481 (Conv2D)             (None, 30, 30, 384)  995328      mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_484 (Conv2D)             (None, 30, 30, 96)   82944       activation_405[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_416 (BatchN (None, 30, 30, 384)  1152        conv2d_481[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_419 (BatchN (None, 30, 30, 96)   288         conv2d_484[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_403 (Activation)     (None, 30, 30, 384)  0           batch_normalization_416[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_406 (Activation)     (None, 30, 30, 96)   0           batch_normalization_419[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_97 (MaxPooling2D) (None, 30, 30, 288)  0           mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "mixed3 (Concatenate)            (None, 30, 30, 768)  0           activation_403[0][0]             \n",
      "                                                                 activation_406[0][0]             \n",
      "                                                                 max_pooling2d_97[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_489 (Conv2D)             (None, 30, 30, 128)  98304       mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_424 (BatchN (None, 30, 30, 128)  384         conv2d_489[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_411 (Activation)     (None, 30, 30, 128)  0           batch_normalization_424[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_490 (Conv2D)             (None, 30, 30, 128)  114688      activation_411[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_425 (BatchN (None, 30, 30, 128)  384         conv2d_490[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_412 (Activation)     (None, 30, 30, 128)  0           batch_normalization_425[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_486 (Conv2D)             (None, 30, 30, 128)  98304       mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_491 (Conv2D)             (None, 30, 30, 128)  114688      activation_412[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_421 (BatchN (None, 30, 30, 128)  384         conv2d_486[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_426 (BatchN (None, 30, 30, 128)  384         conv2d_491[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_408 (Activation)     (None, 30, 30, 128)  0           batch_normalization_421[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_413 (Activation)     (None, 30, 30, 128)  0           batch_normalization_426[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_487 (Conv2D)             (None, 30, 30, 128)  114688      activation_408[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_492 (Conv2D)             (None, 30, 30, 128)  114688      activation_413[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_422 (BatchN (None, 30, 30, 128)  384         conv2d_487[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_427 (BatchN (None, 30, 30, 128)  384         conv2d_492[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_409 (Activation)     (None, 30, 30, 128)  0           batch_normalization_422[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_414 (Activation)     (None, 30, 30, 128)  0           batch_normalization_427[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_40 (AveragePo (None, 30, 30, 768)  0           mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_485 (Conv2D)             (None, 30, 30, 192)  147456      mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_488 (Conv2D)             (None, 30, 30, 192)  172032      activation_409[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_493 (Conv2D)             (None, 30, 30, 192)  172032      activation_414[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_494 (Conv2D)             (None, 30, 30, 192)  147456      average_pooling2d_40[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_420 (BatchN (None, 30, 30, 192)  576         conv2d_485[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_423 (BatchN (None, 30, 30, 192)  576         conv2d_488[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_428 (BatchN (None, 30, 30, 192)  576         conv2d_493[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_429 (BatchN (None, 30, 30, 192)  576         conv2d_494[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_407 (Activation)     (None, 30, 30, 192)  0           batch_normalization_420[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_410 (Activation)     (None, 30, 30, 192)  0           batch_normalization_423[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_415 (Activation)     (None, 30, 30, 192)  0           batch_normalization_428[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_416 (Activation)     (None, 30, 30, 192)  0           batch_normalization_429[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed4 (Concatenate)            (None, 30, 30, 768)  0           activation_407[0][0]             \n",
      "                                                                 activation_410[0][0]             \n",
      "                                                                 activation_415[0][0]             \n",
      "                                                                 activation_416[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_499 (Conv2D)             (None, 30, 30, 160)  122880      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_434 (BatchN (None, 30, 30, 160)  480         conv2d_499[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_421 (Activation)     (None, 30, 30, 160)  0           batch_normalization_434[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_500 (Conv2D)             (None, 30, 30, 160)  179200      activation_421[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_435 (BatchN (None, 30, 30, 160)  480         conv2d_500[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_422 (Activation)     (None, 30, 30, 160)  0           batch_normalization_435[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_496 (Conv2D)             (None, 30, 30, 160)  122880      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_501 (Conv2D)             (None, 30, 30, 160)  179200      activation_422[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_431 (BatchN (None, 30, 30, 160)  480         conv2d_496[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_436 (BatchN (None, 30, 30, 160)  480         conv2d_501[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_418 (Activation)     (None, 30, 30, 160)  0           batch_normalization_431[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_423 (Activation)     (None, 30, 30, 160)  0           batch_normalization_436[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_497 (Conv2D)             (None, 30, 30, 160)  179200      activation_418[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_502 (Conv2D)             (None, 30, 30, 160)  179200      activation_423[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_432 (BatchN (None, 30, 30, 160)  480         conv2d_497[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_437 (BatchN (None, 30, 30, 160)  480         conv2d_502[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_419 (Activation)     (None, 30, 30, 160)  0           batch_normalization_432[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_424 (Activation)     (None, 30, 30, 160)  0           batch_normalization_437[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_41 (AveragePo (None, 30, 30, 768)  0           mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_495 (Conv2D)             (None, 30, 30, 192)  147456      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_498 (Conv2D)             (None, 30, 30, 192)  215040      activation_419[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_503 (Conv2D)             (None, 30, 30, 192)  215040      activation_424[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_504 (Conv2D)             (None, 30, 30, 192)  147456      average_pooling2d_41[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_430 (BatchN (None, 30, 30, 192)  576         conv2d_495[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_433 (BatchN (None, 30, 30, 192)  576         conv2d_498[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_438 (BatchN (None, 30, 30, 192)  576         conv2d_503[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_439 (BatchN (None, 30, 30, 192)  576         conv2d_504[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_417 (Activation)     (None, 30, 30, 192)  0           batch_normalization_430[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_420 (Activation)     (None, 30, 30, 192)  0           batch_normalization_433[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_425 (Activation)     (None, 30, 30, 192)  0           batch_normalization_438[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_426 (Activation)     (None, 30, 30, 192)  0           batch_normalization_439[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed5 (Concatenate)            (None, 30, 30, 768)  0           activation_417[0][0]             \n",
      "                                                                 activation_420[0][0]             \n",
      "                                                                 activation_425[0][0]             \n",
      "                                                                 activation_426[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_509 (Conv2D)             (None, 30, 30, 160)  122880      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_444 (BatchN (None, 30, 30, 160)  480         conv2d_509[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_431 (Activation)     (None, 30, 30, 160)  0           batch_normalization_444[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_510 (Conv2D)             (None, 30, 30, 160)  179200      activation_431[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_445 (BatchN (None, 30, 30, 160)  480         conv2d_510[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_432 (Activation)     (None, 30, 30, 160)  0           batch_normalization_445[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_506 (Conv2D)             (None, 30, 30, 160)  122880      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_511 (Conv2D)             (None, 30, 30, 160)  179200      activation_432[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_441 (BatchN (None, 30, 30, 160)  480         conv2d_506[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_446 (BatchN (None, 30, 30, 160)  480         conv2d_511[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_428 (Activation)     (None, 30, 30, 160)  0           batch_normalization_441[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_433 (Activation)     (None, 30, 30, 160)  0           batch_normalization_446[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_507 (Conv2D)             (None, 30, 30, 160)  179200      activation_428[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_512 (Conv2D)             (None, 30, 30, 160)  179200      activation_433[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_442 (BatchN (None, 30, 30, 160)  480         conv2d_507[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_447 (BatchN (None, 30, 30, 160)  480         conv2d_512[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_429 (Activation)     (None, 30, 30, 160)  0           batch_normalization_442[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_434 (Activation)     (None, 30, 30, 160)  0           batch_normalization_447[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_42 (AveragePo (None, 30, 30, 768)  0           mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_505 (Conv2D)             (None, 30, 30, 192)  147456      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_508 (Conv2D)             (None, 30, 30, 192)  215040      activation_429[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_513 (Conv2D)             (None, 30, 30, 192)  215040      activation_434[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_514 (Conv2D)             (None, 30, 30, 192)  147456      average_pooling2d_42[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_440 (BatchN (None, 30, 30, 192)  576         conv2d_505[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_443 (BatchN (None, 30, 30, 192)  576         conv2d_508[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_448 (BatchN (None, 30, 30, 192)  576         conv2d_513[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_449 (BatchN (None, 30, 30, 192)  576         conv2d_514[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_427 (Activation)     (None, 30, 30, 192)  0           batch_normalization_440[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_430 (Activation)     (None, 30, 30, 192)  0           batch_normalization_443[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_435 (Activation)     (None, 30, 30, 192)  0           batch_normalization_448[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_436 (Activation)     (None, 30, 30, 192)  0           batch_normalization_449[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed6 (Concatenate)            (None, 30, 30, 768)  0           activation_427[0][0]             \n",
      "                                                                 activation_430[0][0]             \n",
      "                                                                 activation_435[0][0]             \n",
      "                                                                 activation_436[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_519 (Conv2D)             (None, 30, 30, 192)  147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_454 (BatchN (None, 30, 30, 192)  576         conv2d_519[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_441 (Activation)     (None, 30, 30, 192)  0           batch_normalization_454[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_520 (Conv2D)             (None, 30, 30, 192)  258048      activation_441[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_455 (BatchN (None, 30, 30, 192)  576         conv2d_520[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_442 (Activation)     (None, 30, 30, 192)  0           batch_normalization_455[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_516 (Conv2D)             (None, 30, 30, 192)  147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_521 (Conv2D)             (None, 30, 30, 192)  258048      activation_442[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_451 (BatchN (None, 30, 30, 192)  576         conv2d_516[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_456 (BatchN (None, 30, 30, 192)  576         conv2d_521[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_438 (Activation)     (None, 30, 30, 192)  0           batch_normalization_451[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_443 (Activation)     (None, 30, 30, 192)  0           batch_normalization_456[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_517 (Conv2D)             (None, 30, 30, 192)  258048      activation_438[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_522 (Conv2D)             (None, 30, 30, 192)  258048      activation_443[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_452 (BatchN (None, 30, 30, 192)  576         conv2d_517[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_457 (BatchN (None, 30, 30, 192)  576         conv2d_522[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_439 (Activation)     (None, 30, 30, 192)  0           batch_normalization_452[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_444 (Activation)     (None, 30, 30, 192)  0           batch_normalization_457[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_43 (AveragePo (None, 30, 30, 768)  0           mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_515 (Conv2D)             (None, 30, 30, 192)  147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_518 (Conv2D)             (None, 30, 30, 192)  258048      activation_439[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_523 (Conv2D)             (None, 30, 30, 192)  258048      activation_444[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_524 (Conv2D)             (None, 30, 30, 192)  147456      average_pooling2d_43[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_450 (BatchN (None, 30, 30, 192)  576         conv2d_515[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_453 (BatchN (None, 30, 30, 192)  576         conv2d_518[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_458 (BatchN (None, 30, 30, 192)  576         conv2d_523[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_459 (BatchN (None, 30, 30, 192)  576         conv2d_524[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_437 (Activation)     (None, 30, 30, 192)  0           batch_normalization_450[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_440 (Activation)     (None, 30, 30, 192)  0           batch_normalization_453[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_445 (Activation)     (None, 30, 30, 192)  0           batch_normalization_458[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_446 (Activation)     (None, 30, 30, 192)  0           batch_normalization_459[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed7 (Concatenate)            (None, 30, 30, 768)  0           activation_437[0][0]             \n",
      "                                                                 activation_440[0][0]             \n",
      "                                                                 activation_445[0][0]             \n",
      "                                                                 activation_446[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_527 (Conv2D)             (None, 30, 30, 192)  147456      mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_462 (BatchN (None, 30, 30, 192)  576         conv2d_527[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_449 (Activation)     (None, 30, 30, 192)  0           batch_normalization_462[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_528 (Conv2D)             (None, 30, 30, 192)  258048      activation_449[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_463 (BatchN (None, 30, 30, 192)  576         conv2d_528[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_450 (Activation)     (None, 30, 30, 192)  0           batch_normalization_463[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_525 (Conv2D)             (None, 30, 30, 192)  147456      mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_529 (Conv2D)             (None, 30, 30, 192)  258048      activation_450[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_460 (BatchN (None, 30, 30, 192)  576         conv2d_525[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_464 (BatchN (None, 30, 30, 192)  576         conv2d_529[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_447 (Activation)     (None, 30, 30, 192)  0           batch_normalization_460[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_451 (Activation)     (None, 30, 30, 192)  0           batch_normalization_464[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_526 (Conv2D)             (None, 14, 14, 320)  552960      activation_447[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_530 (Conv2D)             (None, 14, 14, 192)  331776      activation_451[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_461 (BatchN (None, 14, 14, 320)  960         conv2d_526[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_465 (BatchN (None, 14, 14, 192)  576         conv2d_530[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_448 (Activation)     (None, 14, 14, 320)  0           batch_normalization_461[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_452 (Activation)     (None, 14, 14, 192)  0           batch_normalization_465[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_98 (MaxPooling2D) (None, 14, 14, 768)  0           mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "mixed8 (Concatenate)            (None, 14, 14, 1280) 0           activation_448[0][0]             \n",
      "                                                                 activation_452[0][0]             \n",
      "                                                                 max_pooling2d_98[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_535 (Conv2D)             (None, 14, 14, 448)  573440      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_470 (BatchN (None, 14, 14, 448)  1344        conv2d_535[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_457 (Activation)     (None, 14, 14, 448)  0           batch_normalization_470[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_532 (Conv2D)             (None, 14, 14, 384)  491520      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_536 (Conv2D)             (None, 14, 14, 384)  1548288     activation_457[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_467 (BatchN (None, 14, 14, 384)  1152        conv2d_532[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_471 (BatchN (None, 14, 14, 384)  1152        conv2d_536[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_454 (Activation)     (None, 14, 14, 384)  0           batch_normalization_467[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_458 (Activation)     (None, 14, 14, 384)  0           batch_normalization_471[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_533 (Conv2D)             (None, 14, 14, 384)  442368      activation_454[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_534 (Conv2D)             (None, 14, 14, 384)  442368      activation_454[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_537 (Conv2D)             (None, 14, 14, 384)  442368      activation_458[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_538 (Conv2D)             (None, 14, 14, 384)  442368      activation_458[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_44 (AveragePo (None, 14, 14, 1280) 0           mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_531 (Conv2D)             (None, 14, 14, 320)  409600      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_468 (BatchN (None, 14, 14, 384)  1152        conv2d_533[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_469 (BatchN (None, 14, 14, 384)  1152        conv2d_534[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_472 (BatchN (None, 14, 14, 384)  1152        conv2d_537[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_473 (BatchN (None, 14, 14, 384)  1152        conv2d_538[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_539 (Conv2D)             (None, 14, 14, 192)  245760      average_pooling2d_44[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_466 (BatchN (None, 14, 14, 320)  960         conv2d_531[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_455 (Activation)     (None, 14, 14, 384)  0           batch_normalization_468[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_456 (Activation)     (None, 14, 14, 384)  0           batch_normalization_469[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_459 (Activation)     (None, 14, 14, 384)  0           batch_normalization_472[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_460 (Activation)     (None, 14, 14, 384)  0           batch_normalization_473[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_474 (BatchN (None, 14, 14, 192)  576         conv2d_539[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_453 (Activation)     (None, 14, 14, 320)  0           batch_normalization_466[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed9_0 (Concatenate)          (None, 14, 14, 768)  0           activation_455[0][0]             \n",
      "                                                                 activation_456[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 14, 14, 768)  0           activation_459[0][0]             \n",
      "                                                                 activation_460[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_461 (Activation)     (None, 14, 14, 192)  0           batch_normalization_474[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed9 (Concatenate)            (None, 14, 14, 2048) 0           activation_453[0][0]             \n",
      "                                                                 mixed9_0[0][0]                   \n",
      "                                                                 concatenate_9[0][0]              \n",
      "                                                                 activation_461[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_544 (Conv2D)             (None, 14, 14, 448)  917504      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_479 (BatchN (None, 14, 14, 448)  1344        conv2d_544[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_466 (Activation)     (None, 14, 14, 448)  0           batch_normalization_479[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_541 (Conv2D)             (None, 14, 14, 384)  786432      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_545 (Conv2D)             (None, 14, 14, 384)  1548288     activation_466[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_476 (BatchN (None, 14, 14, 384)  1152        conv2d_541[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_480 (BatchN (None, 14, 14, 384)  1152        conv2d_545[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_463 (Activation)     (None, 14, 14, 384)  0           batch_normalization_476[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_467 (Activation)     (None, 14, 14, 384)  0           batch_normalization_480[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_542 (Conv2D)             (None, 14, 14, 384)  442368      activation_463[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_543 (Conv2D)             (None, 14, 14, 384)  442368      activation_463[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_546 (Conv2D)             (None, 14, 14, 384)  442368      activation_467[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_547 (Conv2D)             (None, 14, 14, 384)  442368      activation_467[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_45 (AveragePo (None, 14, 14, 2048) 0           mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_540 (Conv2D)             (None, 14, 14, 320)  655360      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_477 (BatchN (None, 14, 14, 384)  1152        conv2d_542[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_478 (BatchN (None, 14, 14, 384)  1152        conv2d_543[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_481 (BatchN (None, 14, 14, 384)  1152        conv2d_546[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_482 (BatchN (None, 14, 14, 384)  1152        conv2d_547[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_548 (Conv2D)             (None, 14, 14, 192)  393216      average_pooling2d_45[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_475 (BatchN (None, 14, 14, 320)  960         conv2d_540[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_464 (Activation)     (None, 14, 14, 384)  0           batch_normalization_477[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_465 (Activation)     (None, 14, 14, 384)  0           batch_normalization_478[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_468 (Activation)     (None, 14, 14, 384)  0           batch_normalization_481[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_469 (Activation)     (None, 14, 14, 384)  0           batch_normalization_482[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_483 (BatchN (None, 14, 14, 192)  576         conv2d_548[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_462 (Activation)     (None, 14, 14, 320)  0           batch_normalization_475[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed9_1 (Concatenate)          (None, 14, 14, 768)  0           activation_464[0][0]             \n",
      "                                                                 activation_465[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 14, 14, 768)  0           activation_468[0][0]             \n",
      "                                                                 activation_469[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_470 (Activation)     (None, 14, 14, 192)  0           batch_normalization_483[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed10 (Concatenate)           (None, 14, 14, 2048) 0           activation_462[0][0]             \n",
      "                                                                 mixed9_1[0][0]                   \n",
      "                                                                 concatenate_10[0][0]             \n",
      "                                                                 activation_470[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_5 (Glo (None, 2048)         0           mixed10[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_35 (Dense)                (None, 1024)         2098176     global_average_pooling2d_5[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_36 (Dense)                (None, 5)            5125        dense_35[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 23,906,085\n",
      "Trainable params: 23,871,653\n",
      "Non-trainable params: 34,432\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/10\n",
      "15/15 [==============================] - 41s 3s/step - loss: 2.9250 - acc: 0.3997 - val_loss: 10.0194 - val_acc: 0.3784\n",
      "Epoch 2/10\n",
      "15/15 [==============================] - 8s 513ms/step - loss: 2.9729 - acc: 0.4639 - val_loss: 10.0194 - val_acc: 0.3784\n",
      "Epoch 3/10\n",
      "15/15 [==============================] - 8s 518ms/step - loss: 2.6274 - acc: 0.3997 - val_loss: 6.7309 - val_acc: 0.3784\n",
      "Epoch 4/10\n",
      "15/15 [==============================] - 8s 532ms/step - loss: 2.4737 - acc: 0.4478 - val_loss: 5.1535 - val_acc: 0.3243\n",
      "Epoch 5/10\n",
      "15/15 [==============================] - 9s 618ms/step - loss: 2.3482 - acc: 0.4398 - val_loss: 1.3526 - val_acc: 0.3784\n",
      "Epoch 6/10\n",
      "15/15 [==============================] - 10s 686ms/step - loss: 2.6225 - acc: 0.4265 - val_loss: 1.3670 - val_acc: 0.3784\n",
      "Epoch 7/10\n",
      "15/15 [==============================] - 9s 617ms/step - loss: 2.2190 - acc: 0.4987 - val_loss: 10.0194 - val_acc: 0.3784\n",
      "Epoch 8/10\n",
      "15/15 [==============================] - 9s 611ms/step - loss: 2.3024 - acc: 0.4666 - val_loss: 10.0194 - val_acc: 0.3784\n",
      "Epoch 9/10\n",
      "15/15 [==============================] - 9s 617ms/step - loss: 2.4805 - acc: 0.4639 - val_loss: 10.0194 - val_acc: 0.3784\n",
      "Epoch 10/10\n",
      "15/15 [==============================] - 9s 619ms/step - loss: 2.2582 - acc: 0.4639 - val_loss: 10.0194 - val_acc: 0.3784\n"
     ]
    }
   ],
   "source": [
    "model = create_inception_model()\n",
    "loss, accuracy = train_inception_gen_model(model, 10, 10, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "y9eMLw685wYK",
    "outputId": "a2f193db-1924-48ae-ab1e-c9b22739f831"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 10.01935677915006\n",
      "Test accuracy: 0.37837838160025106\n"
     ]
    }
   ],
   "source": [
    "print('Test loss:', loss)\n",
    "print('Test accuracy:', accuracy) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fALffJsy9FQy"
   },
   "source": [
    "Same accuracy, but higher loss than with a larger batch size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "30 batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 12181
    },
    "colab_type": "code",
    "id": "KDvatC3X9LD5",
    "outputId": "a45485c9-82d9-4e4f-a660-1acbe9c6f13a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 512, 512, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_643 (Conv2D)             (None, 255, 255, 32) 864         input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_578 (BatchN (None, 255, 255, 32) 96          conv2d_643[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_565 (Activation)     (None, 255, 255, 32) 0           batch_normalization_578[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_644 (Conv2D)             (None, 253, 253, 32) 9216        activation_565[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_579 (BatchN (None, 253, 253, 32) 96          conv2d_644[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_566 (Activation)     (None, 253, 253, 32) 0           batch_normalization_579[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_645 (Conv2D)             (None, 253, 253, 64) 18432       activation_566[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_580 (BatchN (None, 253, 253, 64) 192         conv2d_645[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_567 (Activation)     (None, 253, 253, 64) 0           batch_normalization_580[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_103 (MaxPooling2D (None, 126, 126, 64) 0           activation_567[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_646 (Conv2D)             (None, 126, 126, 80) 5120        max_pooling2d_103[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_581 (BatchN (None, 126, 126, 80) 240         conv2d_646[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_568 (Activation)     (None, 126, 126, 80) 0           batch_normalization_581[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_647 (Conv2D)             (None, 124, 124, 192 138240      activation_568[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_582 (BatchN (None, 124, 124, 192 576         conv2d_647[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_569 (Activation)     (None, 124, 124, 192 0           batch_normalization_582[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_104 (MaxPooling2D (None, 61, 61, 192)  0           activation_569[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_651 (Conv2D)             (None, 61, 61, 64)   12288       max_pooling2d_104[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_586 (BatchN (None, 61, 61, 64)   192         conv2d_651[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_573 (Activation)     (None, 61, 61, 64)   0           batch_normalization_586[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_649 (Conv2D)             (None, 61, 61, 48)   9216        max_pooling2d_104[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_652 (Conv2D)             (None, 61, 61, 96)   55296       activation_573[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_584 (BatchN (None, 61, 61, 48)   144         conv2d_649[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_587 (BatchN (None, 61, 61, 96)   288         conv2d_652[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_571 (Activation)     (None, 61, 61, 48)   0           batch_normalization_584[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_574 (Activation)     (None, 61, 61, 96)   0           batch_normalization_587[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_55 (AveragePo (None, 61, 61, 192)  0           max_pooling2d_104[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_648 (Conv2D)             (None, 61, 61, 64)   12288       max_pooling2d_104[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_650 (Conv2D)             (None, 61, 61, 64)   76800       activation_571[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_653 (Conv2D)             (None, 61, 61, 96)   82944       activation_574[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_654 (Conv2D)             (None, 61, 61, 32)   6144        average_pooling2d_55[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_583 (BatchN (None, 61, 61, 64)   192         conv2d_648[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_585 (BatchN (None, 61, 61, 64)   192         conv2d_650[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_588 (BatchN (None, 61, 61, 96)   288         conv2d_653[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_589 (BatchN (None, 61, 61, 32)   96          conv2d_654[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_570 (Activation)     (None, 61, 61, 64)   0           batch_normalization_583[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_572 (Activation)     (None, 61, 61, 64)   0           batch_normalization_585[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_575 (Activation)     (None, 61, 61, 96)   0           batch_normalization_588[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_576 (Activation)     (None, 61, 61, 32)   0           batch_normalization_589[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed0 (Concatenate)            (None, 61, 61, 256)  0           activation_570[0][0]             \n",
      "                                                                 activation_572[0][0]             \n",
      "                                                                 activation_575[0][0]             \n",
      "                                                                 activation_576[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_658 (Conv2D)             (None, 61, 61, 64)   16384       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_593 (BatchN (None, 61, 61, 64)   192         conv2d_658[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_580 (Activation)     (None, 61, 61, 64)   0           batch_normalization_593[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_656 (Conv2D)             (None, 61, 61, 48)   12288       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_659 (Conv2D)             (None, 61, 61, 96)   55296       activation_580[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_591 (BatchN (None, 61, 61, 48)   144         conv2d_656[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_594 (BatchN (None, 61, 61, 96)   288         conv2d_659[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_578 (Activation)     (None, 61, 61, 48)   0           batch_normalization_591[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_581 (Activation)     (None, 61, 61, 96)   0           batch_normalization_594[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_56 (AveragePo (None, 61, 61, 256)  0           mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_655 (Conv2D)             (None, 61, 61, 64)   16384       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_657 (Conv2D)             (None, 61, 61, 64)   76800       activation_578[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_660 (Conv2D)             (None, 61, 61, 96)   82944       activation_581[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_661 (Conv2D)             (None, 61, 61, 64)   16384       average_pooling2d_56[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_590 (BatchN (None, 61, 61, 64)   192         conv2d_655[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_592 (BatchN (None, 61, 61, 64)   192         conv2d_657[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_595 (BatchN (None, 61, 61, 96)   288         conv2d_660[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_596 (BatchN (None, 61, 61, 64)   192         conv2d_661[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_577 (Activation)     (None, 61, 61, 64)   0           batch_normalization_590[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_579 (Activation)     (None, 61, 61, 64)   0           batch_normalization_592[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_582 (Activation)     (None, 61, 61, 96)   0           batch_normalization_595[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_583 (Activation)     (None, 61, 61, 64)   0           batch_normalization_596[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed1 (Concatenate)            (None, 61, 61, 288)  0           activation_577[0][0]             \n",
      "                                                                 activation_579[0][0]             \n",
      "                                                                 activation_582[0][0]             \n",
      "                                                                 activation_583[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_665 (Conv2D)             (None, 61, 61, 64)   18432       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_600 (BatchN (None, 61, 61, 64)   192         conv2d_665[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_587 (Activation)     (None, 61, 61, 64)   0           batch_normalization_600[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_663 (Conv2D)             (None, 61, 61, 48)   13824       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_666 (Conv2D)             (None, 61, 61, 96)   55296       activation_587[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_598 (BatchN (None, 61, 61, 48)   144         conv2d_663[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_601 (BatchN (None, 61, 61, 96)   288         conv2d_666[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_585 (Activation)     (None, 61, 61, 48)   0           batch_normalization_598[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_588 (Activation)     (None, 61, 61, 96)   0           batch_normalization_601[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_57 (AveragePo (None, 61, 61, 288)  0           mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_662 (Conv2D)             (None, 61, 61, 64)   18432       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_664 (Conv2D)             (None, 61, 61, 64)   76800       activation_585[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_667 (Conv2D)             (None, 61, 61, 96)   82944       activation_588[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_668 (Conv2D)             (None, 61, 61, 64)   18432       average_pooling2d_57[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_597 (BatchN (None, 61, 61, 64)   192         conv2d_662[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_599 (BatchN (None, 61, 61, 64)   192         conv2d_664[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_602 (BatchN (None, 61, 61, 96)   288         conv2d_667[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_603 (BatchN (None, 61, 61, 64)   192         conv2d_668[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_584 (Activation)     (None, 61, 61, 64)   0           batch_normalization_597[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_586 (Activation)     (None, 61, 61, 64)   0           batch_normalization_599[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_589 (Activation)     (None, 61, 61, 96)   0           batch_normalization_602[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_590 (Activation)     (None, 61, 61, 64)   0           batch_normalization_603[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed2 (Concatenate)            (None, 61, 61, 288)  0           activation_584[0][0]             \n",
      "                                                                 activation_586[0][0]             \n",
      "                                                                 activation_589[0][0]             \n",
      "                                                                 activation_590[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_670 (Conv2D)             (None, 61, 61, 64)   18432       mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_605 (BatchN (None, 61, 61, 64)   192         conv2d_670[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_592 (Activation)     (None, 61, 61, 64)   0           batch_normalization_605[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_671 (Conv2D)             (None, 61, 61, 96)   55296       activation_592[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_606 (BatchN (None, 61, 61, 96)   288         conv2d_671[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_593 (Activation)     (None, 61, 61, 96)   0           batch_normalization_606[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_669 (Conv2D)             (None, 30, 30, 384)  995328      mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_672 (Conv2D)             (None, 30, 30, 96)   82944       activation_593[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_604 (BatchN (None, 30, 30, 384)  1152        conv2d_669[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_607 (BatchN (None, 30, 30, 96)   288         conv2d_672[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_591 (Activation)     (None, 30, 30, 384)  0           batch_normalization_604[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_594 (Activation)     (None, 30, 30, 96)   0           batch_normalization_607[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_105 (MaxPooling2D (None, 30, 30, 288)  0           mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "mixed3 (Concatenate)            (None, 30, 30, 768)  0           activation_591[0][0]             \n",
      "                                                                 activation_594[0][0]             \n",
      "                                                                 max_pooling2d_105[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_677 (Conv2D)             (None, 30, 30, 128)  98304       mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_612 (BatchN (None, 30, 30, 128)  384         conv2d_677[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_599 (Activation)     (None, 30, 30, 128)  0           batch_normalization_612[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_678 (Conv2D)             (None, 30, 30, 128)  114688      activation_599[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_613 (BatchN (None, 30, 30, 128)  384         conv2d_678[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_600 (Activation)     (None, 30, 30, 128)  0           batch_normalization_613[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_674 (Conv2D)             (None, 30, 30, 128)  98304       mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_679 (Conv2D)             (None, 30, 30, 128)  114688      activation_600[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_609 (BatchN (None, 30, 30, 128)  384         conv2d_674[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_614 (BatchN (None, 30, 30, 128)  384         conv2d_679[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_596 (Activation)     (None, 30, 30, 128)  0           batch_normalization_609[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_601 (Activation)     (None, 30, 30, 128)  0           batch_normalization_614[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_675 (Conv2D)             (None, 30, 30, 128)  114688      activation_596[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_680 (Conv2D)             (None, 30, 30, 128)  114688      activation_601[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_610 (BatchN (None, 30, 30, 128)  384         conv2d_675[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_615 (BatchN (None, 30, 30, 128)  384         conv2d_680[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_597 (Activation)     (None, 30, 30, 128)  0           batch_normalization_610[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_602 (Activation)     (None, 30, 30, 128)  0           batch_normalization_615[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_58 (AveragePo (None, 30, 30, 768)  0           mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_673 (Conv2D)             (None, 30, 30, 192)  147456      mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_676 (Conv2D)             (None, 30, 30, 192)  172032      activation_597[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_681 (Conv2D)             (None, 30, 30, 192)  172032      activation_602[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_682 (Conv2D)             (None, 30, 30, 192)  147456      average_pooling2d_58[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_608 (BatchN (None, 30, 30, 192)  576         conv2d_673[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_611 (BatchN (None, 30, 30, 192)  576         conv2d_676[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_616 (BatchN (None, 30, 30, 192)  576         conv2d_681[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_617 (BatchN (None, 30, 30, 192)  576         conv2d_682[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_595 (Activation)     (None, 30, 30, 192)  0           batch_normalization_608[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_598 (Activation)     (None, 30, 30, 192)  0           batch_normalization_611[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_603 (Activation)     (None, 30, 30, 192)  0           batch_normalization_616[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_604 (Activation)     (None, 30, 30, 192)  0           batch_normalization_617[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed4 (Concatenate)            (None, 30, 30, 768)  0           activation_595[0][0]             \n",
      "                                                                 activation_598[0][0]             \n",
      "                                                                 activation_603[0][0]             \n",
      "                                                                 activation_604[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_687 (Conv2D)             (None, 30, 30, 160)  122880      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_622 (BatchN (None, 30, 30, 160)  480         conv2d_687[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_609 (Activation)     (None, 30, 30, 160)  0           batch_normalization_622[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_688 (Conv2D)             (None, 30, 30, 160)  179200      activation_609[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_623 (BatchN (None, 30, 30, 160)  480         conv2d_688[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_610 (Activation)     (None, 30, 30, 160)  0           batch_normalization_623[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_684 (Conv2D)             (None, 30, 30, 160)  122880      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_689 (Conv2D)             (None, 30, 30, 160)  179200      activation_610[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_619 (BatchN (None, 30, 30, 160)  480         conv2d_684[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_624 (BatchN (None, 30, 30, 160)  480         conv2d_689[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_606 (Activation)     (None, 30, 30, 160)  0           batch_normalization_619[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_611 (Activation)     (None, 30, 30, 160)  0           batch_normalization_624[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_685 (Conv2D)             (None, 30, 30, 160)  179200      activation_606[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_690 (Conv2D)             (None, 30, 30, 160)  179200      activation_611[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_620 (BatchN (None, 30, 30, 160)  480         conv2d_685[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_625 (BatchN (None, 30, 30, 160)  480         conv2d_690[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_607 (Activation)     (None, 30, 30, 160)  0           batch_normalization_620[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_612 (Activation)     (None, 30, 30, 160)  0           batch_normalization_625[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_59 (AveragePo (None, 30, 30, 768)  0           mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_683 (Conv2D)             (None, 30, 30, 192)  147456      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_686 (Conv2D)             (None, 30, 30, 192)  215040      activation_607[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_691 (Conv2D)             (None, 30, 30, 192)  215040      activation_612[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_692 (Conv2D)             (None, 30, 30, 192)  147456      average_pooling2d_59[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_618 (BatchN (None, 30, 30, 192)  576         conv2d_683[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_621 (BatchN (None, 30, 30, 192)  576         conv2d_686[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_626 (BatchN (None, 30, 30, 192)  576         conv2d_691[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_627 (BatchN (None, 30, 30, 192)  576         conv2d_692[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_605 (Activation)     (None, 30, 30, 192)  0           batch_normalization_618[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_608 (Activation)     (None, 30, 30, 192)  0           batch_normalization_621[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_613 (Activation)     (None, 30, 30, 192)  0           batch_normalization_626[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_614 (Activation)     (None, 30, 30, 192)  0           batch_normalization_627[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed5 (Concatenate)            (None, 30, 30, 768)  0           activation_605[0][0]             \n",
      "                                                                 activation_608[0][0]             \n",
      "                                                                 activation_613[0][0]             \n",
      "                                                                 activation_614[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_697 (Conv2D)             (None, 30, 30, 160)  122880      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_632 (BatchN (None, 30, 30, 160)  480         conv2d_697[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_619 (Activation)     (None, 30, 30, 160)  0           batch_normalization_632[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_698 (Conv2D)             (None, 30, 30, 160)  179200      activation_619[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_633 (BatchN (None, 30, 30, 160)  480         conv2d_698[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_620 (Activation)     (None, 30, 30, 160)  0           batch_normalization_633[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_694 (Conv2D)             (None, 30, 30, 160)  122880      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_699 (Conv2D)             (None, 30, 30, 160)  179200      activation_620[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_629 (BatchN (None, 30, 30, 160)  480         conv2d_694[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_634 (BatchN (None, 30, 30, 160)  480         conv2d_699[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_616 (Activation)     (None, 30, 30, 160)  0           batch_normalization_629[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_621 (Activation)     (None, 30, 30, 160)  0           batch_normalization_634[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_695 (Conv2D)             (None, 30, 30, 160)  179200      activation_616[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_700 (Conv2D)             (None, 30, 30, 160)  179200      activation_621[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_630 (BatchN (None, 30, 30, 160)  480         conv2d_695[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_635 (BatchN (None, 30, 30, 160)  480         conv2d_700[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_617 (Activation)     (None, 30, 30, 160)  0           batch_normalization_630[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_622 (Activation)     (None, 30, 30, 160)  0           batch_normalization_635[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_60 (AveragePo (None, 30, 30, 768)  0           mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_693 (Conv2D)             (None, 30, 30, 192)  147456      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_696 (Conv2D)             (None, 30, 30, 192)  215040      activation_617[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_701 (Conv2D)             (None, 30, 30, 192)  215040      activation_622[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_702 (Conv2D)             (None, 30, 30, 192)  147456      average_pooling2d_60[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_628 (BatchN (None, 30, 30, 192)  576         conv2d_693[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_631 (BatchN (None, 30, 30, 192)  576         conv2d_696[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_636 (BatchN (None, 30, 30, 192)  576         conv2d_701[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_637 (BatchN (None, 30, 30, 192)  576         conv2d_702[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_615 (Activation)     (None, 30, 30, 192)  0           batch_normalization_628[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_618 (Activation)     (None, 30, 30, 192)  0           batch_normalization_631[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_623 (Activation)     (None, 30, 30, 192)  0           batch_normalization_636[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_624 (Activation)     (None, 30, 30, 192)  0           batch_normalization_637[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed6 (Concatenate)            (None, 30, 30, 768)  0           activation_615[0][0]             \n",
      "                                                                 activation_618[0][0]             \n",
      "                                                                 activation_623[0][0]             \n",
      "                                                                 activation_624[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_707 (Conv2D)             (None, 30, 30, 192)  147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_642 (BatchN (None, 30, 30, 192)  576         conv2d_707[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_629 (Activation)     (None, 30, 30, 192)  0           batch_normalization_642[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_708 (Conv2D)             (None, 30, 30, 192)  258048      activation_629[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_643 (BatchN (None, 30, 30, 192)  576         conv2d_708[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_630 (Activation)     (None, 30, 30, 192)  0           batch_normalization_643[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_704 (Conv2D)             (None, 30, 30, 192)  147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_709 (Conv2D)             (None, 30, 30, 192)  258048      activation_630[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_639 (BatchN (None, 30, 30, 192)  576         conv2d_704[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_644 (BatchN (None, 30, 30, 192)  576         conv2d_709[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_626 (Activation)     (None, 30, 30, 192)  0           batch_normalization_639[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_631 (Activation)     (None, 30, 30, 192)  0           batch_normalization_644[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_705 (Conv2D)             (None, 30, 30, 192)  258048      activation_626[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_710 (Conv2D)             (None, 30, 30, 192)  258048      activation_631[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_640 (BatchN (None, 30, 30, 192)  576         conv2d_705[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_645 (BatchN (None, 30, 30, 192)  576         conv2d_710[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_627 (Activation)     (None, 30, 30, 192)  0           batch_normalization_640[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_632 (Activation)     (None, 30, 30, 192)  0           batch_normalization_645[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_61 (AveragePo (None, 30, 30, 768)  0           mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_703 (Conv2D)             (None, 30, 30, 192)  147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_706 (Conv2D)             (None, 30, 30, 192)  258048      activation_627[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_711 (Conv2D)             (None, 30, 30, 192)  258048      activation_632[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_712 (Conv2D)             (None, 30, 30, 192)  147456      average_pooling2d_61[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_638 (BatchN (None, 30, 30, 192)  576         conv2d_703[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_641 (BatchN (None, 30, 30, 192)  576         conv2d_706[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_646 (BatchN (None, 30, 30, 192)  576         conv2d_711[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_647 (BatchN (None, 30, 30, 192)  576         conv2d_712[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_625 (Activation)     (None, 30, 30, 192)  0           batch_normalization_638[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_628 (Activation)     (None, 30, 30, 192)  0           batch_normalization_641[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_633 (Activation)     (None, 30, 30, 192)  0           batch_normalization_646[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_634 (Activation)     (None, 30, 30, 192)  0           batch_normalization_647[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed7 (Concatenate)            (None, 30, 30, 768)  0           activation_625[0][0]             \n",
      "                                                                 activation_628[0][0]             \n",
      "                                                                 activation_633[0][0]             \n",
      "                                                                 activation_634[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_715 (Conv2D)             (None, 30, 30, 192)  147456      mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_650 (BatchN (None, 30, 30, 192)  576         conv2d_715[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_637 (Activation)     (None, 30, 30, 192)  0           batch_normalization_650[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_716 (Conv2D)             (None, 30, 30, 192)  258048      activation_637[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_651 (BatchN (None, 30, 30, 192)  576         conv2d_716[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_638 (Activation)     (None, 30, 30, 192)  0           batch_normalization_651[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_713 (Conv2D)             (None, 30, 30, 192)  147456      mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_717 (Conv2D)             (None, 30, 30, 192)  258048      activation_638[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_648 (BatchN (None, 30, 30, 192)  576         conv2d_713[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_652 (BatchN (None, 30, 30, 192)  576         conv2d_717[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_635 (Activation)     (None, 30, 30, 192)  0           batch_normalization_648[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_639 (Activation)     (None, 30, 30, 192)  0           batch_normalization_652[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_714 (Conv2D)             (None, 14, 14, 320)  552960      activation_635[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_718 (Conv2D)             (None, 14, 14, 192)  331776      activation_639[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_649 (BatchN (None, 14, 14, 320)  960         conv2d_714[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_653 (BatchN (None, 14, 14, 192)  576         conv2d_718[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_636 (Activation)     (None, 14, 14, 320)  0           batch_normalization_649[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_640 (Activation)     (None, 14, 14, 192)  0           batch_normalization_653[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_106 (MaxPooling2D (None, 14, 14, 768)  0           mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "mixed8 (Concatenate)            (None, 14, 14, 1280) 0           activation_636[0][0]             \n",
      "                                                                 activation_640[0][0]             \n",
      "                                                                 max_pooling2d_106[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_723 (Conv2D)             (None, 14, 14, 448)  573440      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_658 (BatchN (None, 14, 14, 448)  1344        conv2d_723[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_645 (Activation)     (None, 14, 14, 448)  0           batch_normalization_658[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_720 (Conv2D)             (None, 14, 14, 384)  491520      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_724 (Conv2D)             (None, 14, 14, 384)  1548288     activation_645[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_655 (BatchN (None, 14, 14, 384)  1152        conv2d_720[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_659 (BatchN (None, 14, 14, 384)  1152        conv2d_724[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_642 (Activation)     (None, 14, 14, 384)  0           batch_normalization_655[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_646 (Activation)     (None, 14, 14, 384)  0           batch_normalization_659[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_721 (Conv2D)             (None, 14, 14, 384)  442368      activation_642[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_722 (Conv2D)             (None, 14, 14, 384)  442368      activation_642[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_725 (Conv2D)             (None, 14, 14, 384)  442368      activation_646[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_726 (Conv2D)             (None, 14, 14, 384)  442368      activation_646[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_62 (AveragePo (None, 14, 14, 1280) 0           mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_719 (Conv2D)             (None, 14, 14, 320)  409600      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_656 (BatchN (None, 14, 14, 384)  1152        conv2d_721[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_657 (BatchN (None, 14, 14, 384)  1152        conv2d_722[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_660 (BatchN (None, 14, 14, 384)  1152        conv2d_725[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_661 (BatchN (None, 14, 14, 384)  1152        conv2d_726[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_727 (Conv2D)             (None, 14, 14, 192)  245760      average_pooling2d_62[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_654 (BatchN (None, 14, 14, 320)  960         conv2d_719[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_643 (Activation)     (None, 14, 14, 384)  0           batch_normalization_656[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_644 (Activation)     (None, 14, 14, 384)  0           batch_normalization_657[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_647 (Activation)     (None, 14, 14, 384)  0           batch_normalization_660[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_648 (Activation)     (None, 14, 14, 384)  0           batch_normalization_661[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_662 (BatchN (None, 14, 14, 192)  576         conv2d_727[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_641 (Activation)     (None, 14, 14, 320)  0           batch_normalization_654[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed9_0 (Concatenate)          (None, 14, 14, 768)  0           activation_643[0][0]             \n",
      "                                                                 activation_644[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_13 (Concatenate)    (None, 14, 14, 768)  0           activation_647[0][0]             \n",
      "                                                                 activation_648[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_649 (Activation)     (None, 14, 14, 192)  0           batch_normalization_662[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed9 (Concatenate)            (None, 14, 14, 2048) 0           activation_641[0][0]             \n",
      "                                                                 mixed9_0[0][0]                   \n",
      "                                                                 concatenate_13[0][0]             \n",
      "                                                                 activation_649[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_732 (Conv2D)             (None, 14, 14, 448)  917504      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_667 (BatchN (None, 14, 14, 448)  1344        conv2d_732[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_654 (Activation)     (None, 14, 14, 448)  0           batch_normalization_667[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_729 (Conv2D)             (None, 14, 14, 384)  786432      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_733 (Conv2D)             (None, 14, 14, 384)  1548288     activation_654[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_664 (BatchN (None, 14, 14, 384)  1152        conv2d_729[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_668 (BatchN (None, 14, 14, 384)  1152        conv2d_733[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_651 (Activation)     (None, 14, 14, 384)  0           batch_normalization_664[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_655 (Activation)     (None, 14, 14, 384)  0           batch_normalization_668[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_730 (Conv2D)             (None, 14, 14, 384)  442368      activation_651[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_731 (Conv2D)             (None, 14, 14, 384)  442368      activation_651[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_734 (Conv2D)             (None, 14, 14, 384)  442368      activation_655[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_735 (Conv2D)             (None, 14, 14, 384)  442368      activation_655[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_63 (AveragePo (None, 14, 14, 2048) 0           mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_728 (Conv2D)             (None, 14, 14, 320)  655360      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_665 (BatchN (None, 14, 14, 384)  1152        conv2d_730[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_666 (BatchN (None, 14, 14, 384)  1152        conv2d_731[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_669 (BatchN (None, 14, 14, 384)  1152        conv2d_734[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_670 (BatchN (None, 14, 14, 384)  1152        conv2d_735[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_736 (Conv2D)             (None, 14, 14, 192)  393216      average_pooling2d_63[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_663 (BatchN (None, 14, 14, 320)  960         conv2d_728[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_652 (Activation)     (None, 14, 14, 384)  0           batch_normalization_665[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_653 (Activation)     (None, 14, 14, 384)  0           batch_normalization_666[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_656 (Activation)     (None, 14, 14, 384)  0           batch_normalization_669[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_657 (Activation)     (None, 14, 14, 384)  0           batch_normalization_670[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_671 (BatchN (None, 14, 14, 192)  576         conv2d_736[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_650 (Activation)     (None, 14, 14, 320)  0           batch_normalization_663[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed9_1 (Concatenate)          (None, 14, 14, 768)  0           activation_652[0][0]             \n",
      "                                                                 activation_653[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_14 (Concatenate)    (None, 14, 14, 768)  0           activation_656[0][0]             \n",
      "                                                                 activation_657[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_658 (Activation)     (None, 14, 14, 192)  0           batch_normalization_671[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed10 (Concatenate)           (None, 14, 14, 2048) 0           activation_650[0][0]             \n",
      "                                                                 mixed9_1[0][0]                   \n",
      "                                                                 concatenate_14[0][0]             \n",
      "                                                                 activation_658[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_7 (Glo (None, 2048)         0           mixed10[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_39 (Dense)                (None, 1024)         2098176     global_average_pooling2d_7[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_40 (Dense)                (None, 5)            5125        dense_39[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 23,906,085\n",
      "Trainable params: 23,871,653\n",
      "Non-trainable params: 34,432\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/10\n",
      "5/5 [==============================] - 66s 13s/step - loss: 3.1205 - acc: 0.2998 - val_loss: 9.3368 - val_acc: 0.3243\n",
      "Epoch 2/10\n",
      "5/5 [==============================] - 7s 1s/step - loss: 1.7896 - acc: 0.3979 - val_loss: 3.4913 - val_acc: 0.3784\n",
      "Epoch 3/10\n",
      "5/5 [==============================] - 7s 1s/step - loss: 2.1434 - acc: 0.4704 - val_loss: 1.7562 - val_acc: 0.3784\n",
      "Epoch 4/10\n",
      "5/5 [==============================] - 7s 1s/step - loss: 1.7820 - acc: 0.4731 - val_loss: 2.4601 - val_acc: 0.3784\n",
      "Epoch 5/10\n",
      "5/5 [==============================] - 7s 1s/step - loss: 1.7824 - acc: 0.4704 - val_loss: 10.0194 - val_acc: 0.3784\n",
      "Epoch 6/10\n",
      "5/5 [==============================] - 7s 1s/step - loss: 1.6968 - acc: 0.4745 - val_loss: 10.0194 - val_acc: 0.3784\n",
      "Epoch 7/10\n",
      "5/5 [==============================] - 7s 1s/step - loss: 1.7266 - acc: 0.4745 - val_loss: 10.0194 - val_acc: 0.3784\n",
      "Epoch 8/10\n",
      "5/5 [==============================] - 9s 2s/step - loss: 1.6124 - acc: 0.4704 - val_loss: 4.0257 - val_acc: 0.3514\n",
      "Epoch 9/10\n",
      "5/5 [==============================] - 9s 2s/step - loss: 1.6256 - acc: 0.4382 - val_loss: 1.9498 - val_acc: 0.3784\n",
      "Epoch 10/10\n",
      "5/5 [==============================] - 9s 2s/step - loss: 1.5178 - acc: 0.4516 - val_loss: 1.7153 - val_acc: 0.3514\n"
     ]
    }
   ],
   "source": [
    "model = create_inception_model()\n",
    "loss, accuracy = train_inception_gen_model(model, 30, 10, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "ZOcWIjFkld-T",
    "outputId": "cbb4f330-6ae8-48a8-a939-b08e3e93f7a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 1.7152826705494442\n",
      "Test accuracy: 0.35135135457322403\n"
     ]
    }
   ],
   "source": [
    "print('Test loss:', loss)\n",
    "print('Test accuracy:', accuracy) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5CrAbw-lrgYj"
   },
   "source": [
    "Much lower test loss than Inception has ever had, but not super high test accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "30 batch and 15 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 12463
    },
    "colab_type": "code",
    "id": "o3hkcgLors5P",
    "outputId": "dc79f31c-b076-47ff-a537-2ee40b2f4137"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 512, 512, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 255, 255, 32) 864         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 255, 255, 32) 96          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 255, 255, 32) 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 253, 253, 32) 9216        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 253, 253, 32) 96          conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 253, 253, 32) 0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 253, 253, 64) 18432       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 253, 253, 64) 192         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 253, 253, 64) 0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 126, 126, 64) 0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 126, 126, 80) 5120        max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 126, 126, 80) 240         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 126, 126, 80) 0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 124, 124, 192 138240      activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 124, 124, 192 576         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 124, 124, 192 0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 61, 61, 192)  0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 61, 61, 64)   12288       max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 61, 61, 64)   192         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 61, 61, 64)   0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 61, 61, 48)   9216        max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 61, 61, 96)   55296       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 61, 61, 48)   144         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 61, 61, 96)   288         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 61, 61, 48)   0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 61, 61, 96)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 61, 61, 192)  0           max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 61, 61, 64)   12288       max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 61, 61, 64)   76800       activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 61, 61, 96)   82944       activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 61, 61, 32)   6144        average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 61, 61, 64)   192         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 61, 61, 64)   192         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 61, 61, 96)   288         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 61, 61, 32)   96          conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 61, 61, 64)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 61, 61, 64)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 61, 61, 96)   0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 61, 61, 32)   0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed0 (Concatenate)            (None, 61, 61, 256)  0           activation_6[0][0]               \n",
      "                                                                 activation_8[0][0]               \n",
      "                                                                 activation_11[0][0]              \n",
      "                                                                 activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 61, 61, 64)   16384       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 61, 61, 64)   192         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 61, 61, 64)   0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 61, 61, 48)   12288       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 61, 61, 96)   55296       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 61, 61, 48)   144         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 61, 61, 96)   288         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 61, 61, 48)   0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 61, 61, 96)   0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_2 (AveragePoo (None, 61, 61, 256)  0           mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 61, 61, 64)   16384       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 61, 61, 64)   76800       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 61, 61, 96)   82944       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 61, 61, 64)   16384       average_pooling2d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 61, 61, 64)   192         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 61, 61, 64)   192         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 61, 61, 96)   288         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 61, 61, 64)   192         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 61, 61, 64)   0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 61, 61, 64)   0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 61, 61, 96)   0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 61, 61, 64)   0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed1 (Concatenate)            (None, 61, 61, 288)  0           activation_13[0][0]              \n",
      "                                                                 activation_15[0][0]              \n",
      "                                                                 activation_18[0][0]              \n",
      "                                                                 activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 61, 61, 64)   18432       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 61, 61, 64)   192         conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 61, 61, 64)   0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 61, 61, 48)   13824       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 61, 61, 96)   55296       activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 61, 61, 48)   144         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 61, 61, 96)   288         conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 61, 61, 48)   0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 61, 61, 96)   0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_3 (AveragePoo (None, 61, 61, 288)  0           mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 61, 61, 64)   18432       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 61, 61, 64)   76800       activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 61, 61, 96)   82944       activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 61, 61, 64)   18432       average_pooling2d_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 61, 61, 64)   192         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 61, 61, 64)   192         conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 61, 61, 96)   288         conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 61, 61, 64)   192         conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 61, 61, 64)   0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 61, 61, 64)   0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 61, 61, 96)   0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 61, 61, 64)   0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed2 (Concatenate)            (None, 61, 61, 288)  0           activation_20[0][0]              \n",
      "                                                                 activation_22[0][0]              \n",
      "                                                                 activation_25[0][0]              \n",
      "                                                                 activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 61, 61, 64)   18432       mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 61, 61, 64)   192         conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 61, 61, 64)   0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 61, 61, 96)   55296       activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 61, 61, 96)   288         conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 61, 61, 96)   0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 30, 30, 384)  995328      mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 30, 30, 96)   82944       activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 30, 30, 384)  1152        conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 30, 30, 96)   288         conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 30, 30, 384)  0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 30, 30, 96)   0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 30, 30, 288)  0           mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "mixed3 (Concatenate)            (None, 30, 30, 768)  0           activation_27[0][0]              \n",
      "                                                                 activation_30[0][0]              \n",
      "                                                                 max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 30, 30, 128)  98304       mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 30, 30, 128)  384         conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 30, 30, 128)  0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 30, 30, 128)  114688      activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 30, 30, 128)  384         conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 30, 30, 128)  0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 30, 30, 128)  98304       mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 30, 30, 128)  114688      activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 30, 30, 128)  384         conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 30, 30, 128)  384         conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 30, 30, 128)  0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 30, 30, 128)  0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 30, 30, 128)  114688      activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 30, 30, 128)  114688      activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 30, 30, 128)  384         conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 30, 30, 128)  384         conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 30, 30, 128)  0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 30, 30, 128)  0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_4 (AveragePoo (None, 30, 30, 768)  0           mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 30, 30, 192)  147456      mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 30, 30, 192)  172032      activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 30, 30, 192)  172032      activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, 30, 30, 192)  147456      average_pooling2d_4[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 30, 30, 192)  576         conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 30, 30, 192)  576         conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 30, 30, 192)  576         conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 30, 30, 192)  576         conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 30, 30, 192)  0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 30, 30, 192)  0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 30, 30, 192)  0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 30, 30, 192)  0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed4 (Concatenate)            (None, 30, 30, 768)  0           activation_31[0][0]              \n",
      "                                                                 activation_34[0][0]              \n",
      "                                                                 activation_39[0][0]              \n",
      "                                                                 activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (None, 30, 30, 160)  122880      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNo (None, 30, 30, 160)  480         conv2d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 30, 30, 160)  0           batch_normalization_45[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)              (None, 30, 30, 160)  179200      activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNo (None, 30, 30, 160)  480         conv2d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 30, 30, 160)  0           batch_normalization_46[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, 30, 30, 160)  122880      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_47 (Conv2D)              (None, 30, 30, 160)  179200      activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 30, 30, 160)  480         conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_47 (BatchNo (None, 30, 30, 160)  480         conv2d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 30, 30, 160)  0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 30, 30, 160)  0           batch_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, 30, 30, 160)  179200      activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)              (None, 30, 30, 160)  179200      activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNo (None, 30, 30, 160)  480         conv2d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_48 (BatchNo (None, 30, 30, 160)  480         conv2d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 30, 30, 160)  0           batch_normalization_43[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 30, 30, 160)  0           batch_normalization_48[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_5 (AveragePoo (None, 30, 30, 768)  0           mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, 30, 30, 192)  147456      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, 30, 30, 192)  215040      activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_49 (Conv2D)              (None, 30, 30, 192)  215040      activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_50 (Conv2D)              (None, 30, 30, 192)  147456      average_pooling2d_5[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 30, 30, 192)  576         conv2d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, 30, 30, 192)  576         conv2d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_49 (BatchNo (None, 30, 30, 192)  576         conv2d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_50 (BatchNo (None, 30, 30, 192)  576         conv2d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 30, 30, 192)  0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 30, 30, 192)  0           batch_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 30, 30, 192)  0           batch_normalization_49[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 30, 30, 192)  0           batch_normalization_50[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed5 (Concatenate)            (None, 30, 30, 768)  0           activation_41[0][0]              \n",
      "                                                                 activation_44[0][0]              \n",
      "                                                                 activation_49[0][0]              \n",
      "                                                                 activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_55 (Conv2D)              (None, 30, 30, 160)  122880      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_55 (BatchNo (None, 30, 30, 160)  480         conv2d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 30, 30, 160)  0           batch_normalization_55[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_56 (Conv2D)              (None, 30, 30, 160)  179200      activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_56 (BatchNo (None, 30, 30, 160)  480         conv2d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 30, 30, 160)  0           batch_normalization_56[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_52 (Conv2D)              (None, 30, 30, 160)  122880      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_57 (Conv2D)              (None, 30, 30, 160)  179200      activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_52 (BatchNo (None, 30, 30, 160)  480         conv2d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_57 (BatchNo (None, 30, 30, 160)  480         conv2d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 30, 30, 160)  0           batch_normalization_52[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 30, 30, 160)  0           batch_normalization_57[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_53 (Conv2D)              (None, 30, 30, 160)  179200      activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_58 (Conv2D)              (None, 30, 30, 160)  179200      activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_53 (BatchNo (None, 30, 30, 160)  480         conv2d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_58 (BatchNo (None, 30, 30, 160)  480         conv2d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 30, 30, 160)  0           batch_normalization_53[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 30, 30, 160)  0           batch_normalization_58[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_6 (AveragePoo (None, 30, 30, 768)  0           mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_51 (Conv2D)              (None, 30, 30, 192)  147456      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_54 (Conv2D)              (None, 30, 30, 192)  215040      activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_59 (Conv2D)              (None, 30, 30, 192)  215040      activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_60 (Conv2D)              (None, 30, 30, 192)  147456      average_pooling2d_6[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_51 (BatchNo (None, 30, 30, 192)  576         conv2d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_54 (BatchNo (None, 30, 30, 192)  576         conv2d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_59 (BatchNo (None, 30, 30, 192)  576         conv2d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_60 (BatchNo (None, 30, 30, 192)  576         conv2d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 30, 30, 192)  0           batch_normalization_51[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 30, 30, 192)  0           batch_normalization_54[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 30, 30, 192)  0           batch_normalization_59[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 30, 30, 192)  0           batch_normalization_60[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed6 (Concatenate)            (None, 30, 30, 768)  0           activation_51[0][0]              \n",
      "                                                                 activation_54[0][0]              \n",
      "                                                                 activation_59[0][0]              \n",
      "                                                                 activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_65 (Conv2D)              (None, 30, 30, 192)  147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_65 (BatchNo (None, 30, 30, 192)  576         conv2d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 30, 30, 192)  0           batch_normalization_65[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_66 (Conv2D)              (None, 30, 30, 192)  258048      activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_66 (BatchNo (None, 30, 30, 192)  576         conv2d_66[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_66 (Activation)      (None, 30, 30, 192)  0           batch_normalization_66[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_62 (Conv2D)              (None, 30, 30, 192)  147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_67 (Conv2D)              (None, 30, 30, 192)  258048      activation_66[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_62 (BatchNo (None, 30, 30, 192)  576         conv2d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_67 (BatchNo (None, 30, 30, 192)  576         conv2d_67[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 30, 30, 192)  0           batch_normalization_62[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_67 (Activation)      (None, 30, 30, 192)  0           batch_normalization_67[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_63 (Conv2D)              (None, 30, 30, 192)  258048      activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_68 (Conv2D)              (None, 30, 30, 192)  258048      activation_67[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_63 (BatchNo (None, 30, 30, 192)  576         conv2d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_68 (BatchNo (None, 30, 30, 192)  576         conv2d_68[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 30, 30, 192)  0           batch_normalization_63[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_68 (Activation)      (None, 30, 30, 192)  0           batch_normalization_68[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_7 (AveragePoo (None, 30, 30, 768)  0           mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_61 (Conv2D)              (None, 30, 30, 192)  147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_64 (Conv2D)              (None, 30, 30, 192)  258048      activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_69 (Conv2D)              (None, 30, 30, 192)  258048      activation_68[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_70 (Conv2D)              (None, 30, 30, 192)  147456      average_pooling2d_7[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_61 (BatchNo (None, 30, 30, 192)  576         conv2d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_64 (BatchNo (None, 30, 30, 192)  576         conv2d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_69 (BatchNo (None, 30, 30, 192)  576         conv2d_69[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_70 (BatchNo (None, 30, 30, 192)  576         conv2d_70[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 30, 30, 192)  0           batch_normalization_61[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 30, 30, 192)  0           batch_normalization_64[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_69 (Activation)      (None, 30, 30, 192)  0           batch_normalization_69[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_70 (Activation)      (None, 30, 30, 192)  0           batch_normalization_70[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed7 (Concatenate)            (None, 30, 30, 768)  0           activation_61[0][0]              \n",
      "                                                                 activation_64[0][0]              \n",
      "                                                                 activation_69[0][0]              \n",
      "                                                                 activation_70[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_73 (Conv2D)              (None, 30, 30, 192)  147456      mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_73 (BatchNo (None, 30, 30, 192)  576         conv2d_73[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_73 (Activation)      (None, 30, 30, 192)  0           batch_normalization_73[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_74 (Conv2D)              (None, 30, 30, 192)  258048      activation_73[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_74 (BatchNo (None, 30, 30, 192)  576         conv2d_74[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_74 (Activation)      (None, 30, 30, 192)  0           batch_normalization_74[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_71 (Conv2D)              (None, 30, 30, 192)  147456      mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_75 (Conv2D)              (None, 30, 30, 192)  258048      activation_74[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_71 (BatchNo (None, 30, 30, 192)  576         conv2d_71[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_75 (BatchNo (None, 30, 30, 192)  576         conv2d_75[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_71 (Activation)      (None, 30, 30, 192)  0           batch_normalization_71[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_75 (Activation)      (None, 30, 30, 192)  0           batch_normalization_75[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_72 (Conv2D)              (None, 14, 14, 320)  552960      activation_71[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_76 (Conv2D)              (None, 14, 14, 192)  331776      activation_75[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_72 (BatchNo (None, 14, 14, 320)  960         conv2d_72[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_76 (BatchNo (None, 14, 14, 192)  576         conv2d_76[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_72 (Activation)      (None, 14, 14, 320)  0           batch_normalization_72[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_76 (Activation)      (None, 14, 14, 192)  0           batch_normalization_76[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 14, 14, 768)  0           mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "mixed8 (Concatenate)            (None, 14, 14, 1280) 0           activation_72[0][0]              \n",
      "                                                                 activation_76[0][0]              \n",
      "                                                                 max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_81 (Conv2D)              (None, 14, 14, 448)  573440      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_81 (BatchNo (None, 14, 14, 448)  1344        conv2d_81[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_81 (Activation)      (None, 14, 14, 448)  0           batch_normalization_81[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_78 (Conv2D)              (None, 14, 14, 384)  491520      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_82 (Conv2D)              (None, 14, 14, 384)  1548288     activation_81[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_78 (BatchNo (None, 14, 14, 384)  1152        conv2d_78[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_82 (BatchNo (None, 14, 14, 384)  1152        conv2d_82[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_78 (Activation)      (None, 14, 14, 384)  0           batch_normalization_78[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_82 (Activation)      (None, 14, 14, 384)  0           batch_normalization_82[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_79 (Conv2D)              (None, 14, 14, 384)  442368      activation_78[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_80 (Conv2D)              (None, 14, 14, 384)  442368      activation_78[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_83 (Conv2D)              (None, 14, 14, 384)  442368      activation_82[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_84 (Conv2D)              (None, 14, 14, 384)  442368      activation_82[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_8 (AveragePoo (None, 14, 14, 1280) 0           mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_77 (Conv2D)              (None, 14, 14, 320)  409600      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_79 (BatchNo (None, 14, 14, 384)  1152        conv2d_79[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_80 (BatchNo (None, 14, 14, 384)  1152        conv2d_80[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_83 (BatchNo (None, 14, 14, 384)  1152        conv2d_83[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_84 (BatchNo (None, 14, 14, 384)  1152        conv2d_84[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_85 (Conv2D)              (None, 14, 14, 192)  245760      average_pooling2d_8[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_77 (BatchNo (None, 14, 14, 320)  960         conv2d_77[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_79 (Activation)      (None, 14, 14, 384)  0           batch_normalization_79[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_80 (Activation)      (None, 14, 14, 384)  0           batch_normalization_80[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_83 (Activation)      (None, 14, 14, 384)  0           batch_normalization_83[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_84 (Activation)      (None, 14, 14, 384)  0           batch_normalization_84[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_85 (BatchNo (None, 14, 14, 192)  576         conv2d_85[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_77 (Activation)      (None, 14, 14, 320)  0           batch_normalization_77[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed9_0 (Concatenate)          (None, 14, 14, 768)  0           activation_79[0][0]              \n",
      "                                                                 activation_80[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 14, 14, 768)  0           activation_83[0][0]              \n",
      "                                                                 activation_84[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_85 (Activation)      (None, 14, 14, 192)  0           batch_normalization_85[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed9 (Concatenate)            (None, 14, 14, 2048) 0           activation_77[0][0]              \n",
      "                                                                 mixed9_0[0][0]                   \n",
      "                                                                 concatenate_1[0][0]              \n",
      "                                                                 activation_85[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_90 (Conv2D)              (None, 14, 14, 448)  917504      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_90 (BatchNo (None, 14, 14, 448)  1344        conv2d_90[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_90 (Activation)      (None, 14, 14, 448)  0           batch_normalization_90[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_87 (Conv2D)              (None, 14, 14, 384)  786432      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_91 (Conv2D)              (None, 14, 14, 384)  1548288     activation_90[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_87 (BatchNo (None, 14, 14, 384)  1152        conv2d_87[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_91 (BatchNo (None, 14, 14, 384)  1152        conv2d_91[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_87 (Activation)      (None, 14, 14, 384)  0           batch_normalization_87[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_91 (Activation)      (None, 14, 14, 384)  0           batch_normalization_91[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_88 (Conv2D)              (None, 14, 14, 384)  442368      activation_87[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_89 (Conv2D)              (None, 14, 14, 384)  442368      activation_87[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_92 (Conv2D)              (None, 14, 14, 384)  442368      activation_91[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_93 (Conv2D)              (None, 14, 14, 384)  442368      activation_91[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_9 (AveragePoo (None, 14, 14, 2048) 0           mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_86 (Conv2D)              (None, 14, 14, 320)  655360      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_88 (BatchNo (None, 14, 14, 384)  1152        conv2d_88[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_89 (BatchNo (None, 14, 14, 384)  1152        conv2d_89[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_92 (BatchNo (None, 14, 14, 384)  1152        conv2d_92[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_93 (BatchNo (None, 14, 14, 384)  1152        conv2d_93[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_94 (Conv2D)              (None, 14, 14, 192)  393216      average_pooling2d_9[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_86 (BatchNo (None, 14, 14, 320)  960         conv2d_86[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_88 (Activation)      (None, 14, 14, 384)  0           batch_normalization_88[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_89 (Activation)      (None, 14, 14, 384)  0           batch_normalization_89[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_92 (Activation)      (None, 14, 14, 384)  0           batch_normalization_92[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_93 (Activation)      (None, 14, 14, 384)  0           batch_normalization_93[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_94 (BatchNo (None, 14, 14, 192)  576         conv2d_94[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_86 (Activation)      (None, 14, 14, 320)  0           batch_normalization_86[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed9_1 (Concatenate)          (None, 14, 14, 768)  0           activation_88[0][0]              \n",
      "                                                                 activation_89[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 14, 14, 768)  0           activation_92[0][0]              \n",
      "                                                                 activation_93[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_94 (Activation)      (None, 14, 14, 192)  0           batch_normalization_94[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed10 (Concatenate)           (None, 14, 14, 2048) 0           activation_86[0][0]              \n",
      "                                                                 mixed9_1[0][0]                   \n",
      "                                                                 concatenate_2[0][0]              \n",
      "                                                                 activation_94[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_1 (Glo (None, 2048)         0           mixed10[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1024)         2098176     global_average_pooling2d_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 5)            5125        dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 23,906,085\n",
      "Trainable params: 23,871,653\n",
      "Non-trainable params: 34,432\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/15\n",
      "5/5 [==============================] - 43s 9s/step - loss: 2.8191 - acc: 0.3644 - val_loss: 10.8906 - val_acc: 0.3243\n",
      "Epoch 2/15\n",
      "5/5 [==============================] - 7s 1s/step - loss: 2.0924 - acc: 0.4488 - val_loss: 9.6604 - val_acc: 0.3784\n",
      "Epoch 3/15\n",
      "5/5 [==============================] - 7s 1s/step - loss: 1.7845 - acc: 0.4718 - val_loss: 6.1453 - val_acc: 0.3784\n",
      "Epoch 4/15\n",
      "5/5 [==============================] - 7s 1s/step - loss: 1.9347 - acc: 0.4194 - val_loss: 10.8906 - val_acc: 0.3243\n",
      "Epoch 5/15\n",
      "5/5 [==============================] - 7s 1s/step - loss: 1.9583 - acc: 0.4611 - val_loss: 10.8906 - val_acc: 0.3243\n",
      "Epoch 6/15\n",
      "5/5 [==============================] - 7s 1s/step - loss: 1.7176 - acc: 0.4019 - val_loss: 10.8906 - val_acc: 0.3243\n",
      "Epoch 7/15\n",
      "5/5 [==============================] - 7s 1s/step - loss: 1.7052 - acc: 0.4625 - val_loss: 2.3937 - val_acc: 0.3514\n",
      "Epoch 8/15\n",
      "5/5 [==============================] - 9s 2s/step - loss: 1.9020 - acc: 0.4394 - val_loss: 10.8906 - val_acc: 0.3243\n",
      "Epoch 9/15\n",
      "5/5 [==============================] - 9s 2s/step - loss: 1.9324 - acc: 0.3778 - val_loss: 10.8906 - val_acc: 0.3243\n",
      "Epoch 10/15\n",
      "5/5 [==============================] - 9s 2s/step - loss: 1.9675 - acc: 0.4569 - val_loss: 10.8906 - val_acc: 0.3243\n",
      "Epoch 11/15\n",
      "5/5 [==============================] - 9s 2s/step - loss: 2.0178 - acc: 0.4583 - val_loss: 10.8906 - val_acc: 0.3243\n",
      "Epoch 12/15\n",
      "5/5 [==============================] - 10s 2s/step - loss: 1.7042 - acc: 0.4729 - val_loss: 10.5115 - val_acc: 0.3243\n",
      "Epoch 13/15\n",
      "5/5 [==============================] - 9s 2s/step - loss: 1.7426 - acc: 0.4731 - val_loss: 10.5154 - val_acc: 0.3243\n",
      "Epoch 14/15\n",
      "5/5 [==============================] - 9s 2s/step - loss: 1.7886 - acc: 0.4676 - val_loss: 10.8906 - val_acc: 0.3243\n",
      "Epoch 15/15\n",
      "5/5 [==============================] - 9s 2s/step - loss: 1.4901 - acc: 0.4852 - val_loss: 10.8906 - val_acc: 0.3243\n"
     ]
    }
   ],
   "source": [
    "model = create_inception_model()\n",
    "loss, accuracy = train_inception_gen_model(model, 30, 15, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "zbqp_FZbv5Al",
    "outputId": "f6cd428d-89d7-4bf9-84da-c845f72bd551"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 10.890604611989614\n",
      "accuracy 0.3243243251297925\n"
     ]
    }
   ],
   "source": [
    "print(\"loss\", loss)\n",
    "print('accuracy', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Va83PmXN2QtJ"
   },
   "source": [
    "### Try Changing the Ranges in the Creation of Generators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G0V4eBxz3QX_"
   },
   "source": [
    "#### Simple Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1111
    },
    "colab_type": "code",
    "id": "GT4J4F0B2cuC",
    "outputId": "efb50408-11e8-46cb-e100-0a6c8ef3fe5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_13 (Conv2D)           (None, 512, 512, 32)      832       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling (None, 256, 256, 32)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 256, 256, 32)      128       \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 256, 256, 64)      51264     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling (None, 128, 128, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 128, 128, 64)      102464    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling (None, 64, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 64, 64, 64)        102464    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_16 (MaxPooling (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 32, 32, 64)        102464    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_17 (MaxPooling (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 16, 16, 64)        102464    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_18 (MaxPooling (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1024)              4195328   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 5)                 5125      \n",
      "=================================================================\n",
      "Total params: 4,662,533\n",
      "Trainable params: 4,662,469\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "4/4 [==============================] - 11s 3s/step - loss: 1.4469 - acc: 0.3531 - val_loss: 1.4016 - val_acc: 0.3784\n",
      "Epoch 2/10\n",
      "4/4 [==============================] - 2s 452ms/step - loss: 1.3557 - acc: 0.4848 - val_loss: 1.4012 - val_acc: 0.3784\n",
      "Epoch 3/10\n",
      "4/4 [==============================] - 2s 460ms/step - loss: 1.3604 - acc: 0.4738 - val_loss: 1.3748 - val_acc: 0.3784\n",
      "Epoch 4/10\n",
      "4/4 [==============================] - 2s 451ms/step - loss: 1.3321 - acc: 0.4725 - val_loss: 1.3657 - val_acc: 0.3784\n",
      "Epoch 5/10\n",
      "4/4 [==============================] - 2s 453ms/step - loss: 1.2808 - acc: 0.4718 - val_loss: 1.3543 - val_acc: 0.3784\n",
      "Epoch 6/10\n",
      "4/4 [==============================] - 2s 522ms/step - loss: 1.3059 - acc: 0.4705 - val_loss: 1.3568 - val_acc: 0.3784\n",
      "Epoch 7/10\n",
      "4/4 [==============================] - 3s 772ms/step - loss: 1.3069 - acc: 0.4732 - val_loss: 1.3531 - val_acc: 0.3784\n",
      "Epoch 8/10\n",
      "4/4 [==============================] - 3s 677ms/step - loss: 1.2975 - acc: 0.4692 - val_loss: 1.3588 - val_acc: 0.3784\n",
      "Epoch 9/10\n",
      "4/4 [==============================] - 3s 684ms/step - loss: 1.2855 - acc: 0.4732 - val_loss: 1.3516 - val_acc: 0.3784\n",
      "Epoch 10/10\n",
      "4/4 [==============================] - 3s 679ms/step - loss: 1.3109 - acc: 0.4745 - val_loss: 1.3522 - val_acc: 0.3784\n"
     ]
    }
   ],
   "source": [
    "model = create_simple_model()\n",
    "loss, accuracy = train_simple_gen_model(model, 37, 10, X_train, y_train, X_test, y_test, 90, 0.5, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "T-CuL-du2x8H",
    "outputId": "a63da556-8fc2-43c2-9731-e94b0d5b744e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 1.352188397098232\n",
      "Test accuracy: 0.37837838160025106\n"
     ]
    }
   ],
   "source": [
    "print('Test loss:', loss)\n",
    "print('Test accuracy:', accuracy) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1111
    },
    "colab_type": "code",
    "id": "L96ZZB_Z1uYf",
    "outputId": "9a5fcb3a-47c1-4f5b-ccc0-e059ad63e48b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_213 (Conv2D)          (None, 512, 512, 32)      832       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_33 (MaxPooling (None, 256, 256, 32)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_193 (Bat (None, 256, 256, 32)      128       \n",
      "_________________________________________________________________\n",
      "conv2d_214 (Conv2D)          (None, 256, 256, 64)      51264     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_34 (MaxPooling (None, 128, 128, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_215 (Conv2D)          (None, 128, 128, 64)      102464    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_35 (MaxPooling (None, 64, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_216 (Conv2D)          (None, 64, 64, 64)        102464    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_36 (MaxPooling (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_217 (Conv2D)          (None, 32, 32, 64)        102464    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_37 (MaxPooling (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_218 (Conv2D)          (None, 16, 16, 64)        102464    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_38 (MaxPooling (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 1024)              4195328   \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 5)                 5125      \n",
      "=================================================================\n",
      "Total params: 4,662,533\n",
      "Trainable params: 4,662,469\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "8/8 [==============================] - 15s 2s/step - loss: 1.5203 - acc: 0.4495 - val_loss: 1.4356 - val_acc: 0.3784\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 2s 253ms/step - loss: 1.3415 - acc: 0.4810 - val_loss: 1.3550 - val_acc: 0.3784\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - 2s 251ms/step - loss: 1.3613 - acc: 0.4367 - val_loss: 1.3711 - val_acc: 0.3784\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - 2s 279ms/step - loss: 1.3209 - acc: 0.4747 - val_loss: 1.3695 - val_acc: 0.3784\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - 3s 354ms/step - loss: 1.3619 - acc: 0.4747 - val_loss: 1.3652 - val_acc: 0.3784\n",
      "Epoch 6/10\n",
      "8/8 [==============================] - 3s 352ms/step - loss: 1.3373 - acc: 0.4302 - val_loss: 1.3723 - val_acc: 0.3784\n",
      "Epoch 7/10\n",
      "8/8 [==============================] - 3s 353ms/step - loss: 1.3371 - acc: 0.4525 - val_loss: 1.3641 - val_acc: 0.3784\n",
      "Epoch 8/10\n",
      "8/8 [==============================] - 3s 355ms/step - loss: 1.3453 - acc: 0.4302 - val_loss: 1.3563 - val_acc: 0.3784\n",
      "Epoch 9/10\n",
      "8/8 [==============================] - 3s 356ms/step - loss: 1.2868 - acc: 0.4525 - val_loss: 1.3514 - val_acc: 0.3784\n",
      "Epoch 10/10\n",
      "8/8 [==============================] - 3s 354ms/step - loss: 1.2749 - acc: 0.4461 - val_loss: 1.3542 - val_acc: 0.3784\n"
     ]
    }
   ],
   "source": [
    "model = create_simple_model()\n",
    "loss, accuracy = train_simple_gen_model(model, 20, 10, X_train, y_train, X_test, y_test, 90, 0.5, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "EPkPyi971yIv",
    "outputId": "f0a9c781-c780-4af7-9123-56e5c1b5e851"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 1.3542336096634735\n",
      "Test accuracy: 0.37837838160025106\n"
     ]
    }
   ],
   "source": [
    "print('Test loss:', loss)\n",
    "print('Test accuracy:', accuracy) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "68bf-c513WAj"
   },
   "source": [
    "#### Inception Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 12181
    },
    "colab_type": "code",
    "id": "Gh-3CW8W3ZBT",
    "outputId": "e111fe25-7a49-4863-e0ef-3521d7996465"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 512, 512, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 255, 255, 32) 864         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 255, 255, 32) 96          conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 255, 255, 32) 0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 253, 253, 32) 9216        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 253, 253, 32) 96          conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 253, 253, 32) 0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 253, 253, 64) 18432       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 253, 253, 64) 192         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 253, 253, 64) 0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_19 (MaxPooling2D) (None, 126, 126, 64) 0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 126, 126, 80) 5120        max_pooling2d_19[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 126, 126, 80) 240         conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 126, 126, 80) 0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 124, 124, 192 138240      activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 124, 124, 192 576         conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 124, 124, 192 0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_20 (MaxPooling2D) (None, 61, 61, 192)  0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 61, 61, 64)   12288       max_pooling2d_20[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 61, 61, 64)   192         conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 61, 61, 64)   0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 61, 61, 48)   9216        max_pooling2d_20[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 61, 61, 96)   55296       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 61, 61, 48)   144         conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 61, 61, 96)   288         conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 61, 61, 48)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 61, 61, 96)   0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 61, 61, 192)  0           max_pooling2d_20[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 61, 61, 64)   12288       max_pooling2d_20[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 61, 61, 64)   76800       activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 61, 61, 96)   82944       activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 61, 61, 32)   6144        average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 61, 61, 64)   192         conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 61, 61, 64)   192         conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 61, 61, 96)   288         conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 61, 61, 32)   96          conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 61, 61, 64)   0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 61, 61, 64)   0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 61, 61, 96)   0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 61, 61, 32)   0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed0 (Concatenate)            (None, 61, 61, 256)  0           activation_6[0][0]               \n",
      "                                                                 activation_8[0][0]               \n",
      "                                                                 activation_11[0][0]              \n",
      "                                                                 activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 61, 61, 64)   16384       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 61, 61, 64)   192         conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 61, 61, 64)   0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 61, 61, 48)   12288       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 61, 61, 96)   55296       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 61, 61, 48)   144         conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 61, 61, 96)   288         conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 61, 61, 48)   0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 61, 61, 96)   0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_2 (AveragePoo (None, 61, 61, 256)  0           mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 61, 61, 64)   16384       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 61, 61, 64)   76800       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 61, 61, 96)   82944       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 61, 61, 64)   16384       average_pooling2d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 61, 61, 64)   192         conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 61, 61, 64)   192         conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 61, 61, 96)   288         conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 61, 61, 64)   192         conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 61, 61, 64)   0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 61, 61, 64)   0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 61, 61, 96)   0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 61, 61, 64)   0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed1 (Concatenate)            (None, 61, 61, 288)  0           activation_13[0][0]              \n",
      "                                                                 activation_15[0][0]              \n",
      "                                                                 activation_18[0][0]              \n",
      "                                                                 activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, 61, 61, 64)   18432       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 61, 61, 64)   192         conv2d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 61, 61, 64)   0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 61, 61, 48)   13824       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, 61, 61, 96)   55296       activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 61, 61, 48)   144         conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 61, 61, 96)   288         conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 61, 61, 48)   0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 61, 61, 96)   0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_3 (AveragePoo (None, 61, 61, 288)  0           mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 61, 61, 64)   18432       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, 61, 61, 64)   76800       activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, 61, 61, 96)   82944       activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, 61, 61, 64)   18432       average_pooling2d_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 61, 61, 64)   192         conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 61, 61, 64)   192         conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 61, 61, 96)   288         conv2d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 61, 61, 64)   192         conv2d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 61, 61, 64)   0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 61, 61, 64)   0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 61, 61, 96)   0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 61, 61, 64)   0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed2 (Concatenate)            (None, 61, 61, 288)  0           activation_20[0][0]              \n",
      "                                                                 activation_22[0][0]              \n",
      "                                                                 activation_25[0][0]              \n",
      "                                                                 activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)              (None, 61, 61, 64)   18432       mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 61, 61, 64)   192         conv2d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 61, 61, 64)   0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_47 (Conv2D)              (None, 61, 61, 96)   55296       activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 61, 61, 96)   288         conv2d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 61, 61, 96)   0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (None, 30, 30, 384)  995328      mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)              (None, 30, 30, 96)   82944       activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 30, 30, 384)  1152        conv2d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 30, 30, 96)   288         conv2d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 30, 30, 384)  0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 30, 30, 96)   0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_21 (MaxPooling2D) (None, 30, 30, 288)  0           mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "mixed3 (Concatenate)            (None, 30, 30, 768)  0           activation_27[0][0]              \n",
      "                                                                 activation_30[0][0]              \n",
      "                                                                 max_pooling2d_21[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_53 (Conv2D)              (None, 30, 30, 128)  98304       mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 30, 30, 128)  384         conv2d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 30, 30, 128)  0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_54 (Conv2D)              (None, 30, 30, 128)  114688      activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 30, 30, 128)  384         conv2d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 30, 30, 128)  0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_50 (Conv2D)              (None, 30, 30, 128)  98304       mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_55 (Conv2D)              (None, 30, 30, 128)  114688      activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 30, 30, 128)  384         conv2d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 30, 30, 128)  384         conv2d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 30, 30, 128)  0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 30, 30, 128)  0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_51 (Conv2D)              (None, 30, 30, 128)  114688      activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_56 (Conv2D)              (None, 30, 30, 128)  114688      activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 30, 30, 128)  384         conv2d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 30, 30, 128)  384         conv2d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 30, 30, 128)  0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 30, 30, 128)  0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_4 (AveragePoo (None, 30, 30, 768)  0           mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_49 (Conv2D)              (None, 30, 30, 192)  147456      mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_52 (Conv2D)              (None, 30, 30, 192)  172032      activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_57 (Conv2D)              (None, 30, 30, 192)  172032      activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_58 (Conv2D)              (None, 30, 30, 192)  147456      average_pooling2d_4[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 30, 30, 192)  576         conv2d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 30, 30, 192)  576         conv2d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 30, 30, 192)  576         conv2d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNo (None, 30, 30, 192)  576         conv2d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 30, 30, 192)  0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 30, 30, 192)  0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 30, 30, 192)  0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 30, 30, 192)  0           batch_normalization_43[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed4 (Concatenate)            (None, 30, 30, 768)  0           activation_31[0][0]              \n",
      "                                                                 activation_34[0][0]              \n",
      "                                                                 activation_39[0][0]              \n",
      "                                                                 activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_63 (Conv2D)              (None, 30, 30, 160)  122880      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_48 (BatchNo (None, 30, 30, 160)  480         conv2d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 30, 30, 160)  0           batch_normalization_48[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_64 (Conv2D)              (None, 30, 30, 160)  179200      activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_49 (BatchNo (None, 30, 30, 160)  480         conv2d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 30, 30, 160)  0           batch_normalization_49[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_60 (Conv2D)              (None, 30, 30, 160)  122880      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_65 (Conv2D)              (None, 30, 30, 160)  179200      activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNo (None, 30, 30, 160)  480         conv2d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_50 (BatchNo (None, 30, 30, 160)  480         conv2d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 30, 30, 160)  0           batch_normalization_45[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 30, 30, 160)  0           batch_normalization_50[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_61 (Conv2D)              (None, 30, 30, 160)  179200      activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_66 (Conv2D)              (None, 30, 30, 160)  179200      activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNo (None, 30, 30, 160)  480         conv2d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_51 (BatchNo (None, 30, 30, 160)  480         conv2d_66[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 30, 30, 160)  0           batch_normalization_46[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 30, 30, 160)  0           batch_normalization_51[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_5 (AveragePoo (None, 30, 30, 768)  0           mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_59 (Conv2D)              (None, 30, 30, 192)  147456      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_62 (Conv2D)              (None, 30, 30, 192)  215040      activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_67 (Conv2D)              (None, 30, 30, 192)  215040      activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_68 (Conv2D)              (None, 30, 30, 192)  147456      average_pooling2d_5[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, 30, 30, 192)  576         conv2d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_47 (BatchNo (None, 30, 30, 192)  576         conv2d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_52 (BatchNo (None, 30, 30, 192)  576         conv2d_67[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_53 (BatchNo (None, 30, 30, 192)  576         conv2d_68[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 30, 30, 192)  0           batch_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 30, 30, 192)  0           batch_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 30, 30, 192)  0           batch_normalization_52[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 30, 30, 192)  0           batch_normalization_53[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed5 (Concatenate)            (None, 30, 30, 768)  0           activation_41[0][0]              \n",
      "                                                                 activation_44[0][0]              \n",
      "                                                                 activation_49[0][0]              \n",
      "                                                                 activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_73 (Conv2D)              (None, 30, 30, 160)  122880      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_58 (BatchNo (None, 30, 30, 160)  480         conv2d_73[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 30, 30, 160)  0           batch_normalization_58[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_74 (Conv2D)              (None, 30, 30, 160)  179200      activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_59 (BatchNo (None, 30, 30, 160)  480         conv2d_74[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 30, 30, 160)  0           batch_normalization_59[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_70 (Conv2D)              (None, 30, 30, 160)  122880      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_75 (Conv2D)              (None, 30, 30, 160)  179200      activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_55 (BatchNo (None, 30, 30, 160)  480         conv2d_70[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_60 (BatchNo (None, 30, 30, 160)  480         conv2d_75[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 30, 30, 160)  0           batch_normalization_55[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 30, 30, 160)  0           batch_normalization_60[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_71 (Conv2D)              (None, 30, 30, 160)  179200      activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_76 (Conv2D)              (None, 30, 30, 160)  179200      activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_56 (BatchNo (None, 30, 30, 160)  480         conv2d_71[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_61 (BatchNo (None, 30, 30, 160)  480         conv2d_76[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 30, 30, 160)  0           batch_normalization_56[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 30, 30, 160)  0           batch_normalization_61[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_6 (AveragePoo (None, 30, 30, 768)  0           mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_69 (Conv2D)              (None, 30, 30, 192)  147456      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_72 (Conv2D)              (None, 30, 30, 192)  215040      activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_77 (Conv2D)              (None, 30, 30, 192)  215040      activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_78 (Conv2D)              (None, 30, 30, 192)  147456      average_pooling2d_6[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_54 (BatchNo (None, 30, 30, 192)  576         conv2d_69[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_57 (BatchNo (None, 30, 30, 192)  576         conv2d_72[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_62 (BatchNo (None, 30, 30, 192)  576         conv2d_77[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_63 (BatchNo (None, 30, 30, 192)  576         conv2d_78[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 30, 30, 192)  0           batch_normalization_54[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 30, 30, 192)  0           batch_normalization_57[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 30, 30, 192)  0           batch_normalization_62[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 30, 30, 192)  0           batch_normalization_63[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed6 (Concatenate)            (None, 30, 30, 768)  0           activation_51[0][0]              \n",
      "                                                                 activation_54[0][0]              \n",
      "                                                                 activation_59[0][0]              \n",
      "                                                                 activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_83 (Conv2D)              (None, 30, 30, 192)  147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_68 (BatchNo (None, 30, 30, 192)  576         conv2d_83[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 30, 30, 192)  0           batch_normalization_68[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_84 (Conv2D)              (None, 30, 30, 192)  258048      activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_69 (BatchNo (None, 30, 30, 192)  576         conv2d_84[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_66 (Activation)      (None, 30, 30, 192)  0           batch_normalization_69[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_80 (Conv2D)              (None, 30, 30, 192)  147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_85 (Conv2D)              (None, 30, 30, 192)  258048      activation_66[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_65 (BatchNo (None, 30, 30, 192)  576         conv2d_80[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_70 (BatchNo (None, 30, 30, 192)  576         conv2d_85[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 30, 30, 192)  0           batch_normalization_65[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_67 (Activation)      (None, 30, 30, 192)  0           batch_normalization_70[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_81 (Conv2D)              (None, 30, 30, 192)  258048      activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_86 (Conv2D)              (None, 30, 30, 192)  258048      activation_67[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_66 (BatchNo (None, 30, 30, 192)  576         conv2d_81[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_71 (BatchNo (None, 30, 30, 192)  576         conv2d_86[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 30, 30, 192)  0           batch_normalization_66[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_68 (Activation)      (None, 30, 30, 192)  0           batch_normalization_71[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_7 (AveragePoo (None, 30, 30, 768)  0           mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_79 (Conv2D)              (None, 30, 30, 192)  147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_82 (Conv2D)              (None, 30, 30, 192)  258048      activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_87 (Conv2D)              (None, 30, 30, 192)  258048      activation_68[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_88 (Conv2D)              (None, 30, 30, 192)  147456      average_pooling2d_7[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_64 (BatchNo (None, 30, 30, 192)  576         conv2d_79[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_67 (BatchNo (None, 30, 30, 192)  576         conv2d_82[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_72 (BatchNo (None, 30, 30, 192)  576         conv2d_87[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_73 (BatchNo (None, 30, 30, 192)  576         conv2d_88[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 30, 30, 192)  0           batch_normalization_64[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 30, 30, 192)  0           batch_normalization_67[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_69 (Activation)      (None, 30, 30, 192)  0           batch_normalization_72[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_70 (Activation)      (None, 30, 30, 192)  0           batch_normalization_73[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed7 (Concatenate)            (None, 30, 30, 768)  0           activation_61[0][0]              \n",
      "                                                                 activation_64[0][0]              \n",
      "                                                                 activation_69[0][0]              \n",
      "                                                                 activation_70[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_91 (Conv2D)              (None, 30, 30, 192)  147456      mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_76 (BatchNo (None, 30, 30, 192)  576         conv2d_91[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_73 (Activation)      (None, 30, 30, 192)  0           batch_normalization_76[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_92 (Conv2D)              (None, 30, 30, 192)  258048      activation_73[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_77 (BatchNo (None, 30, 30, 192)  576         conv2d_92[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_74 (Activation)      (None, 30, 30, 192)  0           batch_normalization_77[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_89 (Conv2D)              (None, 30, 30, 192)  147456      mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_93 (Conv2D)              (None, 30, 30, 192)  258048      activation_74[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_74 (BatchNo (None, 30, 30, 192)  576         conv2d_89[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_78 (BatchNo (None, 30, 30, 192)  576         conv2d_93[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_71 (Activation)      (None, 30, 30, 192)  0           batch_normalization_74[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_75 (Activation)      (None, 30, 30, 192)  0           batch_normalization_78[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_90 (Conv2D)              (None, 14, 14, 320)  552960      activation_71[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_94 (Conv2D)              (None, 14, 14, 192)  331776      activation_75[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_75 (BatchNo (None, 14, 14, 320)  960         conv2d_90[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_79 (BatchNo (None, 14, 14, 192)  576         conv2d_94[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_72 (Activation)      (None, 14, 14, 320)  0           batch_normalization_75[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_76 (Activation)      (None, 14, 14, 192)  0           batch_normalization_79[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_22 (MaxPooling2D) (None, 14, 14, 768)  0           mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "mixed8 (Concatenate)            (None, 14, 14, 1280) 0           activation_72[0][0]              \n",
      "                                                                 activation_76[0][0]              \n",
      "                                                                 max_pooling2d_22[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_99 (Conv2D)              (None, 14, 14, 448)  573440      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_84 (BatchNo (None, 14, 14, 448)  1344        conv2d_99[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_81 (Activation)      (None, 14, 14, 448)  0           batch_normalization_84[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_96 (Conv2D)              (None, 14, 14, 384)  491520      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_100 (Conv2D)             (None, 14, 14, 384)  1548288     activation_81[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_81 (BatchNo (None, 14, 14, 384)  1152        conv2d_96[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_85 (BatchNo (None, 14, 14, 384)  1152        conv2d_100[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_78 (Activation)      (None, 14, 14, 384)  0           batch_normalization_81[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_82 (Activation)      (None, 14, 14, 384)  0           batch_normalization_85[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_97 (Conv2D)              (None, 14, 14, 384)  442368      activation_78[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_98 (Conv2D)              (None, 14, 14, 384)  442368      activation_78[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_101 (Conv2D)             (None, 14, 14, 384)  442368      activation_82[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_102 (Conv2D)             (None, 14, 14, 384)  442368      activation_82[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_8 (AveragePoo (None, 14, 14, 1280) 0           mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_95 (Conv2D)              (None, 14, 14, 320)  409600      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_82 (BatchNo (None, 14, 14, 384)  1152        conv2d_97[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_83 (BatchNo (None, 14, 14, 384)  1152        conv2d_98[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_86 (BatchNo (None, 14, 14, 384)  1152        conv2d_101[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_87 (BatchNo (None, 14, 14, 384)  1152        conv2d_102[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_103 (Conv2D)             (None, 14, 14, 192)  245760      average_pooling2d_8[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_80 (BatchNo (None, 14, 14, 320)  960         conv2d_95[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_79 (Activation)      (None, 14, 14, 384)  0           batch_normalization_82[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_80 (Activation)      (None, 14, 14, 384)  0           batch_normalization_83[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_83 (Activation)      (None, 14, 14, 384)  0           batch_normalization_86[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_84 (Activation)      (None, 14, 14, 384)  0           batch_normalization_87[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_88 (BatchNo (None, 14, 14, 192)  576         conv2d_103[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_77 (Activation)      (None, 14, 14, 320)  0           batch_normalization_80[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed9_0 (Concatenate)          (None, 14, 14, 768)  0           activation_79[0][0]              \n",
      "                                                                 activation_80[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 14, 14, 768)  0           activation_83[0][0]              \n",
      "                                                                 activation_84[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_85 (Activation)      (None, 14, 14, 192)  0           batch_normalization_88[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed9 (Concatenate)            (None, 14, 14, 2048) 0           activation_77[0][0]              \n",
      "                                                                 mixed9_0[0][0]                   \n",
      "                                                                 concatenate_1[0][0]              \n",
      "                                                                 activation_85[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_108 (Conv2D)             (None, 14, 14, 448)  917504      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_93 (BatchNo (None, 14, 14, 448)  1344        conv2d_108[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_90 (Activation)      (None, 14, 14, 448)  0           batch_normalization_93[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_105 (Conv2D)             (None, 14, 14, 384)  786432      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_109 (Conv2D)             (None, 14, 14, 384)  1548288     activation_90[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_90 (BatchNo (None, 14, 14, 384)  1152        conv2d_105[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_94 (BatchNo (None, 14, 14, 384)  1152        conv2d_109[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_87 (Activation)      (None, 14, 14, 384)  0           batch_normalization_90[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_91 (Activation)      (None, 14, 14, 384)  0           batch_normalization_94[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_106 (Conv2D)             (None, 14, 14, 384)  442368      activation_87[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_107 (Conv2D)             (None, 14, 14, 384)  442368      activation_87[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_110 (Conv2D)             (None, 14, 14, 384)  442368      activation_91[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_111 (Conv2D)             (None, 14, 14, 384)  442368      activation_91[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_9 (AveragePoo (None, 14, 14, 2048) 0           mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_104 (Conv2D)             (None, 14, 14, 320)  655360      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_91 (BatchNo (None, 14, 14, 384)  1152        conv2d_106[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_92 (BatchNo (None, 14, 14, 384)  1152        conv2d_107[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_95 (BatchNo (None, 14, 14, 384)  1152        conv2d_110[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_96 (BatchNo (None, 14, 14, 384)  1152        conv2d_111[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_112 (Conv2D)             (None, 14, 14, 192)  393216      average_pooling2d_9[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_89 (BatchNo (None, 14, 14, 320)  960         conv2d_104[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_88 (Activation)      (None, 14, 14, 384)  0           batch_normalization_91[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_89 (Activation)      (None, 14, 14, 384)  0           batch_normalization_92[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_92 (Activation)      (None, 14, 14, 384)  0           batch_normalization_95[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_93 (Activation)      (None, 14, 14, 384)  0           batch_normalization_96[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_97 (BatchNo (None, 14, 14, 192)  576         conv2d_112[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_86 (Activation)      (None, 14, 14, 320)  0           batch_normalization_89[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed9_1 (Concatenate)          (None, 14, 14, 768)  0           activation_88[0][0]              \n",
      "                                                                 activation_89[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 14, 14, 768)  0           activation_92[0][0]              \n",
      "                                                                 activation_93[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_94 (Activation)      (None, 14, 14, 192)  0           batch_normalization_97[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed10 (Concatenate)           (None, 14, 14, 2048) 0           activation_86[0][0]              \n",
      "                                                                 mixed9_1[0][0]                   \n",
      "                                                                 concatenate_2[0][0]              \n",
      "                                                                 activation_94[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_1 (Glo (None, 2048)         0           mixed10[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 1024)         2098176     global_average_pooling2d_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 5)            5125        dense_7[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 23,906,085\n",
      "Trainable params: 23,871,653\n",
      "Non-trainable params: 34,432\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/10\n",
      "8/8 [==============================] - 36s 4s/step - loss: 2.4896 - acc: 0.3324 - val_loss: 10.8906 - val_acc: 0.3243\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 7s 907ms/step - loss: 2.2721 - acc: 0.4461 - val_loss: 1.9460 - val_acc: 0.3784\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - 7s 919ms/step - loss: 2.0516 - acc: 0.4747 - val_loss: 1.3430 - val_acc: 0.3784\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - 7s 930ms/step - loss: 1.9796 - acc: 0.4747 - val_loss: 10.0194 - val_acc: 0.3784\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - 8s 988ms/step - loss: 2.2769 - acc: 0.3924 - val_loss: 10.0194 - val_acc: 0.3784\n",
      "Epoch 6/10\n",
      "8/8 [==============================] - 9s 1s/step - loss: 2.3043 - acc: 0.4302 - val_loss: 10.0194 - val_acc: 0.3784\n",
      "Epoch 7/10\n",
      "8/8 [==============================] - 9s 1s/step - loss: 1.6581 - acc: 0.3765 - val_loss: 10.0194 - val_acc: 0.3784\n",
      "Epoch 8/10\n",
      "8/8 [==============================] - 9s 1s/step - loss: 2.4737 - acc: 0.4525 - val_loss: 10.0194 - val_acc: 0.3784\n",
      "Epoch 9/10\n",
      "8/8 [==============================] - 9s 1s/step - loss: 1.5848 - acc: 0.4747 - val_loss: 10.0194 - val_acc: 0.3784\n",
      "Epoch 10/10\n",
      "8/8 [==============================] - 9s 1s/step - loss: 2.1957 - acc: 0.4747 - val_loss: 9.6732 - val_acc: 0.3784\n"
     ]
    }
   ],
   "source": [
    "model = create_inception_model()\n",
    "loss, accuracy = train_inception_gen_model(model, 20, 10, X_train, y_train, X_test, y_test, 90, 0.5, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "oyFu0aW33hz7",
    "outputId": "449313f2-1ee6-4fbb-85c9-7329f61b11e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 9.673247234241382\n",
      "Test accuracy: 0.37837838160025106\n"
     ]
    }
   ],
   "source": [
    "print('Test loss:', loss)\n",
    "print('Test accuracy:', accuracy) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 12181
    },
    "colab_type": "code",
    "id": "-lV33ssw0b8V",
    "outputId": "0e772041-7806-465a-964a-6fb5afc9c99e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 512, 512, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_113 (Conv2D)             (None, 255, 255, 32) 864         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_98 (BatchNo (None, 255, 255, 32) 96          conv2d_113[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_95 (Activation)      (None, 255, 255, 32) 0           batch_normalization_98[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_114 (Conv2D)             (None, 253, 253, 32) 9216        activation_95[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_99 (BatchNo (None, 253, 253, 32) 96          conv2d_114[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_96 (Activation)      (None, 253, 253, 32) 0           batch_normalization_99[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_115 (Conv2D)             (None, 253, 253, 64) 18432       activation_96[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_100 (BatchN (None, 253, 253, 64) 192         conv2d_115[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_97 (Activation)      (None, 253, 253, 64) 0           batch_normalization_100[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_23 (MaxPooling2D) (None, 126, 126, 64) 0           activation_97[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_116 (Conv2D)             (None, 126, 126, 80) 5120        max_pooling2d_23[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_101 (BatchN (None, 126, 126, 80) 240         conv2d_116[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_98 (Activation)      (None, 126, 126, 80) 0           batch_normalization_101[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_117 (Conv2D)             (None, 124, 124, 192 138240      activation_98[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_102 (BatchN (None, 124, 124, 192 576         conv2d_117[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_99 (Activation)      (None, 124, 124, 192 0           batch_normalization_102[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_24 (MaxPooling2D) (None, 61, 61, 192)  0           activation_99[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_121 (Conv2D)             (None, 61, 61, 64)   12288       max_pooling2d_24[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_106 (BatchN (None, 61, 61, 64)   192         conv2d_121[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_103 (Activation)     (None, 61, 61, 64)   0           batch_normalization_106[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_119 (Conv2D)             (None, 61, 61, 48)   9216        max_pooling2d_24[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_122 (Conv2D)             (None, 61, 61, 96)   55296       activation_103[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_104 (BatchN (None, 61, 61, 48)   144         conv2d_119[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_107 (BatchN (None, 61, 61, 96)   288         conv2d_122[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_101 (Activation)     (None, 61, 61, 48)   0           batch_normalization_104[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_104 (Activation)     (None, 61, 61, 96)   0           batch_normalization_107[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_10 (AveragePo (None, 61, 61, 192)  0           max_pooling2d_24[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_118 (Conv2D)             (None, 61, 61, 64)   12288       max_pooling2d_24[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_120 (Conv2D)             (None, 61, 61, 64)   76800       activation_101[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_123 (Conv2D)             (None, 61, 61, 96)   82944       activation_104[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_124 (Conv2D)             (None, 61, 61, 32)   6144        average_pooling2d_10[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_103 (BatchN (None, 61, 61, 64)   192         conv2d_118[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_105 (BatchN (None, 61, 61, 64)   192         conv2d_120[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_108 (BatchN (None, 61, 61, 96)   288         conv2d_123[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_109 (BatchN (None, 61, 61, 32)   96          conv2d_124[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_100 (Activation)     (None, 61, 61, 64)   0           batch_normalization_103[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_102 (Activation)     (None, 61, 61, 64)   0           batch_normalization_105[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_105 (Activation)     (None, 61, 61, 96)   0           batch_normalization_108[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_106 (Activation)     (None, 61, 61, 32)   0           batch_normalization_109[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed0 (Concatenate)            (None, 61, 61, 256)  0           activation_100[0][0]             \n",
      "                                                                 activation_102[0][0]             \n",
      "                                                                 activation_105[0][0]             \n",
      "                                                                 activation_106[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_128 (Conv2D)             (None, 61, 61, 64)   16384       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_113 (BatchN (None, 61, 61, 64)   192         conv2d_128[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_110 (Activation)     (None, 61, 61, 64)   0           batch_normalization_113[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_126 (Conv2D)             (None, 61, 61, 48)   12288       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_129 (Conv2D)             (None, 61, 61, 96)   55296       activation_110[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_111 (BatchN (None, 61, 61, 48)   144         conv2d_126[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_114 (BatchN (None, 61, 61, 96)   288         conv2d_129[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_108 (Activation)     (None, 61, 61, 48)   0           batch_normalization_111[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_111 (Activation)     (None, 61, 61, 96)   0           batch_normalization_114[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_11 (AveragePo (None, 61, 61, 256)  0           mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_125 (Conv2D)             (None, 61, 61, 64)   16384       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_127 (Conv2D)             (None, 61, 61, 64)   76800       activation_108[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_130 (Conv2D)             (None, 61, 61, 96)   82944       activation_111[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_131 (Conv2D)             (None, 61, 61, 64)   16384       average_pooling2d_11[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_110 (BatchN (None, 61, 61, 64)   192         conv2d_125[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_112 (BatchN (None, 61, 61, 64)   192         conv2d_127[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_115 (BatchN (None, 61, 61, 96)   288         conv2d_130[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_116 (BatchN (None, 61, 61, 64)   192         conv2d_131[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_107 (Activation)     (None, 61, 61, 64)   0           batch_normalization_110[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_109 (Activation)     (None, 61, 61, 64)   0           batch_normalization_112[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_112 (Activation)     (None, 61, 61, 96)   0           batch_normalization_115[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_113 (Activation)     (None, 61, 61, 64)   0           batch_normalization_116[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed1 (Concatenate)            (None, 61, 61, 288)  0           activation_107[0][0]             \n",
      "                                                                 activation_109[0][0]             \n",
      "                                                                 activation_112[0][0]             \n",
      "                                                                 activation_113[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_135 (Conv2D)             (None, 61, 61, 64)   18432       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_120 (BatchN (None, 61, 61, 64)   192         conv2d_135[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_117 (Activation)     (None, 61, 61, 64)   0           batch_normalization_120[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_133 (Conv2D)             (None, 61, 61, 48)   13824       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_136 (Conv2D)             (None, 61, 61, 96)   55296       activation_117[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_118 (BatchN (None, 61, 61, 48)   144         conv2d_133[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_121 (BatchN (None, 61, 61, 96)   288         conv2d_136[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_115 (Activation)     (None, 61, 61, 48)   0           batch_normalization_118[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_118 (Activation)     (None, 61, 61, 96)   0           batch_normalization_121[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_12 (AveragePo (None, 61, 61, 288)  0           mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_132 (Conv2D)             (None, 61, 61, 64)   18432       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_134 (Conv2D)             (None, 61, 61, 64)   76800       activation_115[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_137 (Conv2D)             (None, 61, 61, 96)   82944       activation_118[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_138 (Conv2D)             (None, 61, 61, 64)   18432       average_pooling2d_12[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_117 (BatchN (None, 61, 61, 64)   192         conv2d_132[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_119 (BatchN (None, 61, 61, 64)   192         conv2d_134[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_122 (BatchN (None, 61, 61, 96)   288         conv2d_137[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_123 (BatchN (None, 61, 61, 64)   192         conv2d_138[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_114 (Activation)     (None, 61, 61, 64)   0           batch_normalization_117[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_116 (Activation)     (None, 61, 61, 64)   0           batch_normalization_119[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_119 (Activation)     (None, 61, 61, 96)   0           batch_normalization_122[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_120 (Activation)     (None, 61, 61, 64)   0           batch_normalization_123[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed2 (Concatenate)            (None, 61, 61, 288)  0           activation_114[0][0]             \n",
      "                                                                 activation_116[0][0]             \n",
      "                                                                 activation_119[0][0]             \n",
      "                                                                 activation_120[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_140 (Conv2D)             (None, 61, 61, 64)   18432       mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_125 (BatchN (None, 61, 61, 64)   192         conv2d_140[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_122 (Activation)     (None, 61, 61, 64)   0           batch_normalization_125[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_141 (Conv2D)             (None, 61, 61, 96)   55296       activation_122[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_126 (BatchN (None, 61, 61, 96)   288         conv2d_141[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_123 (Activation)     (None, 61, 61, 96)   0           batch_normalization_126[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_139 (Conv2D)             (None, 30, 30, 384)  995328      mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_142 (Conv2D)             (None, 30, 30, 96)   82944       activation_123[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_124 (BatchN (None, 30, 30, 384)  1152        conv2d_139[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_127 (BatchN (None, 30, 30, 96)   288         conv2d_142[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_121 (Activation)     (None, 30, 30, 384)  0           batch_normalization_124[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_124 (Activation)     (None, 30, 30, 96)   0           batch_normalization_127[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_25 (MaxPooling2D) (None, 30, 30, 288)  0           mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "mixed3 (Concatenate)            (None, 30, 30, 768)  0           activation_121[0][0]             \n",
      "                                                                 activation_124[0][0]             \n",
      "                                                                 max_pooling2d_25[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_147 (Conv2D)             (None, 30, 30, 128)  98304       mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_132 (BatchN (None, 30, 30, 128)  384         conv2d_147[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_129 (Activation)     (None, 30, 30, 128)  0           batch_normalization_132[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_148 (Conv2D)             (None, 30, 30, 128)  114688      activation_129[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_133 (BatchN (None, 30, 30, 128)  384         conv2d_148[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_130 (Activation)     (None, 30, 30, 128)  0           batch_normalization_133[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_144 (Conv2D)             (None, 30, 30, 128)  98304       mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_149 (Conv2D)             (None, 30, 30, 128)  114688      activation_130[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_129 (BatchN (None, 30, 30, 128)  384         conv2d_144[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_134 (BatchN (None, 30, 30, 128)  384         conv2d_149[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_126 (Activation)     (None, 30, 30, 128)  0           batch_normalization_129[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_131 (Activation)     (None, 30, 30, 128)  0           batch_normalization_134[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_145 (Conv2D)             (None, 30, 30, 128)  114688      activation_126[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_150 (Conv2D)             (None, 30, 30, 128)  114688      activation_131[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_130 (BatchN (None, 30, 30, 128)  384         conv2d_145[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_135 (BatchN (None, 30, 30, 128)  384         conv2d_150[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_127 (Activation)     (None, 30, 30, 128)  0           batch_normalization_130[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_132 (Activation)     (None, 30, 30, 128)  0           batch_normalization_135[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_13 (AveragePo (None, 30, 30, 768)  0           mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_143 (Conv2D)             (None, 30, 30, 192)  147456      mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_146 (Conv2D)             (None, 30, 30, 192)  172032      activation_127[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_151 (Conv2D)             (None, 30, 30, 192)  172032      activation_132[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_152 (Conv2D)             (None, 30, 30, 192)  147456      average_pooling2d_13[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_128 (BatchN (None, 30, 30, 192)  576         conv2d_143[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_131 (BatchN (None, 30, 30, 192)  576         conv2d_146[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_136 (BatchN (None, 30, 30, 192)  576         conv2d_151[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_137 (BatchN (None, 30, 30, 192)  576         conv2d_152[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_125 (Activation)     (None, 30, 30, 192)  0           batch_normalization_128[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_128 (Activation)     (None, 30, 30, 192)  0           batch_normalization_131[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_133 (Activation)     (None, 30, 30, 192)  0           batch_normalization_136[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_134 (Activation)     (None, 30, 30, 192)  0           batch_normalization_137[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed4 (Concatenate)            (None, 30, 30, 768)  0           activation_125[0][0]             \n",
      "                                                                 activation_128[0][0]             \n",
      "                                                                 activation_133[0][0]             \n",
      "                                                                 activation_134[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_157 (Conv2D)             (None, 30, 30, 160)  122880      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_142 (BatchN (None, 30, 30, 160)  480         conv2d_157[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_139 (Activation)     (None, 30, 30, 160)  0           batch_normalization_142[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_158 (Conv2D)             (None, 30, 30, 160)  179200      activation_139[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_143 (BatchN (None, 30, 30, 160)  480         conv2d_158[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_140 (Activation)     (None, 30, 30, 160)  0           batch_normalization_143[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_154 (Conv2D)             (None, 30, 30, 160)  122880      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_159 (Conv2D)             (None, 30, 30, 160)  179200      activation_140[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_139 (BatchN (None, 30, 30, 160)  480         conv2d_154[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_144 (BatchN (None, 30, 30, 160)  480         conv2d_159[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_136 (Activation)     (None, 30, 30, 160)  0           batch_normalization_139[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_141 (Activation)     (None, 30, 30, 160)  0           batch_normalization_144[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_155 (Conv2D)             (None, 30, 30, 160)  179200      activation_136[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_160 (Conv2D)             (None, 30, 30, 160)  179200      activation_141[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_140 (BatchN (None, 30, 30, 160)  480         conv2d_155[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_145 (BatchN (None, 30, 30, 160)  480         conv2d_160[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_137 (Activation)     (None, 30, 30, 160)  0           batch_normalization_140[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_142 (Activation)     (None, 30, 30, 160)  0           batch_normalization_145[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_14 (AveragePo (None, 30, 30, 768)  0           mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_153 (Conv2D)             (None, 30, 30, 192)  147456      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_156 (Conv2D)             (None, 30, 30, 192)  215040      activation_137[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_161 (Conv2D)             (None, 30, 30, 192)  215040      activation_142[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_162 (Conv2D)             (None, 30, 30, 192)  147456      average_pooling2d_14[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_138 (BatchN (None, 30, 30, 192)  576         conv2d_153[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_141 (BatchN (None, 30, 30, 192)  576         conv2d_156[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_146 (BatchN (None, 30, 30, 192)  576         conv2d_161[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_147 (BatchN (None, 30, 30, 192)  576         conv2d_162[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_135 (Activation)     (None, 30, 30, 192)  0           batch_normalization_138[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_138 (Activation)     (None, 30, 30, 192)  0           batch_normalization_141[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_143 (Activation)     (None, 30, 30, 192)  0           batch_normalization_146[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_144 (Activation)     (None, 30, 30, 192)  0           batch_normalization_147[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed5 (Concatenate)            (None, 30, 30, 768)  0           activation_135[0][0]             \n",
      "                                                                 activation_138[0][0]             \n",
      "                                                                 activation_143[0][0]             \n",
      "                                                                 activation_144[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_167 (Conv2D)             (None, 30, 30, 160)  122880      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_152 (BatchN (None, 30, 30, 160)  480         conv2d_167[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_149 (Activation)     (None, 30, 30, 160)  0           batch_normalization_152[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_168 (Conv2D)             (None, 30, 30, 160)  179200      activation_149[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_153 (BatchN (None, 30, 30, 160)  480         conv2d_168[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_150 (Activation)     (None, 30, 30, 160)  0           batch_normalization_153[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_164 (Conv2D)             (None, 30, 30, 160)  122880      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_169 (Conv2D)             (None, 30, 30, 160)  179200      activation_150[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_149 (BatchN (None, 30, 30, 160)  480         conv2d_164[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_154 (BatchN (None, 30, 30, 160)  480         conv2d_169[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_146 (Activation)     (None, 30, 30, 160)  0           batch_normalization_149[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_151 (Activation)     (None, 30, 30, 160)  0           batch_normalization_154[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_165 (Conv2D)             (None, 30, 30, 160)  179200      activation_146[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_170 (Conv2D)             (None, 30, 30, 160)  179200      activation_151[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_150 (BatchN (None, 30, 30, 160)  480         conv2d_165[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_155 (BatchN (None, 30, 30, 160)  480         conv2d_170[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_147 (Activation)     (None, 30, 30, 160)  0           batch_normalization_150[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_152 (Activation)     (None, 30, 30, 160)  0           batch_normalization_155[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_15 (AveragePo (None, 30, 30, 768)  0           mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_163 (Conv2D)             (None, 30, 30, 192)  147456      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_166 (Conv2D)             (None, 30, 30, 192)  215040      activation_147[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_171 (Conv2D)             (None, 30, 30, 192)  215040      activation_152[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_172 (Conv2D)             (None, 30, 30, 192)  147456      average_pooling2d_15[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_148 (BatchN (None, 30, 30, 192)  576         conv2d_163[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_151 (BatchN (None, 30, 30, 192)  576         conv2d_166[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_156 (BatchN (None, 30, 30, 192)  576         conv2d_171[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_157 (BatchN (None, 30, 30, 192)  576         conv2d_172[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_145 (Activation)     (None, 30, 30, 192)  0           batch_normalization_148[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_148 (Activation)     (None, 30, 30, 192)  0           batch_normalization_151[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_153 (Activation)     (None, 30, 30, 192)  0           batch_normalization_156[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_154 (Activation)     (None, 30, 30, 192)  0           batch_normalization_157[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed6 (Concatenate)            (None, 30, 30, 768)  0           activation_145[0][0]             \n",
      "                                                                 activation_148[0][0]             \n",
      "                                                                 activation_153[0][0]             \n",
      "                                                                 activation_154[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_177 (Conv2D)             (None, 30, 30, 192)  147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_162 (BatchN (None, 30, 30, 192)  576         conv2d_177[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_159 (Activation)     (None, 30, 30, 192)  0           batch_normalization_162[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_178 (Conv2D)             (None, 30, 30, 192)  258048      activation_159[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_163 (BatchN (None, 30, 30, 192)  576         conv2d_178[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_160 (Activation)     (None, 30, 30, 192)  0           batch_normalization_163[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_174 (Conv2D)             (None, 30, 30, 192)  147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_179 (Conv2D)             (None, 30, 30, 192)  258048      activation_160[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_159 (BatchN (None, 30, 30, 192)  576         conv2d_174[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_164 (BatchN (None, 30, 30, 192)  576         conv2d_179[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_156 (Activation)     (None, 30, 30, 192)  0           batch_normalization_159[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_161 (Activation)     (None, 30, 30, 192)  0           batch_normalization_164[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_175 (Conv2D)             (None, 30, 30, 192)  258048      activation_156[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_180 (Conv2D)             (None, 30, 30, 192)  258048      activation_161[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_160 (BatchN (None, 30, 30, 192)  576         conv2d_175[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_165 (BatchN (None, 30, 30, 192)  576         conv2d_180[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_157 (Activation)     (None, 30, 30, 192)  0           batch_normalization_160[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_162 (Activation)     (None, 30, 30, 192)  0           batch_normalization_165[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_16 (AveragePo (None, 30, 30, 768)  0           mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_173 (Conv2D)             (None, 30, 30, 192)  147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_176 (Conv2D)             (None, 30, 30, 192)  258048      activation_157[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_181 (Conv2D)             (None, 30, 30, 192)  258048      activation_162[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_182 (Conv2D)             (None, 30, 30, 192)  147456      average_pooling2d_16[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_158 (BatchN (None, 30, 30, 192)  576         conv2d_173[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_161 (BatchN (None, 30, 30, 192)  576         conv2d_176[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_166 (BatchN (None, 30, 30, 192)  576         conv2d_181[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_167 (BatchN (None, 30, 30, 192)  576         conv2d_182[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_155 (Activation)     (None, 30, 30, 192)  0           batch_normalization_158[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_158 (Activation)     (None, 30, 30, 192)  0           batch_normalization_161[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_163 (Activation)     (None, 30, 30, 192)  0           batch_normalization_166[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_164 (Activation)     (None, 30, 30, 192)  0           batch_normalization_167[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed7 (Concatenate)            (None, 30, 30, 768)  0           activation_155[0][0]             \n",
      "                                                                 activation_158[0][0]             \n",
      "                                                                 activation_163[0][0]             \n",
      "                                                                 activation_164[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_185 (Conv2D)             (None, 30, 30, 192)  147456      mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_170 (BatchN (None, 30, 30, 192)  576         conv2d_185[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_167 (Activation)     (None, 30, 30, 192)  0           batch_normalization_170[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_186 (Conv2D)             (None, 30, 30, 192)  258048      activation_167[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_171 (BatchN (None, 30, 30, 192)  576         conv2d_186[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_168 (Activation)     (None, 30, 30, 192)  0           batch_normalization_171[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_183 (Conv2D)             (None, 30, 30, 192)  147456      mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_187 (Conv2D)             (None, 30, 30, 192)  258048      activation_168[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_168 (BatchN (None, 30, 30, 192)  576         conv2d_183[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_172 (BatchN (None, 30, 30, 192)  576         conv2d_187[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_165 (Activation)     (None, 30, 30, 192)  0           batch_normalization_168[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_169 (Activation)     (None, 30, 30, 192)  0           batch_normalization_172[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_184 (Conv2D)             (None, 14, 14, 320)  552960      activation_165[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_188 (Conv2D)             (None, 14, 14, 192)  331776      activation_169[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_169 (BatchN (None, 14, 14, 320)  960         conv2d_184[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_173 (BatchN (None, 14, 14, 192)  576         conv2d_188[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_166 (Activation)     (None, 14, 14, 320)  0           batch_normalization_169[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_170 (Activation)     (None, 14, 14, 192)  0           batch_normalization_173[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_26 (MaxPooling2D) (None, 14, 14, 768)  0           mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "mixed8 (Concatenate)            (None, 14, 14, 1280) 0           activation_166[0][0]             \n",
      "                                                                 activation_170[0][0]             \n",
      "                                                                 max_pooling2d_26[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_193 (Conv2D)             (None, 14, 14, 448)  573440      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_178 (BatchN (None, 14, 14, 448)  1344        conv2d_193[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_175 (Activation)     (None, 14, 14, 448)  0           batch_normalization_178[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_190 (Conv2D)             (None, 14, 14, 384)  491520      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_194 (Conv2D)             (None, 14, 14, 384)  1548288     activation_175[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_175 (BatchN (None, 14, 14, 384)  1152        conv2d_190[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_179 (BatchN (None, 14, 14, 384)  1152        conv2d_194[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_172 (Activation)     (None, 14, 14, 384)  0           batch_normalization_175[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_176 (Activation)     (None, 14, 14, 384)  0           batch_normalization_179[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_191 (Conv2D)             (None, 14, 14, 384)  442368      activation_172[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_192 (Conv2D)             (None, 14, 14, 384)  442368      activation_172[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_195 (Conv2D)             (None, 14, 14, 384)  442368      activation_176[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_196 (Conv2D)             (None, 14, 14, 384)  442368      activation_176[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_17 (AveragePo (None, 14, 14, 1280) 0           mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_189 (Conv2D)             (None, 14, 14, 320)  409600      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_176 (BatchN (None, 14, 14, 384)  1152        conv2d_191[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_177 (BatchN (None, 14, 14, 384)  1152        conv2d_192[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_180 (BatchN (None, 14, 14, 384)  1152        conv2d_195[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_181 (BatchN (None, 14, 14, 384)  1152        conv2d_196[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_197 (Conv2D)             (None, 14, 14, 192)  245760      average_pooling2d_17[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_174 (BatchN (None, 14, 14, 320)  960         conv2d_189[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_173 (Activation)     (None, 14, 14, 384)  0           batch_normalization_176[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_174 (Activation)     (None, 14, 14, 384)  0           batch_normalization_177[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_177 (Activation)     (None, 14, 14, 384)  0           batch_normalization_180[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_178 (Activation)     (None, 14, 14, 384)  0           batch_normalization_181[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_182 (BatchN (None, 14, 14, 192)  576         conv2d_197[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_171 (Activation)     (None, 14, 14, 320)  0           batch_normalization_174[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed9_0 (Concatenate)          (None, 14, 14, 768)  0           activation_173[0][0]             \n",
      "                                                                 activation_174[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 14, 14, 768)  0           activation_177[0][0]             \n",
      "                                                                 activation_178[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_179 (Activation)     (None, 14, 14, 192)  0           batch_normalization_182[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed9 (Concatenate)            (None, 14, 14, 2048) 0           activation_171[0][0]             \n",
      "                                                                 mixed9_0[0][0]                   \n",
      "                                                                 concatenate_3[0][0]              \n",
      "                                                                 activation_179[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_202 (Conv2D)             (None, 14, 14, 448)  917504      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_187 (BatchN (None, 14, 14, 448)  1344        conv2d_202[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_184 (Activation)     (None, 14, 14, 448)  0           batch_normalization_187[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_199 (Conv2D)             (None, 14, 14, 384)  786432      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_203 (Conv2D)             (None, 14, 14, 384)  1548288     activation_184[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_184 (BatchN (None, 14, 14, 384)  1152        conv2d_199[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_188 (BatchN (None, 14, 14, 384)  1152        conv2d_203[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_181 (Activation)     (None, 14, 14, 384)  0           batch_normalization_184[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_185 (Activation)     (None, 14, 14, 384)  0           batch_normalization_188[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_200 (Conv2D)             (None, 14, 14, 384)  442368      activation_181[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_201 (Conv2D)             (None, 14, 14, 384)  442368      activation_181[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_204 (Conv2D)             (None, 14, 14, 384)  442368      activation_185[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_205 (Conv2D)             (None, 14, 14, 384)  442368      activation_185[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_18 (AveragePo (None, 14, 14, 2048) 0           mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_198 (Conv2D)             (None, 14, 14, 320)  655360      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_185 (BatchN (None, 14, 14, 384)  1152        conv2d_200[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_186 (BatchN (None, 14, 14, 384)  1152        conv2d_201[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_189 (BatchN (None, 14, 14, 384)  1152        conv2d_204[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_190 (BatchN (None, 14, 14, 384)  1152        conv2d_205[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_206 (Conv2D)             (None, 14, 14, 192)  393216      average_pooling2d_18[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_183 (BatchN (None, 14, 14, 320)  960         conv2d_198[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_182 (Activation)     (None, 14, 14, 384)  0           batch_normalization_185[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_183 (Activation)     (None, 14, 14, 384)  0           batch_normalization_186[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_186 (Activation)     (None, 14, 14, 384)  0           batch_normalization_189[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_187 (Activation)     (None, 14, 14, 384)  0           batch_normalization_190[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_191 (BatchN (None, 14, 14, 192)  576         conv2d_206[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_180 (Activation)     (None, 14, 14, 320)  0           batch_normalization_183[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed9_1 (Concatenate)          (None, 14, 14, 768)  0           activation_182[0][0]             \n",
      "                                                                 activation_183[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 14, 14, 768)  0           activation_186[0][0]             \n",
      "                                                                 activation_187[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_188 (Activation)     (None, 14, 14, 192)  0           batch_normalization_191[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed10 (Concatenate)           (None, 14, 14, 2048) 0           activation_180[0][0]             \n",
      "                                                                 mixed9_1[0][0]                   \n",
      "                                                                 concatenate_4[0][0]              \n",
      "                                                                 activation_188[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_2 (Glo (None, 2048)         0           mixed10[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 1024)         2098176     global_average_pooling2d_2[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 5)            5125        dense_9[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 23,906,085\n",
      "Trainable params: 23,871,653\n",
      "Non-trainable params: 34,432\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/10\n",
      "5/5 [==============================] - 46s 9s/step - loss: 2.9741 - acc: 0.3025 - val_loss: 10.8906 - val_acc: 0.3243\n",
      "Epoch 2/10\n",
      "5/5 [==============================] - 7s 1s/step - loss: 1.5930 - acc: 0.4757 - val_loss: 1.7283 - val_acc: 0.3784\n",
      "Epoch 3/10\n",
      "5/5 [==============================] - 7s 1s/step - loss: 1.8972 - acc: 0.4745 - val_loss: 9.7240 - val_acc: 0.3784\n",
      "Epoch 4/10\n",
      "5/5 [==============================] - 7s 1s/step - loss: 1.7684 - acc: 0.4759 - val_loss: 5.8026 - val_acc: 0.3784\n",
      "Epoch 5/10\n",
      "5/5 [==============================] - 7s 1s/step - loss: 1.7979 - acc: 0.4718 - val_loss: 5.7951 - val_acc: 0.3784\n",
      "Epoch 6/10\n",
      "5/5 [==============================] - 7s 1s/step - loss: 1.9354 - acc: 0.4315 - val_loss: 10.0194 - val_acc: 0.3784\n",
      "Epoch 7/10\n",
      "5/5 [==============================] - 8s 2s/step - loss: 1.9720 - acc: 0.4086 - val_loss: 10.0194 - val_acc: 0.3784\n",
      "Epoch 8/10\n",
      "5/5 [==============================] - 10s 2s/step - loss: 1.7072 - acc: 0.4731 - val_loss: 10.0194 - val_acc: 0.3784\n",
      "Epoch 9/10\n",
      "5/5 [==============================] - 10s 2s/step - loss: 1.5848 - acc: 0.4745 - val_loss: 10.0194 - val_acc: 0.3784\n",
      "Epoch 10/10\n",
      "5/5 [==============================] - 9s 2s/step - loss: 1.4761 - acc: 0.4731 - val_loss: 9.6291 - val_acc: 0.3784\n"
     ]
    }
   ],
   "source": [
    "model = create_inception_model()\n",
    "loss, accuracy = train_inception_gen_model(model, 30, 10, X_train, y_train, X_test, y_test, 90, 0.5, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "e9YEIQVQ0eSN",
    "outputId": "daf5c77b-b81d-4389-d56d-6cd9f3b7763a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 9.629059224515348\n",
      "Test accuracy: 0.37837838160025106\n"
     ]
    }
   ],
   "source": [
    "print('Test loss:', loss)\n",
    "print('Test accuracy:', accuracy) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lPp37Z786X9N"
   },
   "source": [
    "## K-Fold Crossvalidation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2624
    },
    "colab_type": "code",
    "id": "_r9YjPOn6nv3",
    "outputId": "f7ac75a3-2c19-4fd2-9cb4-0ba5a503c187"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_315 (Conv2D)          (None, 512, 512, 32)      832       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_135 (MaxPoolin (None, 256, 256, 32)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_210 (Bat (None, 256, 256, 32)      128       \n",
      "_________________________________________________________________\n",
      "conv2d_316 (Conv2D)          (None, 256, 256, 64)      51264     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_136 (MaxPoolin (None, 128, 128, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_317 (Conv2D)          (None, 128, 128, 64)      102464    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_137 (MaxPoolin (None, 64, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_318 (Conv2D)          (None, 64, 64, 64)        102464    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_138 (MaxPoolin (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_319 (Conv2D)          (None, 32, 32, 64)        102464    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_139 (MaxPoolin (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_320 (Conv2D)          (None, 16, 16, 64)        102464    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_140 (MaxPoolin (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_22 (Flatten)         (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             (None, 1024)              4195328   \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_48 (Dense)             (None, 5)                 5125      \n",
      "=================================================================\n",
      "Total params: 4,662,533\n",
      "Trainable params: 4,662,469\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "8/8 [==============================] - 16s 2s/step - loss: 1.3999 - acc: 0.4549 - val_loss: 1.3683 - val_acc: 0.4359\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 2s 252ms/step - loss: 1.3301 - acc: 0.5120 - val_loss: 1.3915 - val_acc: 0.4359\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - 2s 250ms/step - loss: 1.3047 - acc: 0.4620 - val_loss: 1.3742 - val_acc: 0.4359\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - 2s 251ms/step - loss: 1.2983 - acc: 0.4620 - val_loss: 1.3599 - val_acc: 0.4359\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - 3s 327ms/step - loss: 1.2961 - acc: 0.4620 - val_loss: 1.3646 - val_acc: 0.4359\n",
      "Epoch 6/10\n",
      "8/8 [==============================] - 3s 336ms/step - loss: 1.2907 - acc: 0.4620 - val_loss: 1.3719 - val_acc: 0.4359\n",
      "Epoch 7/10\n",
      "8/8 [==============================] - 3s 339ms/step - loss: 1.2593 - acc: 0.4620 - val_loss: 1.3729 - val_acc: 0.4359\n",
      "Epoch 8/10\n",
      "8/8 [==============================] - 3s 334ms/step - loss: 1.2715 - acc: 0.4620 - val_loss: 1.3738 - val_acc: 0.4359\n",
      "Epoch 9/10\n",
      "8/8 [==============================] - 3s 333ms/step - loss: 1.3069 - acc: 0.4120 - val_loss: 1.3658 - val_acc: 0.4359\n",
      "Epoch 10/10\n",
      "8/8 [==============================] - 3s 335ms/step - loss: 1.2883 - acc: 0.4430 - val_loss: 1.3666 - val_acc: 0.4359\n",
      "Fold # 1 Accuracy 0.4358974358974359\n",
      "Epoch 1/10\n",
      "8/8 [==============================] - 16s 2s/step - loss: 1.2984 - acc: 0.4746 - val_loss: 1.3251 - val_acc: 0.4474\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 2s 251ms/step - loss: 1.3453 - acc: 0.4431 - val_loss: 1.3241 - val_acc: 0.4474\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - 2s 251ms/step - loss: 1.3174 - acc: 0.4431 - val_loss: 1.3281 - val_acc: 0.4474\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - 2s 250ms/step - loss: 1.3001 - acc: 0.4116 - val_loss: 1.3279 - val_acc: 0.4474\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - 2s 279ms/step - loss: 1.3221 - acc: 0.4431 - val_loss: 1.3389 - val_acc: 0.4474\n",
      "Epoch 6/10\n",
      "8/8 [==============================] - 3s 332ms/step - loss: 1.3879 - acc: 0.4179 - val_loss: 1.3329 - val_acc: 0.4474\n",
      "Epoch 7/10\n",
      "8/8 [==============================] - 3s 335ms/step - loss: 1.3004 - acc: 0.4619 - val_loss: 1.3382 - val_acc: 0.4474\n",
      "Epoch 8/10\n",
      "8/8 [==============================] - 3s 340ms/step - loss: 1.2654 - acc: 0.4746 - val_loss: 1.3355 - val_acc: 0.4474\n",
      "Epoch 9/10\n",
      "8/8 [==============================] - 3s 338ms/step - loss: 1.3278 - acc: 0.4431 - val_loss: 1.3364 - val_acc: 0.4474\n",
      "Epoch 10/10\n",
      "8/8 [==============================] - 3s 334ms/step - loss: 1.3013 - acc: 0.4431 - val_loss: 1.3279 - val_acc: 0.4474\n",
      "Fold # 2 Accuracy 0.4473684210526316\n",
      "Epoch 1/10\n",
      "8/8 [==============================] - 17s 2s/step - loss: 1.3238 - acc: 0.4558 - val_loss: 1.2702 - val_acc: 0.4571\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 2s 250ms/step - loss: 1.3271 - acc: 0.4558 - val_loss: 1.2783 - val_acc: 0.4571\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - 2s 254ms/step - loss: 1.3071 - acc: 0.4365 - val_loss: 1.2670 - val_acc: 0.4571\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - 2s 274ms/step - loss: 1.3258 - acc: 0.4428 - val_loss: 1.2662 - val_acc: 0.4571\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - 3s 342ms/step - loss: 1.3227 - acc: 0.4428 - val_loss: 1.2731 - val_acc: 0.4571\n",
      "Epoch 6/10\n",
      "8/8 [==============================] - 3s 342ms/step - loss: 1.3085 - acc: 0.4558 - val_loss: 1.2676 - val_acc: 0.4571\n",
      "Epoch 7/10\n",
      "8/8 [==============================] - 3s 339ms/step - loss: 1.3298 - acc: 0.4428 - val_loss: 1.2659 - val_acc: 0.4571\n",
      "Epoch 8/10\n",
      "8/8 [==============================] - 3s 343ms/step - loss: 1.3415 - acc: 0.4428 - val_loss: 1.2764 - val_acc: 0.4571\n",
      "Epoch 9/10\n",
      "8/8 [==============================] - 3s 345ms/step - loss: 1.3086 - acc: 0.4558 - val_loss: 1.2727 - val_acc: 0.4571\n",
      "Epoch 10/10\n",
      "8/8 [==============================] - 3s 341ms/step - loss: 1.3537 - acc: 0.4299 - val_loss: 1.2655 - val_acc: 0.4571\n",
      "Fold # 3 Accuracy 0.45714285714285713\n",
      "Epoch 1/10\n",
      "8/8 [==============================] - 16s 2s/step - loss: 1.3295 - acc: 0.4558 - val_loss: 1.2695 - val_acc: 0.4571\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 2s 249ms/step - loss: 1.3218 - acc: 0.4687 - val_loss: 1.2660 - val_acc: 0.4571\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - 2s 251ms/step - loss: 1.3451 - acc: 0.4428 - val_loss: 1.2792 - val_acc: 0.4571\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - 2s 255ms/step - loss: 1.3268 - acc: 0.4428 - val_loss: 1.2761 - val_acc: 0.4571\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - 2s 279ms/step - loss: 1.3208 - acc: 0.4558 - val_loss: 1.2718 - val_acc: 0.4571\n",
      "Epoch 6/10\n",
      "8/8 [==============================] - 3s 342ms/step - loss: 1.2944 - acc: 0.4558 - val_loss: 1.2696 - val_acc: 0.4571\n",
      "Epoch 7/10\n",
      "8/8 [==============================] - 3s 345ms/step - loss: 1.3652 - acc: 0.4428 - val_loss: 1.2700 - val_acc: 0.4571\n",
      "Epoch 8/10\n",
      "8/8 [==============================] - 3s 340ms/step - loss: 1.3662 - acc: 0.4299 - val_loss: 1.2747 - val_acc: 0.4571\n",
      "Epoch 9/10\n",
      "8/8 [==============================] - 3s 340ms/step - loss: 1.2957 - acc: 0.4817 - val_loss: 1.2816 - val_acc: 0.4571\n",
      "Epoch 10/10\n",
      "8/8 [==============================] - 3s 343ms/step - loss: 1.3783 - acc: 0.4299 - val_loss: 1.2690 - val_acc: 0.4571\n",
      "Fold # 4 Accuracy 0.45714285714285713\n",
      "Epoch 1/10\n",
      "8/8 [==============================] - 18s 2s/step - loss: 1.3519 - acc: 0.4372 - val_loss: 1.2778 - val_acc: 0.4706\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 2s 252ms/step - loss: 1.3062 - acc: 0.4578 - val_loss: 1.2731 - val_acc: 0.4706\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - 2s 254ms/step - loss: 1.3341 - acc: 0.4372 - val_loss: 1.2697 - val_acc: 0.4706\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - 2s 262ms/step - loss: 1.3585 - acc: 0.4166 - val_loss: 1.2698 - val_acc: 0.4706\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - 3s 342ms/step - loss: 1.3228 - acc: 0.4475 - val_loss: 1.2816 - val_acc: 0.4706\n",
      "Epoch 6/10\n",
      "8/8 [==============================] - 3s 341ms/step - loss: 1.3493 - acc: 0.4372 - val_loss: 1.2814 - val_acc: 0.4706\n",
      "Epoch 7/10\n",
      "8/8 [==============================] - 3s 342ms/step - loss: 1.3475 - acc: 0.4372 - val_loss: 1.2728 - val_acc: 0.4706\n",
      "Epoch 8/10\n",
      "8/8 [==============================] - 3s 346ms/step - loss: 1.3508 - acc: 0.4372 - val_loss: 1.2709 - val_acc: 0.4706\n",
      "Epoch 9/10\n",
      "8/8 [==============================] - 3s 344ms/step - loss: 1.2997 - acc: 0.4578 - val_loss: 1.2748 - val_acc: 0.4706\n",
      "Epoch 10/10\n",
      "8/8 [==============================] - 3s 343ms/step - loss: 1.3030 - acc: 0.4372 - val_loss: 1.2672 - val_acc: 0.4706\n",
      "Fold # 5 Accuracy 0.47058823529411764\n",
      "0.4536279613059799\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "cv = StratifiedKFold(5)\n",
    "\n",
    "model = create_simple_model()\n",
    "\n",
    "x = clean_data.drop('y', axis=1)\n",
    "y = [str(val) for val in clean_data['y'].values]\n",
    "x = x.values\n",
    "x = np.array([arr[0] for arr in x])\n",
    "acc = 0\n",
    "j = 1\n",
    "for train, test in cv.split(x, y):\n",
    "    x_train = x[train]\n",
    "    y_train = [y[i] for i in train]\n",
    "    x_test = x[test]\n",
    "    y_test = [y[i] for i in test]\n",
    "    \n",
    "    y_train = pd.get_dummies(y_train)\n",
    "    y_test = pd.get_dummies(y_test)\n",
    "    \n",
    "    loss, accuracy = train_simple_gen_model(model, 20, 10, x_train, y_train, x_test, y_test)\n",
    "    print ('Fold #', j, 'Accuracy', accuracy)\n",
    "    acc+=accuracy\n",
    "    j+=1\n",
    "print(acc/(j-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_RH9NW8DIqoU"
   },
   "source": [
    "Mean accuracy of this model is 45.36%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2765
    },
    "colab_type": "code",
    "id": "iqY0a32HpO4_",
    "outputId": "17ce05dd-6d02-442c-a765-00b92b3ad5bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_13 (Conv2D)           (None, 512, 512, 32)      832       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling (None, 256, 256, 32)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 256, 256, 32)      128       \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 256, 256, 64)      51264     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling (None, 128, 128, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 128, 128, 64)      102464    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling (None, 64, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 64, 64, 64)        102464    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_16 (MaxPooling (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 32, 32, 64)        102464    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_17 (MaxPooling (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 16, 16, 64)        102464    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_18 (MaxPooling (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1024)              4195328   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 5)                 5125      \n",
      "=================================================================\n",
      "Total params: 4,662,533\n",
      "Trainable params: 4,662,469\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 142 samples, validate on 39 samples\n",
      "Epoch 1/10\n",
      "142/142 [==============================] - 11s 78ms/step - loss: 1.5519 - acc: 0.2817 - val_loss: 1.4227 - val_acc: 0.2821\n",
      "Epoch 2/10\n",
      "142/142 [==============================] - 2s 14ms/step - loss: 1.3186 - acc: 0.4155 - val_loss: 1.6049 - val_acc: 0.4359\n",
      "Epoch 3/10\n",
      "142/142 [==============================] - 2s 14ms/step - loss: 1.3086 - acc: 0.4577 - val_loss: 1.3602 - val_acc: 0.4359\n",
      "Epoch 4/10\n",
      "142/142 [==============================] - 2s 14ms/step - loss: 1.2894 - acc: 0.4577 - val_loss: 1.3993 - val_acc: 0.4359\n",
      "Epoch 5/10\n",
      "142/142 [==============================] - 2s 14ms/step - loss: 1.3142 - acc: 0.4577 - val_loss: 1.3709 - val_acc: 0.4359\n",
      "Epoch 6/10\n",
      "142/142 [==============================] - 2s 14ms/step - loss: 1.2955 - acc: 0.4577 - val_loss: 1.3638 - val_acc: 0.4359\n",
      "Epoch 7/10\n",
      "142/142 [==============================] - 2s 14ms/step - loss: 1.3372 - acc: 0.4577 - val_loss: 1.3774 - val_acc: 0.4359\n",
      "Epoch 8/10\n",
      "142/142 [==============================] - 2s 15ms/step - loss: 1.3345 - acc: 0.4577 - val_loss: 1.3717 - val_acc: 0.4359\n",
      "Epoch 9/10\n",
      "142/142 [==============================] - 2s 14ms/step - loss: 1.2912 - acc: 0.4577 - val_loss: 1.3681 - val_acc: 0.4359\n",
      "Epoch 10/10\n",
      "142/142 [==============================] - 2s 15ms/step - loss: 1.3331 - acc: 0.4577 - val_loss: 1.3722 - val_acc: 0.4359\n",
      "Fold # 1 Accuracy 0.4358974358974359\n",
      "Train on 143 samples, validate on 38 samples\n",
      "Epoch 1/10\n",
      "143/143 [==============================] - 5s 35ms/step - loss: 1.3251 - acc: 0.4545 - val_loss: 1.3253 - val_acc: 0.4474\n",
      "Epoch 2/10\n",
      "143/143 [==============================] - 2s 15ms/step - loss: 1.3153 - acc: 0.4545 - val_loss: 1.3231 - val_acc: 0.4474\n",
      "Epoch 3/10\n",
      "143/143 [==============================] - 2s 15ms/step - loss: 1.3160 - acc: 0.4545 - val_loss: 1.3256 - val_acc: 0.4474\n",
      "Epoch 4/10\n",
      "143/143 [==============================] - 2s 15ms/step - loss: 1.3079 - acc: 0.4545 - val_loss: 1.3292 - val_acc: 0.4474\n",
      "Epoch 5/10\n",
      "143/143 [==============================] - 2s 15ms/step - loss: 1.3057 - acc: 0.4545 - val_loss: 1.3304 - val_acc: 0.4474\n",
      "Epoch 6/10\n",
      "143/143 [==============================] - 2s 15ms/step - loss: 1.2957 - acc: 0.4545 - val_loss: 1.3336 - val_acc: 0.4474\n",
      "Epoch 7/10\n",
      "143/143 [==============================] - 2s 15ms/step - loss: 1.2993 - acc: 0.4545 - val_loss: 1.3267 - val_acc: 0.4474\n",
      "Epoch 8/10\n",
      "143/143 [==============================] - 2s 15ms/step - loss: 1.2984 - acc: 0.4545 - val_loss: 1.3257 - val_acc: 0.4474\n",
      "Epoch 9/10\n",
      "143/143 [==============================] - 2s 15ms/step - loss: 1.3157 - acc: 0.4545 - val_loss: 1.3299 - val_acc: 0.4474\n",
      "Epoch 10/10\n",
      "143/143 [==============================] - 2s 15ms/step - loss: 1.3009 - acc: 0.4545 - val_loss: 1.3307 - val_acc: 0.4474\n",
      "Fold # 2 Accuracy 0.4473684210526316\n",
      "Train on 146 samples, validate on 35 samples\n",
      "Epoch 1/10\n",
      "146/146 [==============================] - 5s 33ms/step - loss: 1.3316 - acc: 0.4521 - val_loss: 1.2687 - val_acc: 0.4571\n",
      "Epoch 2/10\n",
      "146/146 [==============================] - 2s 14ms/step - loss: 1.3202 - acc: 0.4521 - val_loss: 1.2967 - val_acc: 0.4571\n",
      "Epoch 3/10\n",
      "146/146 [==============================] - 2s 15ms/step - loss: 1.3219 - acc: 0.4521 - val_loss: 1.2691 - val_acc: 0.4571\n",
      "Epoch 4/10\n",
      "146/146 [==============================] - 2s 15ms/step - loss: 1.3274 - acc: 0.4521 - val_loss: 1.2682 - val_acc: 0.4571\n",
      "Epoch 5/10\n",
      "146/146 [==============================] - 2s 15ms/step - loss: 1.3068 - acc: 0.4521 - val_loss: 1.2725 - val_acc: 0.4571\n",
      "Epoch 6/10\n",
      "146/146 [==============================] - 2s 15ms/step - loss: 1.3141 - acc: 0.4521 - val_loss: 1.2710 - val_acc: 0.4571\n",
      "Epoch 7/10\n",
      "146/146 [==============================] - 2s 15ms/step - loss: 1.3314 - acc: 0.4521 - val_loss: 1.2703 - val_acc: 0.4571\n",
      "Epoch 8/10\n",
      "146/146 [==============================] - 2s 15ms/step - loss: 1.3163 - acc: 0.4521 - val_loss: 1.2765 - val_acc: 0.4571\n",
      "Epoch 9/10\n",
      "146/146 [==============================] - 2s 15ms/step - loss: 1.3151 - acc: 0.4521 - val_loss: 1.2821 - val_acc: 0.4571\n",
      "Epoch 10/10\n",
      "146/146 [==============================] - 2s 15ms/step - loss: 1.3228 - acc: 0.4521 - val_loss: 1.2663 - val_acc: 0.4571\n",
      "Fold # 3 Accuracy 0.45714285714285713\n",
      "Train on 146 samples, validate on 35 samples\n",
      "Epoch 1/10\n",
      "146/146 [==============================] - 3s 22ms/step - loss: 1.3319 - acc: 0.4521 - val_loss: 1.2685 - val_acc: 0.4571\n",
      "Epoch 2/10\n",
      "146/146 [==============================] - 2s 15ms/step - loss: 1.3257 - acc: 0.4521 - val_loss: 1.2664 - val_acc: 0.4571\n",
      "Epoch 3/10\n",
      "146/146 [==============================] - 2s 15ms/step - loss: 1.3143 - acc: 0.4521 - val_loss: 1.2672 - val_acc: 0.4571\n",
      "Epoch 4/10\n",
      "146/146 [==============================] - 2s 15ms/step - loss: 1.3257 - acc: 0.4521 - val_loss: 1.2693 - val_acc: 0.4571\n",
      "Epoch 5/10\n",
      "146/146 [==============================] - 2s 15ms/step - loss: 1.3143 - acc: 0.4521 - val_loss: 1.2778 - val_acc: 0.4571\n",
      "Epoch 6/10\n",
      "146/146 [==============================] - 2s 15ms/step - loss: 1.3067 - acc: 0.4521 - val_loss: 1.2673 - val_acc: 0.4571\n",
      "Epoch 7/10\n",
      "146/146 [==============================] - 2s 15ms/step - loss: 1.3138 - acc: 0.4521 - val_loss: 1.2676 - val_acc: 0.4571\n",
      "Epoch 8/10\n",
      "146/146 [==============================] - 2s 15ms/step - loss: 1.3226 - acc: 0.4521 - val_loss: 1.2672 - val_acc: 0.4571\n",
      "Epoch 9/10\n",
      "146/146 [==============================] - 2s 15ms/step - loss: 1.3039 - acc: 0.4521 - val_loss: 1.2702 - val_acc: 0.4571\n",
      "Epoch 10/10\n",
      "146/146 [==============================] - 2s 15ms/step - loss: 1.3060 - acc: 0.4521 - val_loss: 1.2713 - val_acc: 0.4571\n",
      "Fold # 4 Accuracy 0.45714285714285713\n",
      "Train on 147 samples, validate on 34 samples\n",
      "Epoch 1/10\n",
      "147/147 [==============================] - 5s 34ms/step - loss: 1.3466 - acc: 0.4490 - val_loss: 1.2713 - val_acc: 0.4706\n",
      "Epoch 2/10\n",
      "147/147 [==============================] - 2s 15ms/step - loss: 1.3295 - acc: 0.4490 - val_loss: 1.3046 - val_acc: 0.4706\n",
      "Epoch 3/10\n",
      "147/147 [==============================] - 2s 15ms/step - loss: 1.3290 - acc: 0.4490 - val_loss: 1.2679 - val_acc: 0.4706\n",
      "Epoch 4/10\n",
      "147/147 [==============================] - 2s 15ms/step - loss: 1.3159 - acc: 0.4490 - val_loss: 1.2675 - val_acc: 0.4706\n",
      "Epoch 5/10\n",
      "147/147 [==============================] - 2s 15ms/step - loss: 1.3086 - acc: 0.4490 - val_loss: 1.2733 - val_acc: 0.4706\n",
      "Epoch 6/10\n",
      "147/147 [==============================] - 2s 15ms/step - loss: 1.3139 - acc: 0.4490 - val_loss: 1.2795 - val_acc: 0.4706\n",
      "Epoch 7/10\n",
      "147/147 [==============================] - 2s 15ms/step - loss: 1.3147 - acc: 0.4490 - val_loss: 1.2689 - val_acc: 0.4706\n",
      "Epoch 8/10\n",
      "147/147 [==============================] - 2s 15ms/step - loss: 1.3106 - acc: 0.4490 - val_loss: 1.2712 - val_acc: 0.4706\n",
      "Epoch 9/10\n",
      "147/147 [==============================] - 2s 15ms/step - loss: 1.3089 - acc: 0.4490 - val_loss: 1.2757 - val_acc: 0.4706\n",
      "Epoch 10/10\n",
      "147/147 [==============================] - 2s 15ms/step - loss: 1.3138 - acc: 0.4490 - val_loss: 1.2751 - val_acc: 0.4706\n",
      "Fold # 5 Accuracy 0.47058823529411764\n",
      "0.4536279613059799\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "cv = StratifiedKFold(5)\n",
    "\n",
    "model = create_simple_model()\n",
    "\n",
    "x = clean_data.drop('y', axis=1)\n",
    "y = [str(val) for val in clean_data['y'].values]\n",
    "x = x.values\n",
    "x = np.array([arr[0] for arr in x])\n",
    "acc = 0\n",
    "j = 1\n",
    "for train, test in cv.split(x, y):\n",
    "    x_train = x[train]\n",
    "    y_train = [y[i] for i in train]\n",
    "    x_test = x[test]\n",
    "    y_test = [y[i] for i in test]\n",
    "    \n",
    "    y_train = pd.get_dummies(y_train)\n",
    "    y_test = pd.get_dummies(y_test)\n",
    "    \n",
    "    loss, accuracy = train_simple_model(model, 20, 10, x_train, y_train, x_test, y_test)\n",
    "    print ('Fold #', j, 'Accuracy', accuracy)\n",
    "    acc += accuracy\n",
    "    j += 1\n",
    "    \n",
    "print(acc/(j-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2624
    },
    "colab_type": "code",
    "id": "ieL5rU5m-uef",
    "outputId": "bc99199a-744b-49be-c569-2cd4d4f60385"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_321 (Conv2D)          (None, 512, 512, 32)      832       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_141 (MaxPoolin (None, 256, 256, 32)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_211 (Bat (None, 256, 256, 32)      128       \n",
      "_________________________________________________________________\n",
      "conv2d_322 (Conv2D)          (None, 256, 256, 64)      51264     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_142 (MaxPoolin (None, 128, 128, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_323 (Conv2D)          (None, 128, 128, 64)      102464    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_143 (MaxPoolin (None, 64, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_324 (Conv2D)          (None, 64, 64, 64)        102464    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_144 (MaxPoolin (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_325 (Conv2D)          (None, 32, 32, 64)        102464    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_145 (MaxPoolin (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_326 (Conv2D)          (None, 16, 16, 64)        102464    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_146 (MaxPoolin (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_23 (Flatten)         (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_49 (Dense)             (None, 1024)              4195328   \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_50 (Dense)             (None, 5)                 5125      \n",
      "=================================================================\n",
      "Total params: 4,662,533\n",
      "Trainable params: 4,662,469\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "4/4 [==============================] - 18s 5s/step - loss: 1.6078 - acc: 0.3894 - val_loss: 1.5232 - val_acc: 0.2821\n",
      "Epoch 2/10\n",
      "4/4 [==============================] - 2s 444ms/step - loss: 1.5303 - acc: 0.3003 - val_loss: 1.4937 - val_acc: 0.2821\n",
      "Epoch 3/10\n",
      "4/4 [==============================] - 2s 447ms/step - loss: 1.4245 - acc: 0.3003 - val_loss: 1.4544 - val_acc: 0.2821\n",
      "Epoch 4/10\n",
      "4/4 [==============================] - 2s 443ms/step - loss: 1.3019 - acc: 0.4062 - val_loss: 1.3998 - val_acc: 0.4359\n",
      "Epoch 5/10\n",
      "4/4 [==============================] - 2s 446ms/step - loss: 1.3259 - acc: 0.4534 - val_loss: 1.3610 - val_acc: 0.4359\n",
      "Epoch 6/10\n",
      "4/4 [==============================] - 2s 447ms/step - loss: 1.2911 - acc: 0.4565 - val_loss: 1.3585 - val_acc: 0.4359\n",
      "Epoch 7/10\n",
      "4/4 [==============================] - 2s 448ms/step - loss: 1.2733 - acc: 0.4607 - val_loss: 1.3708 - val_acc: 0.4359\n",
      "Epoch 8/10\n",
      "4/4 [==============================] - 3s 665ms/step - loss: 1.3110 - acc: 0.4544 - val_loss: 1.3886 - val_acc: 0.4359\n",
      "Epoch 9/10\n",
      "4/4 [==============================] - 3s 673ms/step - loss: 1.3070 - acc: 0.4586 - val_loss: 1.3630 - val_acc: 0.4359\n",
      "Epoch 10/10\n",
      "4/4 [==============================] - 3s 675ms/step - loss: 1.2842 - acc: 0.4575 - val_loss: 1.3611 - val_acc: 0.4359\n",
      "Fold # 1 Accuracy 0.4358974358974359\n",
      "Epoch 1/10\n",
      "4/4 [==============================] - 18s 5s/step - loss: 1.3264 - acc: 0.4549 - val_loss: 1.3488 - val_acc: 0.4474\n",
      "Epoch 2/10\n",
      "4/4 [==============================] - 2s 445ms/step - loss: 1.3185 - acc: 0.4549 - val_loss: 1.3368 - val_acc: 0.4474\n",
      "Epoch 3/10\n",
      "4/4 [==============================] - 2s 449ms/step - loss: 1.2900 - acc: 0.4575 - val_loss: 1.3531 - val_acc: 0.4474\n",
      "Epoch 4/10\n",
      "4/4 [==============================] - 2s 449ms/step - loss: 1.2910 - acc: 0.4558 - val_loss: 1.3479 - val_acc: 0.4474\n",
      "Epoch 5/10\n",
      "4/4 [==============================] - 2s 449ms/step - loss: 1.2921 - acc: 0.4558 - val_loss: 1.3698 - val_acc: 0.4474\n",
      "Epoch 6/10\n",
      "4/4 [==============================] - 2s 447ms/step - loss: 1.2944 - acc: 0.4566 - val_loss: 1.3273 - val_acc: 0.4474\n",
      "Epoch 7/10\n",
      "4/4 [==============================] - 2s 575ms/step - loss: 1.2951 - acc: 0.4524 - val_loss: 1.3411 - val_acc: 0.4474\n",
      "Epoch 8/10\n",
      "4/4 [==============================] - 3s 668ms/step - loss: 1.2735 - acc: 0.4685 - val_loss: 1.3238 - val_acc: 0.4474\n",
      "Epoch 9/10\n",
      "4/4 [==============================] - 3s 670ms/step - loss: 1.2842 - acc: 0.4524 - val_loss: 1.3322 - val_acc: 0.4474\n",
      "Epoch 10/10\n",
      "4/4 [==============================] - 3s 667ms/step - loss: 1.2746 - acc: 0.4617 - val_loss: 1.3275 - val_acc: 0.4474\n",
      "Fold # 2 Accuracy 0.4473684210526316\n",
      "Epoch 1/10\n",
      "4/4 [==============================] - 18s 5s/step - loss: 1.3294 - acc: 0.4521 - val_loss: 1.2666 - val_acc: 0.4571\n",
      "Epoch 2/10\n",
      "4/4 [==============================] - 2s 456ms/step - loss: 1.3322 - acc: 0.4515 - val_loss: 1.2683 - val_acc: 0.4571\n",
      "Epoch 3/10\n",
      "4/4 [==============================] - 2s 455ms/step - loss: 1.3071 - acc: 0.4512 - val_loss: 1.2801 - val_acc: 0.4571\n",
      "Epoch 4/10\n",
      "4/4 [==============================] - 2s 456ms/step - loss: 1.3243 - acc: 0.4518 - val_loss: 1.2879 - val_acc: 0.4571\n",
      "Epoch 5/10\n",
      "4/4 [==============================] - 2s 456ms/step - loss: 1.3291 - acc: 0.4512 - val_loss: 1.2732 - val_acc: 0.4571\n",
      "Epoch 6/10\n",
      "4/4 [==============================] - 2s 455ms/step - loss: 1.3145 - acc: 0.4509 - val_loss: 1.2677 - val_acc: 0.4571\n",
      "Epoch 7/10\n",
      "4/4 [==============================] - 2s 473ms/step - loss: 1.3077 - acc: 0.4518 - val_loss: 1.2662 - val_acc: 0.4571\n",
      "Epoch 8/10\n",
      "4/4 [==============================] - 3s 676ms/step - loss: 1.3056 - acc: 0.4518 - val_loss: 1.2662 - val_acc: 0.4571\n",
      "Epoch 9/10\n",
      "4/4 [==============================] - 3s 681ms/step - loss: 1.3093 - acc: 0.4512 - val_loss: 1.2688 - val_acc: 0.4571\n",
      "Epoch 10/10\n",
      "4/4 [==============================] - 3s 681ms/step - loss: 1.3155 - acc: 0.4518 - val_loss: 1.2701 - val_acc: 0.4571\n",
      "Fold # 3 Accuracy 0.45714285714285713\n",
      "Epoch 1/10\n",
      "4/4 [==============================] - 19s 5s/step - loss: 1.3340 - acc: 0.4518 - val_loss: 1.2697 - val_acc: 0.4571\n",
      "Epoch 2/10\n",
      "4/4 [==============================] - 2s 456ms/step - loss: 1.3165 - acc: 0.4512 - val_loss: 1.2696 - val_acc: 0.4571\n",
      "Epoch 3/10\n",
      "4/4 [==============================] - 2s 456ms/step - loss: 1.3134 - acc: 0.4524 - val_loss: 1.2672 - val_acc: 0.4571\n",
      "Epoch 4/10\n",
      "4/4 [==============================] - 2s 458ms/step - loss: 1.3280 - acc: 0.4506 - val_loss: 1.2658 - val_acc: 0.4571\n",
      "Epoch 5/10\n",
      "4/4 [==============================] - 2s 459ms/step - loss: 1.3142 - acc: 0.4530 - val_loss: 1.2822 - val_acc: 0.4571\n",
      "Epoch 6/10\n",
      "4/4 [==============================] - 2s 457ms/step - loss: 1.3228 - acc: 0.4524 - val_loss: 1.2883 - val_acc: 0.4571\n",
      "Epoch 7/10\n",
      "4/4 [==============================] - 3s 660ms/step - loss: 1.3226 - acc: 0.4512 - val_loss: 1.2668 - val_acc: 0.4571\n",
      "Epoch 8/10\n",
      "4/4 [==============================] - 3s 688ms/step - loss: 1.3098 - acc: 0.4521 - val_loss: 1.2723 - val_acc: 0.4571\n",
      "Epoch 9/10\n",
      "4/4 [==============================] - 3s 690ms/step - loss: 1.3133 - acc: 0.4518 - val_loss: 1.2814 - val_acc: 0.4571\n",
      "Epoch 10/10\n",
      "4/4 [==============================] - 3s 688ms/step - loss: 1.2939 - acc: 0.4524 - val_loss: 1.2936 - val_acc: 0.4571\n",
      "Fold # 4 Accuracy 0.45714285714285713\n",
      "Epoch 1/10\n",
      "4/4 [==============================] - 19s 5s/step - loss: 1.3362 - acc: 0.4496 - val_loss: 1.2721 - val_acc: 0.4706\n",
      "Epoch 2/10\n",
      "4/4 [==============================] - 2s 450ms/step - loss: 1.3041 - acc: 0.4494 - val_loss: 1.2667 - val_acc: 0.4706\n",
      "Epoch 3/10\n",
      "4/4 [==============================] - 2s 453ms/step - loss: 1.2999 - acc: 0.4485 - val_loss: 1.2593 - val_acc: 0.4706\n",
      "Epoch 4/10\n",
      "4/4 [==============================] - 2s 453ms/step - loss: 1.3019 - acc: 0.4493 - val_loss: 1.2673 - val_acc: 0.4706\n",
      "Epoch 5/10\n",
      "4/4 [==============================] - 2s 455ms/step - loss: 1.3033 - acc: 0.4491 - val_loss: 1.2688 - val_acc: 0.4706\n",
      "Epoch 6/10\n",
      "4/4 [==============================] - 2s 455ms/step - loss: 1.3055 - acc: 0.4496 - val_loss: 1.2812 - val_acc: 0.4706\n",
      "Epoch 7/10\n",
      "4/4 [==============================] - 2s 503ms/step - loss: 1.2901 - acc: 0.4490 - val_loss: 1.2844 - val_acc: 0.4706\n",
      "Epoch 8/10\n",
      "4/4 [==============================] - 3s 687ms/step - loss: 1.2787 - acc: 0.4423 - val_loss: 1.2869 - val_acc: 0.4706\n",
      "Epoch 9/10\n",
      "4/4 [==============================] - 3s 683ms/step - loss: 1.3365 - acc: 0.4285 - val_loss: 1.2876 - val_acc: 0.4706\n",
      "Epoch 10/10\n",
      "4/4 [==============================] - 3s 687ms/step - loss: 1.2785 - acc: 0.4562 - val_loss: 1.2960 - val_acc: 0.4706\n",
      "Fold # 5 Accuracy 0.47058823529411764\n",
      "0.4536279613059799\n"
     ]
    }
   ],
   "source": [
    "model = create_simple_model()\n",
    "acc = 0\n",
    "j = 1\n",
    "for train, test in cv.split(x, y):\n",
    "    x_train = x[train]\n",
    "    y_train = [y[i] for i in train]\n",
    "    x_test = x[test]\n",
    "    y_test = [y[i] for i in test]\n",
    "    \n",
    "    y_train = pd.get_dummies(y_train)\n",
    "    y_test = pd.get_dummies(y_test)\n",
    "    \n",
    "    loss, accuracy = train_simple_gen_model(model, 37, 10, x_train, y_train, x_test, y_test, 90, 0.5, 0.5)\n",
    "    print ('Fold #', j, 'Accuracy', accuracy)\n",
    "    acc += accuracy\n",
    "    j += 1\n",
    "    \n",
    "print(acc/(j-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bGr3BvPXIven"
   },
   "source": [
    "Mean accuracy of 45.36%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-5YLKU9i-HaY"
   },
   "source": [
    "## Confusion Matrix of Best Models (one simple, one inception)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LoBJlMz41Vpk"
   },
   "outputs": [],
   "source": [
    "# from the sklearn library example\n",
    "# https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax\n",
    "\n",
    "\n",
    "np.set_printoptions(precision=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vxxdrrvl-oUQ"
   },
   "outputs": [],
   "source": [
    "img_rows,img_cols=512,512\n",
    "def train_simple_gen_model (model, batch_size, epochs, x, y, xt, yt, rot=20, width=0.2, height=0.2):\n",
    "  x = x.reshape(x.shape[0], img_rows, img_cols, 1)\n",
    "  xt = xt.reshape(xt.shape[0], img_rows, img_cols, 1)\n",
    "  train_generator, validation_generator = create_generators(x, y, xt, yt, batch_size, rot, width, height)\n",
    "  \n",
    "  model.compile(\n",
    "    loss=categorical_crossentropy,\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "  model.fit_generator(\n",
    "        train_generator,\n",
    "        epochs=10,\n",
    "        verbose=1,\n",
    "        steps_per_epoch = np.ceil(x.shape[0]/batch_size),\n",
    "        validation_steps = np.ceil(xt.shape[0]/batch_size),\n",
    "        validation_data=validation_generator)\n",
    "  score = model.evaluate(xt, yt, verbose=0)\n",
    "  return score, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2765
    },
    "colab_type": "code",
    "id": "8pCqzcmOr1x6",
    "outputId": "10392eff-b6b6-48b9-abc2-07bf7db04524"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 512, 512, 32)      832       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 256, 256, 32)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 256, 256, 32)      128       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 256, 256, 64)      51264     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 128, 128, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 128, 128, 64)      102464    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 64, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 64, 64, 64)        102464    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 32, 32, 64)        102464    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 16, 16, 64)        102464    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              4195328   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 5125      \n",
      "=================================================================\n",
      "Total params: 4,662,533\n",
      "Trainable params: 4,662,469\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/10\n",
      "8/8 [==============================] - 10s 1s/step - loss: 1.5383 - acc: 0.3486 - val_loss: 1.4370 - val_acc: 0.2821\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 2s 239ms/step - loss: 1.4703 - acc: 0.3986 - val_loss: 1.4656 - val_acc: 0.4359\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - 2s 239ms/step - loss: 1.4341 - acc: 0.4620 - val_loss: 1.4101 - val_acc: 0.4359\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - 2s 263ms/step - loss: 1.4210 - acc: 0.4056 - val_loss: 1.4049 - val_acc: 0.4359\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - 3s 371ms/step - loss: 1.3386 - acc: 0.4430 - val_loss: 1.3690 - val_acc: 0.4359\n",
      "Epoch 6/10\n",
      "8/8 [==============================] - 3s 360ms/step - loss: 1.3346 - acc: 0.4683 - val_loss: 1.3604 - val_acc: 0.4359\n",
      "Epoch 7/10\n",
      "8/8 [==============================] - 3s 331ms/step - loss: 1.2526 - acc: 0.5120 - val_loss: 1.3655 - val_acc: 0.4359\n",
      "Epoch 8/10\n",
      "8/8 [==============================] - 3s 330ms/step - loss: 1.2461 - acc: 0.5120 - val_loss: 1.3958 - val_acc: 0.4359\n",
      "Epoch 9/10\n",
      "8/8 [==============================] - 3s 329ms/step - loss: 1.2827 - acc: 0.4620 - val_loss: 1.3653 - val_acc: 0.4359\n",
      "Epoch 10/10\n",
      "8/8 [==============================] - 3s 356ms/step - loss: 1.2908 - acc: 0.4493 - val_loss: 1.3670 - val_acc: 0.4359\n",
      "Fold # 1 Accuracy 0.4358974358974359\n",
      "Epoch 1/10\n",
      "8/8 [==============================] - 6s 692ms/step - loss: 1.3225 - acc: 0.4431 - val_loss: 1.3259 - val_acc: 0.4474\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 2s 242ms/step - loss: 1.2824 - acc: 0.4619 - val_loss: 1.3339 - val_acc: 0.4474\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - 2s 248ms/step - loss: 1.3337 - acc: 0.4431 - val_loss: 1.3338 - val_acc: 0.4474\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - 3s 336ms/step - loss: 1.2768 - acc: 0.4746 - val_loss: 1.3280 - val_acc: 0.4474\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - 3s 334ms/step - loss: 1.2624 - acc: 0.5061 - val_loss: 1.3252 - val_acc: 0.4474\n",
      "Epoch 6/10\n",
      "8/8 [==============================] - 3s 332ms/step - loss: 1.2899 - acc: 0.4746 - val_loss: 1.3392 - val_acc: 0.4474\n",
      "Epoch 7/10\n",
      "8/8 [==============================] - 3s 337ms/step - loss: 1.2407 - acc: 0.5061 - val_loss: 1.3282 - val_acc: 0.4474\n",
      "Epoch 8/10\n",
      "8/8 [==============================] - 3s 337ms/step - loss: 1.3009 - acc: 0.4431 - val_loss: 1.3257 - val_acc: 0.4474\n",
      "Epoch 9/10\n",
      "8/8 [==============================] - 3s 334ms/step - loss: 1.3318 - acc: 0.4431 - val_loss: 1.3246 - val_acc: 0.4474\n",
      "Epoch 10/10\n",
      "8/8 [==============================] - 3s 332ms/step - loss: 1.3660 - acc: 0.4116 - val_loss: 1.3272 - val_acc: 0.4474\n",
      "Fold # 2 Accuracy 0.4473684210526316\n",
      "Epoch 1/10\n",
      "8/8 [==============================] - 5s 653ms/step - loss: 1.3304 - acc: 0.4239 - val_loss: 1.2701 - val_acc: 0.4571\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 2s 250ms/step - loss: 1.3203 - acc: 0.4428 - val_loss: 1.2686 - val_acc: 0.4571\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - 2s 287ms/step - loss: 1.3096 - acc: 0.4558 - val_loss: 1.2702 - val_acc: 0.4571\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - 3s 338ms/step - loss: 1.3008 - acc: 0.4558 - val_loss: 1.2694 - val_acc: 0.4571\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - 3s 345ms/step - loss: 1.3382 - acc: 0.4558 - val_loss: 1.2671 - val_acc: 0.4571\n",
      "Epoch 6/10\n",
      "8/8 [==============================] - 3s 340ms/step - loss: 1.3010 - acc: 0.4558 - val_loss: 1.2744 - val_acc: 0.4571\n",
      "Epoch 7/10\n",
      "8/8 [==============================] - 3s 340ms/step - loss: 1.3252 - acc: 0.4428 - val_loss: 1.2689 - val_acc: 0.4571\n",
      "Epoch 8/10\n",
      "8/8 [==============================] - 3s 339ms/step - loss: 1.2892 - acc: 0.4558 - val_loss: 1.2683 - val_acc: 0.4571\n",
      "Epoch 9/10\n",
      "8/8 [==============================] - 3s 366ms/step - loss: 1.3212 - acc: 0.4428 - val_loss: 1.2658 - val_acc: 0.4571\n",
      "Epoch 10/10\n",
      "8/8 [==============================] - 3s 381ms/step - loss: 1.3257 - acc: 0.4687 - val_loss: 1.2674 - val_acc: 0.4571\n",
      "Fold # 3 Accuracy 0.45714285714285713\n",
      "Epoch 1/10\n",
      "8/8 [==============================] - 4s 500ms/step - loss: 1.3128 - acc: 0.4621 - val_loss: 1.2686 - val_acc: 0.4571\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 3s 334ms/step - loss: 1.3527 - acc: 0.4428 - val_loss: 1.2660 - val_acc: 0.4571\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - 3s 342ms/step - loss: 1.3089 - acc: 0.4558 - val_loss: 1.2728 - val_acc: 0.4571\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - 3s 347ms/step - loss: 1.3121 - acc: 0.4558 - val_loss: 1.2691 - val_acc: 0.4571\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - 3s 343ms/step - loss: 1.3078 - acc: 0.4558 - val_loss: 1.2674 - val_acc: 0.4571\n",
      "Epoch 6/10\n",
      "8/8 [==============================] - 3s 343ms/step - loss: 1.3156 - acc: 0.4687 - val_loss: 1.2672 - val_acc: 0.4571\n",
      "Epoch 7/10\n",
      "8/8 [==============================] - 3s 342ms/step - loss: 1.2984 - acc: 0.4428 - val_loss: 1.2716 - val_acc: 0.4571\n",
      "Epoch 8/10\n",
      "8/8 [==============================] - 3s 342ms/step - loss: 1.3003 - acc: 0.4558 - val_loss: 1.2703 - val_acc: 0.4571\n",
      "Epoch 9/10\n",
      "8/8 [==============================] - 3s 340ms/step - loss: 1.3269 - acc: 0.4428 - val_loss: 1.2679 - val_acc: 0.4571\n",
      "Epoch 10/10\n",
      "8/8 [==============================] - 3s 344ms/step - loss: 1.3428 - acc: 0.4365 - val_loss: 1.2683 - val_acc: 0.4571\n",
      "Fold # 4 Accuracy 0.45714285714285713\n",
      "Epoch 1/10\n",
      "8/8 [==============================] - 6s 689ms/step - loss: 1.2950 - acc: 0.4785 - val_loss: 1.2689 - val_acc: 0.4706\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 2s 254ms/step - loss: 1.3384 - acc: 0.4475 - val_loss: 1.2691 - val_acc: 0.4706\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - 2s 263ms/step - loss: 1.3099 - acc: 0.4475 - val_loss: 1.2731 - val_acc: 0.4706\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - 3s 344ms/step - loss: 1.3155 - acc: 0.4475 - val_loss: 1.2770 - val_acc: 0.4706\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - 3s 345ms/step - loss: 1.3298 - acc: 0.4578 - val_loss: 1.2693 - val_acc: 0.4706\n",
      "Epoch 6/10\n",
      "8/8 [==============================] - 3s 341ms/step - loss: 1.3247 - acc: 0.4269 - val_loss: 1.2722 - val_acc: 0.4706\n",
      "Epoch 7/10\n",
      "8/8 [==============================] - 3s 344ms/step - loss: 1.3107 - acc: 0.4578 - val_loss: 1.2731 - val_acc: 0.4706\n",
      "Epoch 8/10\n",
      "8/8 [==============================] - 3s 345ms/step - loss: 1.2847 - acc: 0.4785 - val_loss: 1.2690 - val_acc: 0.4706\n",
      "Epoch 9/10\n",
      "8/8 [==============================] - 3s 347ms/step - loss: 1.3220 - acc: 0.4475 - val_loss: 1.2678 - val_acc: 0.4706\n",
      "Epoch 10/10\n",
      "8/8 [==============================] - 3s 344ms/step - loss: 1.3042 - acc: 0.4475 - val_loss: 1.2725 - val_acc: 0.4706\n",
      "Fold # 5 Accuracy 0.47058823529411764\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "cv = StratifiedKFold(5)\n",
    "\n",
    "model = create_simple_model()\n",
    "\n",
    "x = clean_data.drop('y', axis=1)\n",
    "y = [str(val) for val in clean_data['y'].values]\n",
    "x = x.values\n",
    "x = np.array([arr[0] for arr in x])\n",
    "\n",
    "acc = 0\n",
    "preds = np.array([])\n",
    "ys = np.array([])\n",
    "target_names = ['0', '1', '2a', '2b', '3']\n",
    "\n",
    "j = 1\n",
    "for train, test in cv.split(x, y):\n",
    "    x_train = x[train]\n",
    "    y_train = [y[i] for i in train]\n",
    "    x_test = x[test]\n",
    "    y_test = [y[i] for i in test]\n",
    "    \n",
    "    y_train_hot = pd.get_dummies(y_train)\n",
    "    y_test_hot = pd.get_dummies(y_test)\n",
    "    \n",
    "    batch_size = 20\n",
    "    score, model = train_simple_gen_model(model, batch_size, 10, x_train, y_train_hot, x_test, y_test_hot)\n",
    "    \n",
    "    Y_pred = model.predict(x_test.reshape(x_test.shape[0], img_rows, img_cols, 1), batch_size, len(y_test) // batch_size + 1)\n",
    "    y_pred = np.argmax(Y_pred, axis=1)\n",
    "    preds = np.append(preds, [target_names[ind] for ind in y_pred.astype(int)])\n",
    "    ys = np.append(ys, y_test)\n",
    "    \n",
    "    print ('Fold #', j, 'Accuracy', score[1])\n",
    "    acc+=score[1]\n",
    "    j+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "uoWgY-oY-Tcx",
    "outputId": "8653bd0f-e181-4b33-f47a-08a1c6a8afb1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy 0.4536279613059799\n"
     ]
    }
   ],
   "source": [
    "print('Mean Accuracy', acc/(j-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 806
    },
    "colab_type": "code",
    "id": "rGWpDheN1wJ-",
    "outputId": "8cb797e8-767c-4f68-fb17-0458f98bf067"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix, without normalization\n",
      "[[ 0  0  0 27  0]\n",
      " [ 0  0  0  7  0]\n",
      " [ 0  0  0 54  0]\n",
      " [ 0  0  0 82  0]\n",
      " [ 0  0  0 11  0]]\n",
      "Normalized confusion matrix\n",
      "[[0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATgAAAEYCAYAAADI0+pcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nzrmy9f8BUc315kGfiCwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATsAAAEYCAYAAAAj5FFfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from the sklearn library example\n",
    "# https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "# Plot non-normalized confusion matrix\n",
    "plot_confusion_matrix(ys, preds, classes=['0', '1', '2a', '2b', '3'],\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plot_confusion_matrix(ys, preds, classes=['0', '1', '2a', '2b', '3'], normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()\n",
    "plt.savefig('confusion_simple.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g6urwBNcJrdo"
   },
   "source": [
    "Why is it classifying everything into one class...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "ZmhY84DpKt2-",
    "outputId": "4c70f338-ce2d-4f9d-9180-5faeef9710aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4530386740331492\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i in ys:\n",
    "  if i == '2b':\n",
    "    count+=1\n",
    "print(count/(len(ys)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YBzexqEUNuIx"
   },
   "source": [
    "Ok so it actually is just classifying into 2b only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sFvTlkhiL3a0"
   },
   "outputs": [],
   "source": [
    "def train_inception_gen_model(model, batch_size, epochs, x, y, xt, yt, rot=20, width=0.2, height=0.2):\n",
    "  # force images to have three channels\n",
    "  xnew = np.zeros((x.shape[0], 512, 512, 3))\n",
    "  for i in range(0, x.shape[0]):\n",
    "      xnew[i, :, :, 0] = x[i]\n",
    "      xnew[i, :, :, 1] = x[i]\n",
    "      xnew[i, :, :, 2] = x[i]\n",
    "\n",
    "  xtnew = np.zeros((xt.shape[0], 512, 512, 3))\n",
    "  for i in range(0, xt.shape[0]):\n",
    "      xtnew[i, :, :, 0] = xt[i]\n",
    "      xtnew[i, :, :, 1] = xt[i]\n",
    "      xtnew[i, :, :, 2] = xt[i]\n",
    "      \n",
    "  train_generator, validation_generator = create_generators(xnew, y, xtnew, yt, batch_size, rot, width, height)\n",
    "\n",
    "  rmsprop = keras.optimizers.RMSprop(lr=0.005)\n",
    "  \n",
    "  model.compile(\n",
    "    loss=categorical_crossentropy,\n",
    "    optimizer=rmsprop,\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "  model.fit_generator(\n",
    "        train_generator,\n",
    "        epochs=epochs,\n",
    "        verbose=1,\n",
    "        steps_per_epoch = np.ceil(x.shape[0]/batch_size),\n",
    "        validation_steps = np.ceil(xt.shape[0]/batch_size),\n",
    "        validation_data=validation_generator)\n",
    "  score = model.evaluate(xtnew, yt, verbose=0)\n",
    "  return score, model, xtnew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1885
    },
    "colab_type": "code",
    "id": "uoyrJHnjLQpJ",
    "outputId": "a8a12716-e6f9-4845-d153-653bcf7a681b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "8/8 [==============================] - 45s 6s/step - loss: 4.1737 - acc: 0.3606 - val_loss: 2.1789 - val_acc: 0.4359\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 7s 895ms/step - loss: 2.6635 - acc: 0.4120 - val_loss: 1.4491 - val_acc: 0.4359\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - 7s 893ms/step - loss: 2.6238 - acc: 0.3676 - val_loss: 2.4222 - val_acc: 0.2821\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - 7s 891ms/step - loss: 2.9795 - acc: 0.4176 - val_loss: 11.5720 - val_acc: 0.2821\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - 8s 968ms/step - loss: 3.0503 - acc: 0.3106 - val_loss: 11.5720 - val_acc: 0.2821\n",
      "Epoch 6/10\n",
      "8/8 [==============================] - 9s 1s/step - loss: 2.9331 - acc: 0.4056 - val_loss: 9.2998 - val_acc: 0.2821\n",
      "Epoch 7/10\n",
      "8/8 [==============================] - 9s 1s/step - loss: 2.8168 - acc: 0.3606 - val_loss: 9.0923 - val_acc: 0.4359\n",
      "Epoch 8/10\n",
      "8/8 [==============================] - 10s 1s/step - loss: 2.6782 - acc: 0.4620 - val_loss: 9.0923 - val_acc: 0.4359\n",
      "Epoch 9/10\n",
      "8/8 [==============================] - 10s 1s/step - loss: 2.6472 - acc: 0.4120 - val_loss: 9.0923 - val_acc: 0.4359\n",
      "Epoch 10/10\n",
      "8/8 [==============================] - 9s 1s/step - loss: 1.6607 - acc: 0.3979 - val_loss: 9.0923 - val_acc: 0.4359\n",
      "Fold # 1 Accuracy 0.4358974358974359\n",
      "Epoch 1/10\n",
      "8/8 [==============================] - 48s 6s/step - loss: 2.2648 - acc: 0.4556 - val_loss: 8.9074 - val_acc: 0.4474\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 7s 881ms/step - loss: 1.8859 - acc: 0.4746 - val_loss: 8.9074 - val_acc: 0.4474\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - 7s 879ms/step - loss: 1.8687 - acc: 0.3988 - val_loss: 8.9074 - val_acc: 0.4474\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - 7s 890ms/step - loss: 2.3008 - acc: 0.4431 - val_loss: 8.9074 - val_acc: 0.4474\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - 9s 1s/step - loss: 2.6764 - acc: 0.4431 - val_loss: 8.9290 - val_acc: 0.4474\n",
      "Epoch 6/10\n",
      "8/8 [==============================] - 10s 1s/step - loss: 2.1380 - acc: 0.4431 - val_loss: 8.9275 - val_acc: 0.4474\n",
      "Epoch 7/10\n",
      "8/8 [==============================] - 9s 1s/step - loss: 1.7817 - acc: 0.5061 - val_loss: 8.5353 - val_acc: 0.4474\n",
      "Epoch 8/10\n",
      "8/8 [==============================] - 9s 1s/step - loss: 2.1972 - acc: 0.4431 - val_loss: 7.8305 - val_acc: 0.4474\n",
      "Epoch 9/10\n",
      "8/8 [==============================] - 10s 1s/step - loss: 1.6925 - acc: 0.4746 - val_loss: 1.3176 - val_acc: 0.4474\n",
      "Epoch 10/10\n",
      "8/8 [==============================] - 10s 1s/step - loss: 2.2497 - acc: 0.4368 - val_loss: 8.2046 - val_acc: 0.4474\n",
      "Fold # 2 Accuracy 0.4473684210526316\n",
      "Epoch 1/10\n",
      "8/8 [==============================] - 50s 6s/step - loss: 2.2650 - acc: 0.4299 - val_loss: 2.0616 - val_acc: 0.4571\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 7s 884ms/step - loss: 1.6410 - acc: 0.4558 - val_loss: 1.6306 - val_acc: 0.4571\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - 7s 884ms/step - loss: 2.2115 - acc: 0.4428 - val_loss: 1.2694 - val_acc: 0.4571\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - 7s 881ms/step - loss: 2.2287 - acc: 0.4428 - val_loss: 1.6257 - val_acc: 0.4571\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - 9s 1s/step - loss: 1.7040 - acc: 0.4687 - val_loss: 1.5001 - val_acc: 0.4571\n",
      "Epoch 6/10\n",
      "8/8 [==============================] - 10s 1s/step - loss: 1.9389 - acc: 0.4299 - val_loss: 1.6987 - val_acc: 0.4571\n",
      "Epoch 7/10\n",
      "8/8 [==============================] - 10s 1s/step - loss: 1.8625 - acc: 0.4428 - val_loss: 1.3280 - val_acc: 0.4571\n",
      "Epoch 8/10\n",
      "8/8 [==============================] - 10s 1s/step - loss: 2.5683 - acc: 0.4558 - val_loss: 6.4694 - val_acc: 0.4571\n",
      "Epoch 9/10\n",
      "8/8 [==============================] - 10s 1s/step - loss: 2.1148 - acc: 0.4687 - val_loss: 5.5638 - val_acc: 0.5143\n",
      "Epoch 10/10\n",
      "8/8 [==============================] - 10s 1s/step - loss: 2.2765 - acc: 0.3920 - val_loss: 5.6100 - val_acc: 0.5143\n",
      "Fold # 3 Accuracy 0.5142857151372092\n",
      "Epoch 1/10\n",
      "8/8 [==============================] - 54s 7s/step - loss: 1.9281 - acc: 0.4495 - val_loss: 8.4830 - val_acc: 0.2286\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 7s 882ms/step - loss: 1.9179 - acc: 0.4305 - val_loss: 1.2814 - val_acc: 0.4571\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - 7s 882ms/step - loss: 1.4701 - acc: 0.4495 - val_loss: 1.2760 - val_acc: 0.4571\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - 7s 880ms/step - loss: 1.3196 - acc: 0.4365 - val_loss: 1.2676 - val_acc: 0.4571\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - 10s 1s/step - loss: 1.2967 - acc: 0.4558 - val_loss: 2.1187 - val_acc: 0.3143\n",
      "Epoch 6/10\n",
      "8/8 [==============================] - 10s 1s/step - loss: 1.3358 - acc: 0.4365 - val_loss: 1.2924 - val_acc: 0.4571\n",
      "Epoch 7/10\n",
      "8/8 [==============================] - 10s 1s/step - loss: 1.3882 - acc: 0.4172 - val_loss: 1.2856 - val_acc: 0.4571\n",
      "Epoch 8/10\n",
      "8/8 [==============================] - 9s 1s/step - loss: 1.3284 - acc: 0.4432 - val_loss: 1.2669 - val_acc: 0.4571\n",
      "Epoch 9/10\n",
      "8/8 [==============================] - 9s 1s/step - loss: 1.3109 - acc: 0.4365 - val_loss: 1.2669 - val_acc: 0.4571\n",
      "Epoch 10/10\n",
      "8/8 [==============================] - 10s 1s/step - loss: 1.3353 - acc: 0.4624 - val_loss: 1.2714 - val_acc: 0.4571\n",
      "Fold # 4 Accuracy 0.45714285714285713\n",
      "Epoch 1/10\n",
      "8/8 [==============================] - 55s 7s/step - loss: 1.3831 - acc: 0.4159 - val_loss: 6.5124 - val_acc: 0.4706\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 7s 878ms/step - loss: 1.3040 - acc: 0.4641 - val_loss: 1.2727 - val_acc: 0.4706\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - 7s 882ms/step - loss: 2.0306 - acc: 0.4515 - val_loss: 1.2702 - val_acc: 0.4706\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - 7s 906ms/step - loss: 1.8068 - acc: 0.4372 - val_loss: 1.2768 - val_acc: 0.4706\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - 10s 1s/step - loss: 1.8846 - acc: 0.4475 - val_loss: 1.2756 - val_acc: 0.4706\n",
      "Epoch 6/10\n",
      "8/8 [==============================] - 10s 1s/step - loss: 1.9523 - acc: 0.4206 - val_loss: 1.2793 - val_acc: 0.4706\n",
      "Epoch 7/10\n",
      "8/8 [==============================] - 10s 1s/step - loss: 2.3328 - acc: 0.3867 - val_loss: 1.7237 - val_acc: 0.4412\n",
      "Epoch 8/10\n",
      "8/8 [==============================] - 10s 1s/step - loss: 1.8281 - acc: 0.4618 - val_loss: 1.7224 - val_acc: 0.4412\n",
      "Epoch 9/10\n",
      "8/8 [==============================] - 9s 1s/step - loss: 1.9201 - acc: 0.4412 - val_loss: 2.5128 - val_acc: 0.4706\n",
      "Epoch 10/10\n",
      "8/8 [==============================] - 9s 1s/step - loss: 1.9738 - acc: 0.4372 - val_loss: 2.6073 - val_acc: 0.4706\n",
      "Fold # 5 Accuracy 0.47058823529411764\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "cv = StratifiedKFold(5)\n",
    "\n",
    "model = create_inception_model()\n",
    "\n",
    "x = clean_data.drop('y', axis=1)\n",
    "y = [str(val) for val in clean_data['y'].values]\n",
    "x = x.values\n",
    "x = np.array([arr[0] for arr in x])\n",
    "\n",
    "acc = 0\n",
    "preds = np.array([])\n",
    "ys = np.array([])\n",
    "target_names = ['0', '1', '2a', '2b', '3']\n",
    "\n",
    "j = 1\n",
    "for train, test in cv.split(x, y):\n",
    "    x_train = x[train]\n",
    "    y_train = [y[i] for i in train]\n",
    "    x_test = x[test]\n",
    "    y_test = [y[i] for i in test]\n",
    "    \n",
    "    y_train_hot = pd.get_dummies(y_train)\n",
    "    y_test_hot = pd.get_dummies(y_test)\n",
    "    \n",
    "    batch_size = 20\n",
    "    score, model, xtnew = train_inception_gen_model(model, batch_size, 10, x_train, y_train_hot, x_test, y_test_hot, 90, 0.5, 0.5)\n",
    "    \n",
    "    Y_pred = model.predict(xtnew, batch_size, len(y_test) // batch_size + 1)\n",
    "    y_pred = np.argmax(Y_pred, axis=1)\n",
    "    preds = np.append(preds, [target_names[ind] for ind in y_pred.astype(int)])\n",
    "    ys = np.append(ys, y_test)\n",
    "    \n",
    "    print ('Fold #', j, 'Accuracy', score[1])\n",
    "    acc+=score[1]\n",
    "    j+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 123
    },
    "colab_type": "code",
    "id": "-c2nzC9SMFt_",
    "outputId": "8cbcb3aa-55e8-4a86-99d6-f5838c157670"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy 0.4650565329048503\n",
      "[[ 0  0  4 23  0]\n",
      " [ 0  0  1  6  0]\n",
      " [ 0  0  7 47  0]\n",
      " [ 0  0  5 77  0]\n",
      " [ 0  0  1 10  0]]\n"
     ]
    }
   ],
   "source": [
    "print('Mean Accuracy', acc/(j-1))\n",
    "confusion = confusion_matrix(ys, preds)\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 806
    },
    "colab_type": "code",
    "id": "t5vuNvRI2C2M",
    "outputId": "a1af298b-50d6-43b3-90bf-6cd126c2cb08"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix, without normalization\n",
      "[[ 0  0  4 23  0]\n",
      " [ 0  0  1  6  0]\n",
      " [ 0  0  7 47  0]\n",
      " [ 0  0  5 77  0]\n",
      " [ 0  0  1 10  0]]\n",
      "Normalized confusion matrix\n",
      "[[0.   0.   0.15 0.85 0.  ]\n",
      " [0.   0.   0.14 0.86 0.  ]\n",
      " [0.   0.   0.13 0.87 0.  ]\n",
      " [0.   0.   0.06 0.94 0.  ]\n",
      " [0.   0.   0.09 0.91 0.  ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATgAAAEYCAYAAADI0+pcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nNwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATsAAAEYCAYAAAAj5FFfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nWLIzxsSE/wcRtUWA7Pve2QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from the sklearn library example\n",
    "# https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "fig = plt.figure()\n",
    "# Plot non-normalized confusion matrix\n",
    "plot_confusion_matrix(ys, preds, classes=['0', '1', '2a', '2b', '3'],\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plot_confusion_matrix(ys, preds, classes=['0', '1', '2a', '2b', '3'], normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()\n",
    "fig.savefig('confusion_inception2.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F5A7x686QxbH"
   },
   "source": [
    "Better than just classifying into one class I guess, but fewer were classified into the correct class..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZUsKPDoLQTQh"
   },
   "source": [
    "## Try Classification for all except separating 2b and 2a, and then try run another classifier to separate 2a and 2b --- Failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zDt7aVAwQkjE"
   },
   "outputs": [],
   "source": [
    "conditions = [\n",
    "    (clean_data['y'] == '2b'),\n",
    "    (clean_data['y'] == '2a')]\n",
    "choices = ['2b', '2a']\n",
    "clean_data['ynew'] = np.select(conditions, choices, default='not2a2b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ISG2BSZzhfex"
   },
   "outputs": [],
   "source": [
    "conditions = [\n",
    "    (clean_data['y'] == 0),\n",
    "    (clean_data['y'] == 1),\n",
    "    (clean_data['y'] == 3)]\n",
    "choices = ['0', '1', '3']\n",
    "clean_data['ynew2'] = np.select(conditions, choices, default='is2a2b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fSux0OeNk5ON"
   },
   "outputs": [],
   "source": [
    "def create_simple_model1():\n",
    "  model = Sequential([\n",
    "      l.Conv2D(32, 5, padding='same', activation='relu', input_shape=(512,512,1)),\n",
    "      l.MaxPooling2D((2, 2), (2, 2), padding='same'),\n",
    "      l.BatchNormalization(),\n",
    "      l.Conv2D(64, 5, padding='same', activation='relu'),\n",
    "      l.MaxPooling2D((2, 2), (2, 2), padding='same'),\n",
    "      l.Conv2D(64, 5, padding='same', activation='relu'),\n",
    "      l.MaxPooling2D((2, 2), (2, 2), padding='same'),\n",
    "      l.Conv2D(64, 5, padding='same', activation='relu'),\n",
    "      l.MaxPooling2D((2, 2), (2, 2), padding='same'),\n",
    "      l.Conv2D(64, 5, padding='same', activation='relu'),\n",
    "      l.MaxPooling2D((2, 2), (2, 2), padding='same'),\n",
    "      l.Conv2D(64, 5, padding='same', activation='relu'),\n",
    "      l.MaxPooling2D((2, 2), (2, 2), padding='same'),\n",
    "      l.Flatten(),\n",
    "      l.Dense(1024, activation='relu'),\n",
    "      l.Dropout(0.4),\n",
    "      l.Dense(3,activation='softmax')\n",
    "  ])\n",
    "\n",
    "\n",
    "  return model\n",
    "\n",
    "def create_simple_model2():\n",
    "  model = Sequential([\n",
    "      l.Conv2D(32, 5, padding='same', activation='relu', input_shape=(512,512,1)),\n",
    "      l.MaxPooling2D((2, 2), (2, 2), padding='same'),\n",
    "      l.BatchNormalization(),\n",
    "      l.Conv2D(64, 5, padding='same', activation='relu'),\n",
    "      l.MaxPooling2D((2, 2), (2, 2), padding='same'),\n",
    "      l.Conv2D(64, 5, padding='same', activation='relu'),\n",
    "      l.MaxPooling2D((2, 2), (2, 2), padding='same'),\n",
    "      l.Conv2D(64, 5, padding='same', activation='relu'),\n",
    "      l.MaxPooling2D((2, 2), (2, 2), padding='same'),\n",
    "      l.Conv2D(64, 5, padding='same', activation='relu'),\n",
    "      l.MaxPooling2D((2, 2), (2, 2), padding='same'),\n",
    "      l.Conv2D(64, 5, padding='same', activation='relu'),\n",
    "      l.MaxPooling2D((2, 2), (2, 2), padding='same'),\n",
    "      l.Flatten(),\n",
    "      l.Dense(1024, activation='relu'),\n",
    "      l.Dropout(0.4),\n",
    "      l.Dense(4,activation='softmax')\n",
    "  ])\n",
    "\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3733
    },
    "colab_type": "code",
    "id": "UK3kOZFzVVHa",
    "outputId": "9d78e3a9-0af4-4a12-aec0-6b5a1a30f209"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "8/8 [==============================] - 13s 2s/step - loss: 1.1082 - acc: 0.4113 - val_loss: 1.0839 - val_acc: 0.4359\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 2s 244ms/step - loss: 1.0861 - acc: 0.4620 - val_loss: 1.0803 - val_acc: 0.4359\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - 2s 240ms/step - loss: 1.0847 - acc: 0.4120 - val_loss: 1.0803 - val_acc: 0.4359\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - 2s 239ms/step - loss: 1.0675 - acc: 0.4620 - val_loss: 1.0803 - val_acc: 0.4359\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - 3s 318ms/step - loss: 1.0887 - acc: 0.4120 - val_loss: 1.0839 - val_acc: 0.4359\n",
      "Epoch 6/10\n",
      "8/8 [==============================] - 3s 333ms/step - loss: 1.0965 - acc: 0.4120 - val_loss: 1.0848 - val_acc: 0.4359\n",
      "Epoch 7/10\n",
      "8/8 [==============================] - 3s 334ms/step - loss: 1.0671 - acc: 0.5120 - val_loss: 1.0841 - val_acc: 0.4359\n",
      "Epoch 8/10\n",
      "8/8 [==============================] - 3s 334ms/step - loss: 1.0906 - acc: 0.4120 - val_loss: 1.0795 - val_acc: 0.4359\n",
      "Epoch 9/10\n",
      "8/8 [==============================] - 3s 333ms/step - loss: 1.0568 - acc: 0.4620 - val_loss: 1.0789 - val_acc: 0.4359\n",
      "Epoch 10/10\n",
      "8/8 [==============================] - 3s 326ms/step - loss: 1.0556 - acc: 0.4620 - val_loss: 1.0836 - val_acc: 0.4359\n",
      "Epoch 1/10\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.9802 - acc: 0.6958 - val_loss: 1.0892 - val_acc: 0.7179\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 2s 248ms/step - loss: 0.8983 - acc: 0.7345 - val_loss: 0.8800 - val_acc: 0.7179\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - 2s 243ms/step - loss: 0.7799 - acc: 0.7845 - val_loss: 1.0066 - val_acc: 0.7179\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - 2s 246ms/step - loss: 0.7416 - acc: 0.7845 - val_loss: 0.8960 - val_acc: 0.7179\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - 2s 281ms/step - loss: 0.7691 - acc: 0.7845 - val_loss: 0.8776 - val_acc: 0.7179\n",
      "Epoch 6/10\n",
      "8/8 [==============================] - 3s 332ms/step - loss: 0.7220 - acc: 0.7845 - val_loss: 0.9003 - val_acc: 0.7179\n",
      "Epoch 7/10\n",
      "8/8 [==============================] - 3s 335ms/step - loss: 0.7486 - acc: 0.7845 - val_loss: 0.8771 - val_acc: 0.7179\n",
      "Epoch 8/10\n",
      "8/8 [==============================] - 3s 331ms/step - loss: 0.8737 - acc: 0.7345 - val_loss: 0.8804 - val_acc: 0.7179\n",
      "Epoch 9/10\n",
      "8/8 [==============================] - 3s 329ms/step - loss: 0.8220 - acc: 0.7345 - val_loss: 0.8865 - val_acc: 0.7179\n",
      "Epoch 10/10\n",
      "8/8 [==============================] - 3s 338ms/step - loss: 0.7317 - acc: 0.7845 - val_loss: 0.9077 - val_acc: 0.7179\n",
      "Fold # 1 Accuracy 1st 0.4358974358974359\n",
      "Fold # 1 Accuracy 2nd 0.717948717948718\n",
      "Epoch 1/10\n",
      "8/8 [==============================] - 13s 2s/step - loss: 1.0844 - acc: 0.4116 - val_loss: 1.0704 - val_acc: 0.4474\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 2s 241ms/step - loss: 1.0559 - acc: 0.4746 - val_loss: 1.0721 - val_acc: 0.4474\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - 2s 239ms/step - loss: 1.0501 - acc: 0.4746 - val_loss: 1.0735 - val_acc: 0.4474\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - 2s 240ms/step - loss: 1.0940 - acc: 0.4431 - val_loss: 1.0773 - val_acc: 0.4474\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - 3s 330ms/step - loss: 1.0776 - acc: 0.4431 - val_loss: 1.0711 - val_acc: 0.4474\n",
      "Epoch 6/10\n",
      "8/8 [==============================] - 3s 335ms/step - loss: 1.0578 - acc: 0.4746 - val_loss: 1.0708 - val_acc: 0.4474\n",
      "Epoch 7/10\n",
      "8/8 [==============================] - 3s 332ms/step - loss: 1.0610 - acc: 0.4746 - val_loss: 1.0705 - val_acc: 0.4474\n",
      "Epoch 8/10\n",
      "8/8 [==============================] - 3s 333ms/step - loss: 1.0398 - acc: 0.5061 - val_loss: 1.0715 - val_acc: 0.4474\n",
      "Epoch 9/10\n",
      "8/8 [==============================] - 3s 333ms/step - loss: 1.0585 - acc: 0.4746 - val_loss: 1.0746 - val_acc: 0.4474\n",
      "Epoch 10/10\n",
      "8/8 [==============================] - 3s 336ms/step - loss: 1.1008 - acc: 0.4116 - val_loss: 1.0729 - val_acc: 0.4474\n",
      "Epoch 1/10\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.8313 - acc: 0.7469 - val_loss: 0.8626 - val_acc: 0.7368\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 2s 246ms/step - loss: 0.7737 - acc: 0.7784 - val_loss: 0.8327 - val_acc: 0.7368\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - 2s 243ms/step - loss: 0.8177 - acc: 0.7469 - val_loss: 0.8352 - val_acc: 0.7368\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - 2s 243ms/step - loss: 0.8036 - acc: 0.7469 - val_loss: 0.8313 - val_acc: 0.7368\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - 2s 293ms/step - loss: 0.7576 - acc: 0.7784 - val_loss: 0.8318 - val_acc: 0.7368\n",
      "Epoch 6/10\n",
      "8/8 [==============================] - 3s 334ms/step - loss: 0.9674 - acc: 0.7154 - val_loss: 0.8326 - val_acc: 0.7368\n",
      "Epoch 7/10\n",
      "8/8 [==============================] - 3s 334ms/step - loss: 0.8380 - acc: 0.7469 - val_loss: 0.8632 - val_acc: 0.7368\n",
      "Epoch 8/10\n",
      "8/8 [==============================] - 3s 333ms/step - loss: 0.7495 - acc: 0.7784 - val_loss: 0.8376 - val_acc: 0.7368\n",
      "Epoch 9/10\n",
      "8/8 [==============================] - 3s 338ms/step - loss: 0.7443 - acc: 0.7784 - val_loss: 0.8603 - val_acc: 0.7368\n",
      "Epoch 10/10\n",
      "8/8 [==============================] - 3s 335ms/step - loss: 0.8458 - acc: 0.7469 - val_loss: 0.8335 - val_acc: 0.7368\n",
      "Fold # 2 Accuracy 1st 0.4473684210526316\n",
      "Fold # 2 Accuracy 2nd 0.7368421052631579\n",
      "Epoch 1/10\n",
      "8/8 [==============================] - 14s 2s/step - loss: 1.0713 - acc: 0.4558 - val_loss: 1.0616 - val_acc: 0.4571\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 2s 248ms/step - loss: 1.0790 - acc: 0.4299 - val_loss: 1.0610 - val_acc: 0.4571\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - 2s 247ms/step - loss: 1.0623 - acc: 0.4687 - val_loss: 1.0628 - val_acc: 0.4571\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - 2s 262ms/step - loss: 1.0747 - acc: 0.4428 - val_loss: 1.0611 - val_acc: 0.4571\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - 3s 392ms/step - loss: 1.0625 - acc: 0.4687 - val_loss: 1.0616 - val_acc: 0.4571\n",
      "Epoch 6/10\n",
      "8/8 [==============================] - 3s 390ms/step - loss: 1.0771 - acc: 0.4299 - val_loss: 1.0616 - val_acc: 0.4571\n",
      "Epoch 7/10\n",
      "8/8 [==============================] - 3s 379ms/step - loss: 1.0553 - acc: 0.4687 - val_loss: 1.0618 - val_acc: 0.4571\n",
      "Epoch 8/10\n",
      "8/8 [==============================] - 3s 341ms/step - loss: 1.0649 - acc: 0.4558 - val_loss: 1.0620 - val_acc: 0.4571\n",
      "Epoch 9/10\n",
      "8/8 [==============================] - 3s 344ms/step - loss: 1.0631 - acc: 0.4687 - val_loss: 1.0614 - val_acc: 0.4571\n",
      "Epoch 10/10\n",
      "8/8 [==============================] - 3s 341ms/step - loss: 1.0737 - acc: 0.4428 - val_loss: 1.0616 - val_acc: 0.4571\n",
      "Epoch 1/10\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.8039 - acc: 0.7533 - val_loss: 0.7454 - val_acc: 0.7714\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 2s 245ms/step - loss: 0.8444 - acc: 0.7404 - val_loss: 0.7442 - val_acc: 0.7714\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - 2s 246ms/step - loss: 0.9209 - acc: 0.7015 - val_loss: 0.7511 - val_acc: 0.7714\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - 2s 248ms/step - loss: 0.8321 - acc: 0.7404 - val_loss: 0.7638 - val_acc: 0.7714\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - 2s 304ms/step - loss: 0.7811 - acc: 0.7663 - val_loss: 0.7453 - val_acc: 0.7714\n",
      "Epoch 6/10\n",
      "8/8 [==============================] - 3s 339ms/step - loss: 0.8501 - acc: 0.7533 - val_loss: 0.7465 - val_acc: 0.7714\n",
      "Epoch 7/10\n",
      "8/8 [==============================] - 3s 343ms/step - loss: 0.8510 - acc: 0.7404 - val_loss: 0.7526 - val_acc: 0.7714\n",
      "Epoch 8/10\n",
      "8/8 [==============================] - 3s 339ms/step - loss: 0.8013 - acc: 0.7533 - val_loss: 0.7632 - val_acc: 0.7714\n",
      "Epoch 9/10\n",
      "8/8 [==============================] - 3s 338ms/step - loss: 0.8340 - acc: 0.7404 - val_loss: 0.7457 - val_acc: 0.7714\n",
      "Epoch 10/10\n",
      "8/8 [==============================] - 3s 343ms/step - loss: 0.7862 - acc: 0.7663 - val_loss: 0.7516 - val_acc: 0.7714\n",
      "Fold # 3 Accuracy 1st 0.45714285714285713\n",
      "Fold # 3 Accuracy 2nd 0.7714285722800663\n",
      "Epoch 1/10\n",
      "8/8 [==============================] - 15s 2s/step - loss: 1.0734 - acc: 0.4428 - val_loss: 1.0605 - val_acc: 0.4571\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 2s 246ms/step - loss: 1.0590 - acc: 0.4687 - val_loss: 1.0606 - val_acc: 0.4571\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - 2s 244ms/step - loss: 1.0650 - acc: 0.4687 - val_loss: 1.0611 - val_acc: 0.4571\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - 2s 245ms/step - loss: 1.0785 - acc: 0.4428 - val_loss: 1.0609 - val_acc: 0.4571\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - 2s 307ms/step - loss: 1.0824 - acc: 0.4299 - val_loss: 1.0623 - val_acc: 0.4571\n",
      "Epoch 6/10\n",
      "8/8 [==============================] - 3s 340ms/step - loss: 1.0645 - acc: 0.4558 - val_loss: 1.0640 - val_acc: 0.4571\n",
      "Epoch 7/10\n",
      "8/8 [==============================] - 3s 342ms/step - loss: 1.0572 - acc: 0.4817 - val_loss: 1.0639 - val_acc: 0.4571\n",
      "Epoch 8/10\n",
      "8/8 [==============================] - 3s 341ms/step - loss: 1.0605 - acc: 0.4558 - val_loss: 1.0626 - val_acc: 0.4571\n",
      "Epoch 9/10\n",
      "8/8 [==============================] - 3s 341ms/step - loss: 1.0667 - acc: 0.4558 - val_loss: 1.0617 - val_acc: 0.4571\n",
      "Epoch 10/10\n",
      "8/8 [==============================] - 3s 344ms/step - loss: 1.0503 - acc: 0.4817 - val_loss: 1.0614 - val_acc: 0.4571\n",
      "Epoch 1/10\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.8391 - acc: 0.7404 - val_loss: 0.7460 - val_acc: 0.7714\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 2s 250ms/step - loss: 0.8111 - acc: 0.7404 - val_loss: 0.7519 - val_acc: 0.7714\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - 2s 249ms/step - loss: 0.8061 - acc: 0.7533 - val_loss: 0.7474 - val_acc: 0.7714\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - 2s 248ms/step - loss: 0.8526 - acc: 0.7404 - val_loss: 0.7490 - val_acc: 0.7714\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - 2s 289ms/step - loss: 0.8233 - acc: 0.7404 - val_loss: 0.7543 - val_acc: 0.7714\n",
      "Epoch 6/10\n",
      "8/8 [==============================] - 3s 340ms/step - loss: 0.8151 - acc: 0.7404 - val_loss: 0.7459 - val_acc: 0.7714\n",
      "Epoch 7/10\n",
      "8/8 [==============================] - 3s 340ms/step - loss: 0.9023 - acc: 0.7274 - val_loss: 0.7440 - val_acc: 0.7714\n",
      "Epoch 8/10\n",
      "8/8 [==============================] - 3s 343ms/step - loss: 0.8165 - acc: 0.7274 - val_loss: 0.7620 - val_acc: 0.7714\n",
      "Epoch 9/10\n",
      "8/8 [==============================] - 3s 340ms/step - loss: 0.8030 - acc: 0.7533 - val_loss: 0.7537 - val_acc: 0.7714\n",
      "Epoch 10/10\n",
      "8/8 [==============================] - 3s 343ms/step - loss: 0.8075 - acc: 0.7533 - val_loss: 0.7453 - val_acc: 0.7714\n",
      "Fold # 4 Accuracy 1st 0.45714285714285713\n",
      "Fold # 4 Accuracy 2nd 0.7714285722800663\n",
      "Epoch 1/10\n",
      "8/8 [==============================] - 15s 2s/step - loss: 1.0612 - acc: 0.4681 - val_loss: 1.0558 - val_acc: 0.4706\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 2s 246ms/step - loss: 1.0648 - acc: 0.4578 - val_loss: 1.0560 - val_acc: 0.4706\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - 2s 245ms/step - loss: 1.0722 - acc: 0.4475 - val_loss: 1.0559 - val_acc: 0.4706\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - 2s 248ms/step - loss: 1.0636 - acc: 0.4578 - val_loss: 1.0564 - val_acc: 0.4706\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - 2s 307ms/step - loss: 1.0727 - acc: 0.4372 - val_loss: 1.0565 - val_acc: 0.4706\n",
      "Epoch 6/10\n",
      "8/8 [==============================] - 3s 342ms/step - loss: 1.0733 - acc: 0.4475 - val_loss: 1.0565 - val_acc: 0.4706\n",
      "Epoch 7/10\n",
      "8/8 [==============================] - 3s 346ms/step - loss: 1.0713 - acc: 0.4578 - val_loss: 1.0578 - val_acc: 0.4706\n",
      "Epoch 8/10\n",
      "8/8 [==============================] - 3s 342ms/step - loss: 1.0701 - acc: 0.4475 - val_loss: 1.0578 - val_acc: 0.4706\n",
      "Epoch 9/10\n",
      "8/8 [==============================] - 3s 342ms/step - loss: 1.0754 - acc: 0.4372 - val_loss: 1.0572 - val_acc: 0.4706\n",
      "Epoch 10/10\n",
      "8/8 [==============================] - 3s 347ms/step - loss: 1.0690 - acc: 0.4475 - val_loss: 1.0570 - val_acc: 0.4706\n",
      "Epoch 1/10\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.8778 - acc: 0.7252 - val_loss: 0.7635 - val_acc: 0.7647\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 2s 247ms/step - loss: 0.8508 - acc: 0.7458 - val_loss: 0.7995 - val_acc: 0.7647\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - 2s 248ms/step - loss: 0.8394 - acc: 0.7355 - val_loss: 0.7598 - val_acc: 0.7647\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - 2s 245ms/step - loss: 0.7944 - acc: 0.7562 - val_loss: 0.7594 - val_acc: 0.7647\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - 2s 307ms/step - loss: 0.8401 - acc: 0.7355 - val_loss: 0.7590 - val_acc: 0.7647\n",
      "Epoch 6/10\n",
      "8/8 [==============================] - 3s 343ms/step - loss: 0.8200 - acc: 0.7355 - val_loss: 0.7652 - val_acc: 0.7647\n",
      "Epoch 7/10\n",
      "8/8 [==============================] - 3s 343ms/step - loss: 0.8430 - acc: 0.7355 - val_loss: 0.7668 - val_acc: 0.7647\n",
      "Epoch 8/10\n",
      "8/8 [==============================] - 3s 343ms/step - loss: 0.7997 - acc: 0.7562 - val_loss: 0.7602 - val_acc: 0.7647\n",
      "Epoch 9/10\n",
      "8/8 [==============================] - 3s 344ms/step - loss: 0.8210 - acc: 0.7458 - val_loss: 0.7594 - val_acc: 0.7647\n",
      "Epoch 10/10\n",
      "8/8 [==============================] - 3s 343ms/step - loss: 0.7964 - acc: 0.7562 - val_loss: 0.7634 - val_acc: 0.7647\n",
      "Fold # 5 Accuracy 1st 0.47058823529411764\n",
      "Fold # 5 Accuracy 2nd 0.7647058823529411\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "cv = StratifiedKFold(5)\n",
    "\n",
    "model1 = create_simple_model1()\n",
    "model2 = create_simple_model2()\n",
    "\n",
    "x = clean_data.drop('y', axis=1)\n",
    "x = clean_data.drop('ynew', axis=1)\n",
    "x = x.values\n",
    "x = np.array([arr[0] for arr in x])\n",
    "\n",
    "y1 = [str(i) for i in clean_data['ynew'].values]\n",
    "y2 = [str(i) for i in clean_data['ynew2'].values]\n",
    "y = [str(i) for i in clean_data['y'].values]\n",
    "\n",
    "acc1 = 0\n",
    "acc2 = 0\n",
    "preds = np.array([])\n",
    "ys = np.array([])\n",
    "y2s = np.array([])\n",
    "y2_preds = np.array([])\n",
    "target_names1 = ['2a', '2b', 'not2a2b']\n",
    "target_names2 = ['0', '1', '3', 'is2a2b']\n",
    "\n",
    "j = 1\n",
    "for train, test in cv.split(x, y):\n",
    "    x_train = x[train]\n",
    "    y_train1 = [y1[i] for i in train]\n",
    "    y_train2 = [y2[i] for i in train]\n",
    "    x_test = x[test]\n",
    "    y_test1 = [y1[i] for i in test]\n",
    "    y_test2 = [y2[i] for i in test]\n",
    "    y_test = [y[i] for i in test]\n",
    "    \n",
    "    y_train_hot1 = pd.get_dummies(y_train1)\n",
    "    y_test_hot1 = pd.get_dummies(y_test1)\n",
    "    \n",
    "    y_train_hot2 = pd.get_dummies(y_train2)\n",
    "    y_test_hot2 = pd.get_dummies(y_test2)\n",
    "    \n",
    "    batch_size = 20\n",
    "    score1, model1 = train_simple_gen_model(model1, batch_size, 10, x_train, y_train_hot1, x_test, y_test_hot1)\n",
    "    score2, model2 = train_simple_gen_model(model2, batch_size, 10, x_train, y_train_hot2, x_test, y_test_hot2)\n",
    "    \n",
    "    Y_pred1 = model1.predict(x_test.reshape(x_test.shape[0], img_rows, img_cols, 1), batch_size, len(y_test) // batch_size + 1)\n",
    "    y_pred_ind1 = np.argmax(Y_pred1, axis=1)\n",
    "    y_pred1 = [target_names1[ind] for ind in y_pred_ind1.astype(int)]\n",
    "    \n",
    "    Y_pred2 = model2.predict(x_test.reshape(x_test.shape[0], img_rows, img_cols, 1), batch_size, len(y_test) // batch_size + 1)\n",
    "    y_pred_ind2 = np.argmax(Y_pred2, axis=1)\n",
    "    y_pred2 = [target_names2[ind] for ind in y_pred_ind2.astype(int)]\n",
    "    \n",
    "    y2_preds = np.append(y2_preds, y_pred2)\n",
    "    y2s = np.append(y2s, y_test2)\n",
    "    \n",
    "    for i in range(len(y_pred1)):\n",
    "      if y_pred2[i] != 'is2a2b':\n",
    "        continue\n",
    "      elif y_pred1[i] == 'not2a2b':\n",
    "          y_pred2[i] = '2a'\n",
    "      else:\n",
    "        y_pred2[i] = y_pred1[i]\n",
    "    \n",
    "    \n",
    "    preds = np.append(preds, y_pred2)\n",
    "    ys = np.append(ys, y_test)\n",
    "    \n",
    "    \n",
    "    \n",
    "    print ('Fold #', j, 'Accuracy 1st', score1[1])\n",
    "    print ('Fold #', j, 'Accuracy 2nd', score2[1])\n",
    "    acc1+=score1[1]\n",
    "    acc2+=score2[1]\n",
    "    j+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "NGSTMJTij208",
    "outputId": "c365e737-11c0-4a5c-a01f-d9e72a895e15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy 1 0.4536279613059799\n",
      "Mean Accuracy 2 0.75247077002499\n",
      "Total Accuracy 0.4530386740331492\n"
     ]
    }
   ],
   "source": [
    "print('Mean Accuracy 1', acc1/(j-1))\n",
    "print('Mean Accuracy 2', acc2/(j-1))\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('Total Accuracy', accuracy_score(ys, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JmW0D800mxGd"
   },
   "outputs": [],
   "source": [
    "# from the sklearn library example\n",
    "# https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax\n",
    "\n",
    "\n",
    "np.set_printoptions(precision=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 788
    },
    "colab_type": "code",
    "id": "0JdCxfban-Qr",
    "outputId": "3a9cef2d-ecec-4ef3-8391-3613c9386259"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix, without normalization\n",
      "[[ 0  0  0 27  0]\n",
      " [ 0  0  0  7  0]\n",
      " [ 0  0  0 54  0]\n",
      " [ 0  0  0 82  0]\n",
      " [ 0  0  0 11  0]]\n",
      "Normalized confusion matrix\n",
      "[[0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATgAAAEYCAYAAADI0+pcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nzrmy9f8BUc315kGfiCwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATsAAAEYCAYAAAAj5FFfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3Xt8FfWd//HXGwKIFUgKqE2CgkSN\nxFKVW72gVvHCEmB/KooilcUW24pVe7EKXfDW0pZqW1d2XawuigqI2g1QBV1a26LFcPGCAS9RoJKo\nXCRERUHSz++PmcBJSHIOMOGck/N5+piHZ2a+853PmYRPvnP7fmVmOOdcS9cq2QE459zB4MnOOZcR\nPNk55zKCJzvnXEbwZOecywie7JxzGcGTXYqSdKukR8LPR0n6RFLriPexTtKgKOtMYJ/flfRh+H06\nH0A9n0g6JsrYkkVSmaSzkx1HS5exyS78h75R0pdiln1L0vNJDKtBZvYPMzvMzGqSHcuBkNQGuBs4\nP/w+W/a3rnD7d6OLLnqSZki6M145Mysys+cPQkgZLWOTXag1cP2BVqJAph/LRBwBHAKUJTuQVCAp\nK9kxZJJM/wc6FfiRpOyGVko6TdIySdvC/58Ws+55ST+T9AKwHTgmXHanpBfD06z5kjpLelRSdVhH\n95g6fifpvXDdCkkDG4mjuySTlCXp1LDu2ulzSevCcq0k3SzpHUlbJD0u6csx9YyWtD5cN7GpAyOp\nvaS7wvLbJC2R1D5cNyw89aoKv/MJMdutk/QjSa+F282RdIik44A3w2JVkv4U+73qHddvhZ8LJP0l\nrGezpDkx5UxSQfi5k6SHJW0K4/1p7R8fSWPC2H8taauktZIGN/G910n6cRj/p5IekHSEpGckfSzp\n/yTlxJSfK+mDMMa/SioKl48DRgE31f4uxNT/E0mvAZ+GP9PdlxMkPS3prpj6Z0t6sKmflUuQmWXk\nBKwDBgFPAXeGy74FPB9+/jKwFRgNZAGXh/Odw/XPA/8AisL1bcJl5UBPoBOwGngr3E8W8DDwPzEx\nXAl0Dtf9EPgAOCRcdyvwSPi5O2BAVr3v0Ab4CzAlnL8eWArkA+2A/wZmhet6AZ8AZ4br7gZ2AYMa\nOT7Twu+TR9ACPi3c7jjgU+C8cP83hd+5bcxxLQVyw2O4BvhOQ9+joe8V7vNb4edZwESCP8qHAGfE\nlDOgIPz8MFACdAjrfAu4Olw3BvgC+Hb4Pb4LVAJq4vdiKUErNA/YCKwETg5j+BMwOab82HC/7YDf\nAq/ErJtB+LtVr/5XgG5A+9jfxfDzkeE+zyFIlu8CHZL976UlTEkPIGlffE+yOxHYBnSlbrIbDZTW\n2+bvwJjw8/PA7fXWPw9MjJm/C3gmZn5o7D+GBmLaCnwt/Hwr8ZPdfwELgFbh/Brg3Jj1Xwn/oWcB\nk4DZMeu+BOykgWQXJpfPamOpt+7fgcfrla0Azo45rlfGrP8VcF9D36Oh70XdZPcwMB3IbyAOAwoI\nEthOoFfMumtifo5jgPKYdYeG2x7ZxO/FqJj5J4H/ipm/DvjfRrbNDuvuFM7PoOFkN7ah38WY+YuB\n94DNxCR4nw5syvTTWMzsdYKEcXO9VbnA+nrL1hP8ta/1XgNVfhjz+bMG5g+rnQlP99aEp0BVBK3B\nLonELeka4GzgCjP7Z7j4aOAP4ellFUHyqyFopeTGxmtmnwKN3SDoQtCKeaeBdXWOS7jv96h7XD6I\n+bydmO+8j24CBJSGp81jG4m1DXV/VvV/TrvjMbPt4cemYkroZyiptaRfhJcNqgmSVm1MTWno9ybW\nfIIk/qaZLYlT1iUo45NdaDLBaU7sP5BKguQR6yiCVkyt/e4yJrw+dxNwKZBjZtkELUwluO0dwHAz\nq45Z9R4w2MyyY6ZDzKwCeJ/g1Km2jkMJTqEbshn4nOB0vL46x0WSwnorGigbz6fh/w+NWXZk7Qcz\n+8DMvm1muQSttf+svU5XL9YvqPuzqv9zai5XAMMJzhA6EbRUYc/PsLHfj3i/Nz8j+EP1FUmXH2CM\nLuTJDjCzcmAO8P2YxU8Dx0m6IryIfBnBda8FEe22A8E1s01AlqRJQMd4G0nqBjwOfNPM3qq3+j7g\nZ5KODst2lTQ8XPcEUCzpDEltgdtp5OcfttYeBO6WlBu2YE6V1C7c9xBJ5yp4lOSHwA7gxX369sF+\nNhEkpSvDfYwlJsFKGiEpP5zdSpAk/lmvjpowpp9J6hB+9x8Aj+xrPPuhA8F330KQsH9eb/2HwD49\nCyjpTODfgG8CVwH/ISmv6a1cIjzZ7XE7wXUsACx4BqyY4B/zFoJWWLGZbY5of4uAhQQX09cTtKTi\nnd4AnEtwWvqE9tyRrX2U43fAPOBZSR8TXGgfEH6fMuBa4DGCVt5WYEMT+/kRsApYBnwE/JLg2uCb\nBDdW/oOgVTUUGGpmOxP83vV9G/gxwTEuom7S7Ae8JOmT8Htdbw0/W3cdQSvxXWBJ+B0Pxh3Mhwl+\ndhUEN6OW1lv/ANArvKzwv/Eqk9QxrHO8mVWY2d/COv4nbEG7A6DwgqhzzrVo3rJzzmUET3bOuZQj\n6UEFr3O+3sh6SbpHUnn4APgp8er0ZOecS0UzgAubWD8YODacxhE8c9okT3bOuZRjZn8luDHWmOHA\nwxZYCmRL+kpTdabFi8hdunSxo4/unuwwXJK8vOYfyQ4hYSefcFSyQ0jI+vXr2Lx5c6R3eFt3PNps\n12dxy9lnm8oInj6oNd3Mpu/j7vKo+/TChnDZ+41tkBbJ7uiju/PCS8uTHYZLkpx+45MdQsJeeOne\nZIeQkNMH9I28Ttv1Ge2OvzRuuc9fmfa5mUUfQBxpkeycc+lAcPB6Oqsg5o0ggs4vmnxrxq/ZOeei\nIaBV6/hTNOYB3wzvyn4d2GZmjZ7CgrfsnHNRiuhFD0mzCDq66CJpA8H7620AzOw+gtc5/4Wge7Ht\nBK/YNcmTnXMuItGdxppZkx0gWPDq17X7UqcnO+dcdFL4FV5Pds65aIiDeYNin3myc85FRFHegIic\nJzvnXHT8NNY51/Id1Ofs9pknO+dcNIS37JxzGcJbds65lk/Q2m9QOOdaOn/0xDmXMfyanXOu5fO7\nsc65TJHCLbvUTcMH4NlFC+lddDxFhQVM/dUv9lq/Y8cOrrziMooKCxh42gDWr1u3e93UX06hqLCA\n3kXH89yzizzONIv1vsmjWL94CsvnTmi0zF03XcLrJZMpnXMLJxXm714+augAVpVMYlXJJEYNHdCs\ncdZKl+OaEOlgdvG0z5KS7CRdKOnNcGSgm6Osu6amhhu+fy0l85/h5ddWM3f2LNasXl2nzIwHHyAn\nO4eyN8q57vobmTjhJwCsWb2auXNms/LVMuYtWMj1132PmpqaKMNLuzjTLdaZ85cy/Nppja6/4Ixe\n9DyqKycOv43xd87ingkjAcjpeCgTxw3mzNG/ZuCVU5k4bjDZHdo3W5yQXsc1YWoVf0qSg75nSa2B\naQSjA/UCLpfUK6r6l5WW0rNnAT2OOYa2bdsy4rKRLJhfUqfMgvkljBp9FQAXXXwJz/9pMWbGgvkl\njLhsJO3ataN7jx707FnAstLSqEJLyzjTLdYXVr7DR9u2N7q++KzePLYg2H/pqnV06tCeI7t05LzT\nTmDx0jfYWr2dqo8/Y/HSNzj/9Mh+LRuUTsc1YVL8KUmSkWb7A+Vm9q6Z7QRmE4wUFInKygry8/f0\n1pyXl09FRcXeZboFZbKysujYqRNbtmyhomLvbSsrm+zpucXHmW6xxpN7eDYbPti6e77iwypyD88m\nt2s2Gz6MWb6xityu2c0aS0s6rgF5y66exkYFqkPSOEnLJS3ftHnTQQvOOXcAvGW378xsupn1NbO+\nXbt0TXi73Nw8NmzYk0srKjaQl5e3d5n3gjK7du2iets2OnfuTF7e3tvm5u6VhyORLnGmW6zxVG6s\nIv/InN3zeUdkU7mxispNVeQfEbP88GwqN1U1aywt6bgC4Q2KrPhTkiQj2e3zqED7om+/fpSXv826\ntWvZuXMnc+fMZkjxsDplhhQP49GZDwHw1JNPcNY3zkESQ4qHMXfObHbs2MG6tWspL3+bfv37RxVa\nWsaZbrHG88e/rOKK4mD//b/anepPPuODzdU89+IaBp1aSHaH9mR3aM+gUwt57sU1zRpLSzquu6Vw\nyy4ZaXYZcKykHgRJbiRwRVSVZ2Vl8Zvf3cvQIRdQU1PDVWPG0quoiNtvncQpffpSPHQYY8Zezdgx\noykqLCAn58vMfHQ2AL2Kirh4xKWc3LsXWVlZ/PaeabRupnf90iXOdIv1oSljGNjnWLpkH0b5wju4\n476naZMV7O/3Tyxh4ZIyLjijiLJ5k9n++Rdcc+sjAGyt3s6U+xey5JGbAPj59IVsrW78RkcU0um4\nJiyFHypWMG7FQd6p9C/Ab4HWwINm9rOmyvfp09d8kOzMlU6DZG9dlj6DZK9YsTzSZlar7KOt3dkT\n45b7vOSaFRkzSLaZPU0wFJpzrqWQvy7mnMsQauXJzjnXwgUdFafuu7Ge7Jxz0VA4pShPds65iMhb\nds65zODJzjmXEVr5DQrnXIvn1+ycc5lAfs3OOZcpPNk55zKCJzvnXMsnUCtPds65DJDKLbvUvU/s\nnEsrtTco4k0J1RVnUC5JR0n6s6SXJb0W9qTUJE92zrnIRJHsEhyU66fA42Z2MkGfmP8Zr15Pds65\n6CiBKb5EBuUyoGP4uRNQGa9Sv2bnnIuGEn6Doouk2N54p5vZ9Jj5hgblqj9q+a3As5KuA74EDIq3\nU092zrnIJHhNbnMEPRVfDswws7sknQrMlHSimf2zsQ082TnnIhHhGxSJDMp1NXAhgJn9XdIhQBdg\nY2OV+jU751x0orlmt3tQLkltCW5AzKtX5h/AuQCSTgAOAZocYNpbds65aCia5+zMbJek8cAi9gzK\nVSbpdmC5mc0DfgjcL+lGgpsVYyzO6GGe7JxzkYmqi6eGBuUys0kxn1cDp+9LnZ7snHPRSd0XKDzZ\nOeeik8qvi3myc85FYl9eB0sGT3bOuch4snPOZQTv4sk5lxG8Zeeca/kies6uuXiyc85FQkAK5zpP\nds65qPjdWOdchmjlNyiccy2eUvs0tkX2evLsooX0LjqeosICpv7qF3ut37FjB1decRlFhQUMPG0A\n69et271u6i+nUFRYQO+i43nu2UUeZ5rFet/kUaxfPIXlcyc0Wuaumy7h9ZLJlM65hZMK83cvHzV0\nAKtKJrGqZBKjhtbvK7J5pMtxTYQIWnbxpmRJSrKT9KCkjZJej7rumpoabvj+tZTMf4aXX1vN3Nmz\nWLN6dZ0yMx58gJzsHMreKOe6629k4oSfALBm9WrmzpnNylfLmLdgIddf9z1qamqiDjGt4ky3WGfO\nX8rwa6c1uv6CM3rR86iunDj8NsbfOYt7JowEIKfjoUwcN5gzR/+agVdOZeK4wWR3aN9scUJ6HddE\nSfGnZElWy24GYcd7UVtWWkrPngX0OOYY2rZty4jLRrJgfkmdMgvmlzBq9FUAXHTxJTz/p8WYGQvm\nlzDispG0a9eO7j160LNnActKS5sjzLSJM91ifWHlO3y0bXuj64vP6s1jC4L9l65aR6cO7TmyS0fO\nO+0EFi99g63V26n6+DMWL32D80+vP8ZLtNLpuCYqqtHFmkNSkp2Z/RX4qDnqrqysID9/TyeneXn5\nVFRU7F2mW1AmKyuLjp06sWXLFioq9t62srJ+B6mZFWe6xRpP7uHZbPhg6+75ig+ryD08m9yu2Wz4\nMGb5xipyu2Y3aywt6bhC0GpL5dPYlL1BIWkcMA6g21FHJTka51x8qf3oScreoDCz6WbW18z6du3S\nNeHtcnPz2LBhz8BEFRUbyMvL27vMe0GZXbt2Ub1tG507dyYvb+9tc3PrbhuVdIkz3WKNp3JjFflH\n5uyezzsim8qNVVRuqiL/iJjlh2dTuamqWWNpSce1ll+zO4j69utHefnbrFu7lp07dzJ3zmyGFA+r\nU2ZI8TAenfkQAE89+QRnfeMcJDGkeBhz58xmx44drFu7lvLyt+nXv39Gx5luscbzx7+s4oriYP/9\nv9qd6k8+44PN1Tz34hoGnVpIdof2ZHdoz6BTC3nuxTXNGktLOq61UvmaXcqexu6vrKwsfvO7exk6\n5AJqamq4asxYehUVcfutkzilT1+Khw5jzNirGTtmNEWFBeTkfJmZj84GoFdRERePuJSTe/ciKyuL\n394zjdatW2d0nOkW60NTxjCwz7F0yT6M8oV3cMd9T9MmK9jf759YwsIlZVxwRhFl8yaz/fMvuObW\nRwDYWr2dKfcvZMkjNwHw8+kL2Vrd+I2OKKTTcU1Iij9npzhjVDTPTqVZwNkEQ599CEw2swcaK9+n\nT1974aXlja12LVxOv/HJDiFhW5fdm+wQEnL6gL6sWLE80tT0pbzj7YTv3he33Ip/P2dFBOPG7rOk\ntOzM7PJk7Nc517xS+QZFizuNdc4lTwrnOk92zrmIeH92zrlM4P3ZOecyRHLfkIjHk51zLjJ+Guuc\na/lS/Dk7T3bOuUgE1+xSN9t5snPORcaTnXMuI/gNCudcy+fX7JxzmUAp3p+dJzvnXGRSONd5snPO\nRadVCme7Ftd5p3MuOaIcg0LShZLelFQu6eZGylwqabWkMkmPxauz0ZadpI5NbWhm1fFDds5lkihu\nxkpqDUwDzgM2AMskzTOz1TFljgVuAU43s62SDo9Xb1OnsWWAETwrWKt23gAfBcc5V0dENyj6A+Vm\n9m5Y52xgOBA7qO63gWlmthXAzDbGq7TRZGdm3Rpb55xzDUkw13WRFNv1+HQzmx4znwe8FzO/ARhQ\nr47jgv3pBaA1cKuZLWxqpwndoJA0EjjGzH4uKR84wsxWJLKtcy4ziODxkwRsjqBb9izgWILhHfKB\nv0r6qpk1OiRc3BsUku4FvgGMDhdtB+J3NO+cyywSrVvFnxJQAcSeWeaHy2JtAOaZ2RdmthZ4iyD5\nNSqRu7Gnmdk1wOcAZvYR0DaRiJ1zmSWicWOXAcdK6iGpLTASmFevzP8StOqQ1IXgtPbdpipN5DT2\nC0mtCG5KIKkz8M+EQnbOZQwRzXN2ZrZL0nhgEcH1uAfNrEzS7cByM5sXrjtf0mqgBvixmW1pqt5E\nkt004Emgq6TbgEuB2w7guzjnWqionik2s6eBp+stmxTz2YAfhFNC4iY7M3tY0gpgULhohJm9nugO\nnHOZoyW8G9sa+ILgVNbfunDO7UUi0RsQSZHI3diJwCwgl+CuyGOSbmnuwJxz6UcJTMmSSMvum8DJ\nZrYdQNLPgJeBKc0ZmHMu/aT7aez79cplhcucc2634G5ssqNoXFMdAfyG4BrdR0CZpEXh/PkEz8E4\n59weSt/OO2vvuJYBf4xZvrT5wnHOpbO0HIPCzB44mIE459Jb2p7G1pLUE/gZ0As4pHa5mR3XjHE5\n59JQKp/GJvLM3AzgfwgS92DgcWBOM8bknEtTqfzoSSLJ7lAzWwRgZu+Y2U8Jkp5zzu0mBe/GxpuS\nJZFktyPsCOAdSd+RNBTo0MxxHZBnFy2kd9HxFBUWMPVXv9hr/Y4dO7jyissoKixg4GkDWL9u3e51\nU385haLCAnoXHc9zzy7yONMs1vsmj2L94iksnzuh0TJ33XQJr5dMpnTOLZxUmL97+aihA1hVMolV\nJZMYNbR+X5HNI12Oa6KiGoOiWWJLoMyNwJeA7wOnE3SHPDbeRpK6SfpzzIAY1x9YqImpqanhhu9f\nS8n8Z3j5tdXMnT2LNatX1ykz48EHyMnOoeyNcq67/kYmTvgJAGtWr2bunNmsfLWMeQsWcv1136Om\npiaj40y3WGfOX8rwa6c1uv6CM3rR86iunDj8NsbfOYt7JowEIKfjoUwcN5gzR/+agVdOZeK4wWR3\naN9scUJ6HddERdTFU7OIm+zM7CUz+9jM/mFmo81smJm9kEDdu4Afmlkv4OvAtZJ6HWjA8SwrLaVn\nzwJ6HHMMbdu2ZcRlI1kwv6ROmQXzSxg1+ioALrr4Ep7/02LMjAXzSxhx2UjatWtH9x496NmzgGWl\npRkdZ7rF+sLKd/ho2/ZG1xef1ZvHFgT7L121jk4d2nNkl46cd9oJLF76Blurt1P18WcsXvoG55/e\nvL+u6XRcEyHin8Km5GmspD9IeqqxKV7FZva+ma0MP38MrAHyJH1b0jJJr0p6UtKh0X0dqKysID9/\nTyeneXn5VFRU7F2mW1AmKyuLjp06sWXLFioq9t62srJ+B6mZFWe6xRpP7uHZbPhg6+75ig+ryD08\nm9yu2Wz4MGb5xipyu2Y3aywt6bgCkECrLpktu6YePbk3qp1I6g6cDLwErDSz+8PldwJXA//RwDbj\ngHEA3Y7ygcycSwdp+eiJmS1uakp0B5IOI+j884ZwrNkTJf1N0ipgFFDUyP6nm1lfM+vbtUvXhL9Q\nbm4eGzbsGZioomIDeXl5e5d5Lyiza9cuqrdto3PnzuTl7b1tbm7dbaOSLnGmW6zxVG6sIv/InN3z\neUdkU7mxispNVeQfEbP88GwqNzU6dkskWtJxheCxktZS3ClZmrVvOkltCBLdo2ZWe+o7AxhvZl8l\n6PH4kEY23y99+/WjvPxt1q1dy86dO5k7ZzZDiofVKTOkeBiPznwIgKeefIKzvnEOkhhSPIy5c2az\nY8cO1q1dS3n52/Tr3z/K8NIuznSLNZ4//mUVVxQH++//1e5Uf/IZH2yu5rkX1zDo1EKyO7Qnu0N7\nBp1ayHMvrmnWWFrSca3VSvGnZEm08859pqA9+wCwxszujlnVAXg/TISj2HvUoAOSlZXFb353L0OH\nXEBNTQ1XjRlLr6Iibr91Eqf06Uvx0GGMGXs1Y8eMpqiwgJycLzPz0dkA9Coq4uIRl3Jy715kZWXx\n23um0bp16yjDS7s40y3Wh6aMYWCfY+mSfRjlC+/gjvuepk1WsL/fP7GEhUvKuOCMIsrmTWb7519w\nza2PALC1ejtT7l/IkkduAuDn0xeytbrxGx1RSKfjmqhUfl1MQVfuCRSU2pnZjoQrls4A/gasYs8A\nPROAo4GbgE0E1/A6mNmYpurq06evvfDS8qaKuBYsp9/4ZIeQsK3LIrvU3axOH9CXFSuWR5qajjz2\nRBt195Nxy909rHBFBOPG7rNE3o3tT9BC6wQcJelrwLfM7LqmtjOzJTT+dsh/7WugzrnUl8otu0Su\n2d0DFANbAMzsVYJBs51zbjdBVINkN4tErtm1MrP19W4pJ/9Rbedcyknl0bgSSXbvhaeyJqk1cB3w\nVvOG5ZxLRyn8mF1Cye67BKeyRwEfAv8XLnPOud2U5NfB4klkkOyNwMiDEItzLs2lcK5L6G7s/QQD\n7dRhZuOaJSLnXFoSkJXCt2MTOY39v5jPhwD/D3ivkbLOuQyW1i07M6vTBbukmcCSZovIOZeekvw6\nWDz787pYD+CIqANxzqU/JXWUiaYlcs1uK3uu2bUiGDT75uYMyjmXftJ6KMXwZf6vsedl/X9aoi/T\nOucyTjLfkIinyQeew8T2tJnVhJMnOudcg2pbdqnaxVMib3e8IunkZo/EOZfeIuyWXdKFkt6UVC6p\n0ctmki6WZJLi9qLS6GmspCwz20XQnfoySe8AnwZfCTOzUxIL2zmXKaJ4gyJ8LXUacB6wgSD/zDOz\n1fXKdQCuJ+gqLq6mrtmVAqcAw5oo45xzQKQ3KPoD5Wb2LoCk2cBwYHW9cncAvwR+nEilTSU7AZjZ\nO/scqnMuAyU8xkQXSbG98U43s+kx83nUfXFhA1Bn1HJJpwDdzOyPkg442XWV9IPGVtbrat05l+FE\nwtfkNh9IT8WSWgF3A2P2Zbumkl1r4DAa723YOef2iO5uawXQLWY+n7pj1XQATgSeD/vZPBKYJ2mY\nmTU6fkNTye59M7t9/+N1zmWaiLp4WgYcK6kHQZIbCVxRu9LMtgFdauclPQ/8qKlEB00/euItOudc\nwmpPYw/00ZPwKZDxwCJgDfC4mZVJul3Sft8wbapld+7+Vuqcy0xRvUFhZk8DT9dbNqmRsmcnUmej\nyc7MPtqX4JxzmU2k/xgUzjkXn4Ku2VOVJzvnXGRSN9V5snPORSR4gyJ1050nO+dcZFK4hydPds65\nqMiv2TnnWj6/G+ucyxjesnPOZYTUTXWe7JxzEZFItIunpPBk55yLjJ/GOucyQuqmutS+ebLfnl20\nkN5Fx1NUWMDUX/1ir/U7duzgyisuo6iwgIGnDWD9unW710395RSKCgvoXXQ8zz27yONMs1jvmzyK\n9YunsHzuhEbL3HXTJbxeMpnSObdwUmH+7uWjhg5gVckkVpVMYtTQAY1uH6V0Oa6JimrAnebQbMlO\nUjdJf5a0WlKZpOvD5c8nMhLQ/qqpqeGG719LyfxnePm11cydPYs1q+t2XT/jwQfIyc6h7I1yrrv+\nRiZO+AkAa1avZu6c2ax8tYx5CxZy/XXfo6amJqPjTLdYZ85fyvBrpzW6/oIzetHzqK6cOPw2xt85\ni3smjAQgp+OhTBw3mDNH/5qBV05l4rjBZHdo32xxQnod10QEj54o7pQszdmy2wX80Mx6AV8HrpXU\nqxn3B8Cy0lJ69iygxzHH0LZtW0ZcNpIF80vqlFkwv4RRo68C4KKLL+H5Py3GzFgwv4QRl42kXbt2\ndO/Rg549C1hWWprRcaZbrC+sfIePtm1vdH3xWb15bEGw/9JV6+jUoT1HdunIeaedwOKlb7C1ejtV\nH3/G4qVvcP7pzfvrmk7HNTGileJPydJsyc7M3jezleHnjwk64csLV4+W9Iqk1yX1j3K/lZUV5Ofv\n6dE5Ly+fioqKvct0C8pkZWXRsVMntmzZQkXF3ttWVtbdNtPiTLdY48k9PJsNH2zdPV/xYRW5h2eT\n2zWbDR/GLN9YRW7X7GaNpSUd11qpfBp7UG5QSOpOMP5s7fiOh5rZSZLOBB4k6E++/jbjgHEA3Y46\n6mCE6Zw7ALWnsamq2W9QSDoMeBK4wcyqw8WzAMzsr0BHSXv9CTWz6WbW18z6du3SNeH95ebmsWHD\nnlHYKio2kJeXt3eZ94Iyu3btonrbNjp37kxe3t7b5ubW3TYq6RJnusUaT+XGKvKPzNk9n3dENpUb\nq6jcVEX+ETHLD8+mclNVs8YgYbiWAAAQx0lEQVTSko4rEPZnl7otu2ZNdpLaECS6R83sqZhVVq9o\n/fn91rdfP8rL32bd2rXs3LmTuXNmM6S4brf1Q4qH8ejMhwB46sknOOsb5yCJIcXDmDtnNjt27GDd\n2rWUl79Nv/6RnmWnXZzpFms8f/zLKq4oDvbf/6vdqf7kMz7YXM1zL65h0KmFZHdoT3aH9gw6tZDn\nXlzTrLG0pONaK5WTXbOdxip4uvABYE0DY8xeBvxZ0hnAtnC0oEhkZWXxm9/dy9AhF1BTU8NVY8bS\nq6iI22+dxCl9+lI8dBhjxl7N2DGjKSosICfny8x8dDYAvYqKuHjEpZzcuxdZWVn89p5ptG7dOqrQ\n0jLOdIv1oSljGNjnWLpkH0b5wju4476naZMV7O/3Tyxh4ZIyLjijiLJ5k9n++Rdcc+sjAGyt3s6U\n+xey5JGbAPj59IVsrW78RkcU0um4JkKk9hsUMousUVW34iCR/Q1YBfwzXDwBuAl4BTgLaAOMNbMm\nbyP16dPXXnipyVHSXAuW0298skNI2NZl9yY7hIScPqAvK1YsjzQzHX/iSfZfTyyOW+7cE7qsOJBB\nsvdXs7XszGwJDT9Q/XQDy5xzLUAKN+z8dTHnXHSUwndjPdk55yIRjEGR7Cga58nOOReNJL8hEY8n\nO+dcZFI31Xmyc85FxIdSdM5ljNRNdZ7snHNRSuFs58nOORcZP411zmWE1E11nuycc1FK4Wznyc45\nFwnhb1A45zJBkrtwiqdFji7mnEuOqPqzk3ShpDcllUu6uYH1PwgH83pN0mJJR8er05Odcy4iSui/\nuLVIrYFpwGCgF3B5A4N1vQz0NbPewBPAr+LV68nOOReZiFp2/YFyM3vXzHYCs4HhsQXM7M9mVtu7\n6lIgnzg82TnnIqEEJ6CLpOUx07h6VeUB78XMb2DPyIQNuRp4Jl58foPCORedxFpum6PqqVjSlUBf\ngp7Pm+TJzjkXmYjeoKgAusXM54fL6pA0CJgInGVmO+LGFkVkzjkHCZ/GxrMMOFZSD0ltgZHAvDr7\nkU4G/hsYZmYbE6nUk51zLhr7cNGuKWa2CxgPLALWAI+bWZmk2yXVjjU5FTgMmCvpFUnzGqluNz+N\ndc5FJqo3KMzsaeoNzmVmk2I+D9rXOj3ZOeciIVL7DQpPds65yHiyc85lBO8IwDmXEbxl55zLCCmc\n6zzZOecilMLZzpOdcy4Sko9B4ZzLEKmb6jzZOeeilMLZzpOdcy4iiXXOmSye7JxzkUnhS3YtsyOA\nZxctpHfR8RQVFjD1V7/Ya/2OHTu48orLKCosYOBpA1i/bt3udVN/OYWiwgJ6Fx3Pc88u8jjTLNb7\nJo9i/eIpLJ87odEyd910Ca+XTKZ0zi2cVLing9tRQwewqmQSq0omMWrogGaNs1a6HNdE1L4uFsUY\nFM3hoCc7SYdIKpX0qqQySbdFWX9NTQ03fP9aSuY/w8uvrWbu7FmsWb26TpkZDz5ATnYOZW+Uc931\nNzJxwk8AWLN6NXPnzGblq2XMW7CQ66/7HjU1NVGGl3ZxplusM+cvZfi10xpdf8EZveh5VFdOHH4b\n4++cxT0TRgKQ0/FQJo4bzJmjf83AK6cycdxgsju0b7Y4Ib2Oa6KiGIOiuSSjZbcDOMfMvgacBFwo\n6etRVb6stJSePQvoccwxtG3blhGXjWTB/JI6ZRbML2HU6KsAuOjiS3j+T4sxMxbML2HEZSNp164d\n3Xv0oGfPApaVlkYVWlrGmW6xvrDyHT7atr3R9cVn9eaxBcH+S1eto1OH9hzZpSPnnXYCi5e+wdbq\n7VR9/BmLl77B+afXH+MlWul0XBPlLbsYFvgknG0TThZV/ZWVFeTn7+nkNC8vn4qKir3LdAvKZGVl\n0bFTJ7Zs2UJFxd7bVlbu1UFqRsWZbrHGk3t4Nhs+2Lp7vuLDKnIPzya3azYbPoxZvrGK3K7ZzRpL\nSzqutSLqvLNZJOWanaTWkl4BNgLPmdlLDZQZVzsgx6bNmw5+kM65fZNAqy6jWnYAZlZjZicR9C3f\nX9KJDZSZbmZ9zaxv1y5dE647NzePDRv2DExUUbGBvLy8vcu8F5TZtWsX1du20blzZ/Ly9t42N7ep\nQY32X7rEmW6xxlO5sYr8I3N2z+cdkU3lxioqN1WRf0TM8sOzqdxU1ayxtKTjCrU3KBR3Spak3o01\nsyrgz8CFUdXZt18/ysvfZt3atezcuZO5c2YzpHhYnTJDiofx6MyHAHjqySc46xvnIIkhxcOYO2c2\nO3bsYN3atZSXv02//v2jCi0t40y3WOP5419WcUVxsP/+X+1O9Sef8cHmap57cQ2DTi0ku0N7sju0\nZ9CphTz34ppmjaUlHddaqXwae9Cfs5PUFfjCzKoktQfOA34ZVf1ZWVn85nf3MnTIBdTU1HDVmLH0\nKiri9lsncUqfvhQPHcaYsVczdsxoigoLyMn5MjMfnQ1Ar6IiLh5xKSf37kVWVha/vWcarVu3jiq0\ntIwz3WJ9aMoYBvY5li7Zh1G+8A7uuO9p2mQF+/v9E0tYuKSMC84oomzeZLZ//gXX3PoIAFurtzPl\n/oUseeQmAH4+fSFbqxu/0RGFdDquiUrl5+xkFtm9gcR2KPUGHgJaE7QsHzez25vapk+fvvbCS8sP\nRnguBeX0G5/sEBK2ddm9yQ4hIacP6MuKFcsjTU1fO7mPLXp+adxyX8luuyKqcWP3xUFv2ZnZa8DJ\nB3u/zrmDIIVbdv66mHMuEkEXT8mOonGe7JxzkfGOAJxzmSF1c50nO+dcdFI413myc85FJ5UfPfFk\n55yLhFBKj0HRIvuzc865+rxl55yLTAo37DzZOeei44+eOOdaviR34RSPJzvnXCRqx6BIVZ7snHOR\n8dNY51xGSOWWnT964pyLTFSdd0q6UNKbksol3dzA+naS5oTrX5LUPV6dnuycc9GJINtJag1MAwYD\nvYDLJdUf6u1qYKuZFQC/IYEOgD3ZOeciIaCVFHdKQH+g3MzeNbOdwGxgeL0ywwk6AQZ4AjhXcQa4\nSItrditXrtjcvo3WR1xtF2BzxHU2F481es0SZ/s2jQ/QfQCaI9ajI66PlStXLGrfRl0SKHqIpNiu\nx6eb2fSY+TzgvZj5DcCAenXsLmNmuyRtAzrTxHFKi2RnZokPL5YgScuT0TX0/vBYo5cucUL6xGpm\nkQ2c1Rz8NNY5l2oqgG4x8/nhsgbLSMoCOgFbmqrUk51zLtUsA46V1ENSW2AkMK9emXnAVeHnS4A/\nWZzRw9LiNLaZTI9fJGV4rNFLlzghvWI9YOE1uPHAIoJRCB80szJJtwPLzWwe8AAwU1I58BFBQmzS\nQR9K0TnnksFPY51zGcGTnXMuI3iyc85lhIxLdpKOl3SqpDbhaykpLR1iTCfxnrJPJZKKJJ0lqXOy\nY2kJMuoGhaSLgJ8TPKNTASwHZphZdVIDa4Ck48zsrfBzazOrSXZMDZGkeLf8U0VsrJJaAa3CO3+t\nzOyfSQ6vDkmDCd73fBdoA1xtZh8kN6r0ljEtO0ltgMsIfmnOBUoIHkr8iaSOSQ2uHknFwCuSHgMw\ns5pUbOHVSx7flPRDSRdJykl2bPVJOpbgdSIk3Qg8DJRIOsXM/plKLT5JZwO/A75lZv8K7AROTGpQ\nLUDGJLtQR+DY8PMfgAUEfzWvSJVfdklfAsYDNwA7JT0CqZnwYhLdjcBY4GOC2CdIOjKZscUKj9vd\nwA8lDSV4CPVu4G/AbEkDzMzC1l4q+BC4xsxKw+M4ABgv6b8lXZIqv6vpJlV+uM3OzL4g+AW/SNLA\n8LRlCfAKcEZSg4thZp8SJI7HgB8RvDS9O+ElM7Zakgok9Q37FOsFnACcS/DHJIvgD8iPJR2RzDhh\nd4sun+BYngB8G5hvZivN7BcEvxOzJHVNlVNZM1tjZn8OZ68G/jNs4f2dIFEn8rK9qydjkl3ob8Cz\nwGhJZ5pZjZk9BuQCX0tuaHuYWaWZfWJmm4FrgPa1CU/SKZIKkxVbeIr9FDAV+D3wJeDfgUFAMXAO\n8DJwPvCDZLZGw1ifBB4BriPoI+1ToL+krwCY2X3Ai8AhyYqzKWb2MzO7M/w8g+APSrcmN3INyqhk\nZ2afA48CrwK3SBon6SrgCOD9pAbXCDPbQpDwvpD0BjAH+CQZsUg6jSDJXWVm3yBIHOPM7EOCf4Sv\nmdmusPj/AXcnqzVaL9aBQFvgAmAcQcvzRklDJF1B0LLf1WhlSVL/dFXSxQS/q5XJiSjNmVnGTQS/\n+N8g6BRwBnBysmNKIOYbgQ+AryYxhtOAMTHzXYGS8HMPgmt2jxL0P3ZCko9XQ7EuCD93I/ij8Rpw\nF9Ar2T/fON+lHcHpbBlwYrLjSdcpox49qS88xTJLkWs1jQnvbj4O/NDMXktiHK2BL5lZdfj5K8B8\n4F/M7H1JpxBcElhlZlF3thp1rCcDNwE/MLOUbNXXCp8kOA94x8zeTHY86SqTez3BUuSCfzxmtlXS\nUAtOw5MZRw1Q+0yigCrgozB5fBPoA9xiZtuTFWOtOLFeBRwHfMfMtiUrxkRZcHPt6WTHke4yumXn\nDpykGQTXO88nOG1cldyIGlcv1n9LZivZHXye7Nx+CS+etwHWhP8/18zeTm5UDUunWF3z8WTnDoik\nMcAyMytLdizxpFOsLnqe7NwBSdd3Y13m8WTnnMsIGfVQsXMuc3myc85lBE92zrmM4MnOOZcRPNml\nMUk1kl6R9LqkuZIOPYC6zpa0IPw8TNLNTZTNlvS9/djHrZJ+lOjyemVmSLpkH/bVXdLr+xqja7k8\n2aW3z8zsJDM7kaA32+/ErlRgn3/GZjbPgr7eGpMN7HOycy6ZPNm1HH8DCsIWzZuSHgZeB7pJOl/S\n3yWtDFuAhwFIulDSG5JWAhfVViRpjKR7w89HSPqDpFfD6TTgF0DPsFU5NSz3Y0nLJL0m6baYuiZK\nekvSEuD4eF9C0rfDel6V9GS91uogScvD+orD8q0lTY3Z9zUHeiBdy+TJrgWQlAUMBmrfSz2WoHfb\nIoI+534KDDKzUwgGGfqBpEOA+4GhBC/wN9aN+j3AX8zsa8ApBN0M3UzQA8dJZvZjSeeH++wPnAT0\nkXSmpD7AyHDZvwD9Evg6T5lZv3B/awi6NqrVPdzHEOC+8DtcDWwzs35h/d+W1COB/bgMk9G9nrQA\n7SW9En7+G/AAQRdL681sabj860Av4IWwL8i2BN17FwJra98RDXtCHtfAPs4Bvgm7exLZpr0H1Dk/\nnF4O5w8jSH4dgD/U9oIiaV4C3+lESXcSnCofBiyKWfd42B3X25LeDb/D+UDvmOt5ncJ9v5XAvlwG\n8WSX3j4zs5NiF4QJ7dPYRcBzZnZ5vXJ1tjtAAqaY2X/X28cN+1HXDOBfzezV8F3Ws2PW1X/dx8J9\nX2dmsUkRSd33Y9+uBfPT2JZvKXC6pAIIRi+TdBzwBtBdUs+w3OWNbL8Y+G64bWtJnQh6JO4QU2YR\nMDbmWmCepMOBvwL/Kqm9pA4Ep8zxdADeDzusHFVv3QhJrcKYjwHeDPf93bA8ko5TMEKbc3V4y66F\nM7NNYQtplqR24eKfmtlbksYBf5S0neA0uEMDVVwPTJd0NVADfNfM/i7phfDRjmfC63YnAH8PW5af\nAFea2UpJcwjG/NgILEsg5H8HXgI2hf+PjekfQCnBeBffMbPPJf2e4FreyrArp03AvyZ2dFwm8Y4A\nnHMZwU9jnXMZwZOdcy4jeLJzzmUET3bOuYzgyc45lxE82TnnMoInO+dcRvj/wmAIrAr1EhEAAAAA\nSUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from the sklearn library example\n",
    "# https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "# Plot non-normalized confusion matrix\n",
    "plot_confusion_matrix(ys, preds, classes=['0', '1', '2a', '2b', '3'],\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plot_confusion_matrix(ys, preds, classes=['0', '1', '2a', '2b', '3'], normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 753
    },
    "colab_type": "code",
    "id": "xclyZkcIs7NU",
    "outputId": "f90b32f7-52ce-4dc9-c683-6b23a84cb1b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix, without normalization\n",
      "[[  0   0   0  27]\n",
      " [  0   0   0   7]\n",
      " [  0   0   0  11]\n",
      " [  0   0   0 136]]\n",
      "Normalized confusion matrix\n",
      "[[0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUMAAAEYCAYAAADGepQzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nOQd4MnTOOcCToXPOAZ4MnXMOgP8HG+ocr1qPUSAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUAAAAEYCAYAAAAtTS8wAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from the sklearn library example\n",
    "# https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "# Plot non-normalized confusion matrix\n",
    "plot_confusion_matrix(y2s, y2_preds, classes=['0', '1', '3', 'is2a2b'],\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plot_confusion_matrix(y2s, y2_preds, classes=['0', '1', '3', 'is2a2b'], normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "OPjWjPABzqht",
    "outputId": "a84e7a7d-f264-4d35-f956-8629862b6eb3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i in clean_data['y'].values:\n",
    "  if i == 0 or i == 1 or i == 3:\n",
    "    count+=1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mxV9oh3Z1Dt_"
   },
   "source": [
    "So only 45 data points are values other than 2a and 2b, thus about 136 are 2a and 2b. The data is taken over by 2a and 2b data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oicKFQeL0tGF"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "168 DSA.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
